<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Home</title>
    <link>/post/</link>
    <description>Recent content in Posts on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Of Fortune Tellers and Crystal Balls</title>
      <link>/post/of-fortune-tellers-and-crystal-balls/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/of-fortune-tellers-and-crystal-balls/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Fortune telling is an ancient form of magic (not dark) and is art of predicting a person’s life. They have crystal balls that help them delve into the future. (Un)Fortunately, the plethora of skeptics don’t believe the practice simply because it isn’t based on scientific facts. But what if I told you that fortune telling is all math? Don’t believe me? If you read this chapter you’ll realize why the practice of fore-seeing the future isn’t exactly a fairy tale.&lt;/p&gt;
&lt;div id=&#34;load-the-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load the packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr) # to read in data
library(readxl) # to read in the data
library(dplyr) # for data manipulation
library(tidyr) # for cleaning up data
library(ggplot2) # for data viz
library(fpp2) # time series 
library(forecast) # time series&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;time-series-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time series Data&lt;/h1&gt;
&lt;p&gt;Time series data are a sequence of correlated points (denoted by &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;) indexed by uniformly separated elements of time. Therefore, time series data represents a &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_process&#34;&gt;stochastic&lt;/a&gt; process. Another interesting fact is time series data is a special case of panel data. Panel/Longitudinal data is multidimensional data that involves measurement over time. Cross-sectional data is another instance of panel data which frequently occurs in economics. It’s concerns measuring and comparing multiple subjects at the same point in time.&lt;/p&gt;
&lt;p&gt;An example of time series data is the weekly closing price for AT&amp;amp;T stocks obtained from the &lt;a href=&#34;https://datamarket.com/data/set/22s6/weekly-closing-price-of-att-common-shares-1979#!ds=22s6&amp;amp;display=line&#34;&gt;datamarket&lt;/a&gt; website. The example below shows how the AT&amp;amp;T stock prices behave as time series elements.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;att &amp;lt;- read_excel(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Time series/att.xlsx&amp;quot;)

ggplot(att,aes(Week,Price))+
  geom_line()+
  ggtitle(&amp;quot;Time series plot of weekly closing prices of AT&amp;amp;T&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;components-of-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Components of Time Series&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trend:&lt;/strong&gt; Trend is the overall tendency of the time series data to move in a general upward or downward direction. The term trend is vague and has no mathematical basis. It is only used to give the data an overall direction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Seasonality:&lt;/strong&gt; Seasonality are &lt;em&gt;short&lt;/em&gt;, &lt;em&gt;consistent&lt;/em&gt; periodic pattern in the time series data that is &lt;em&gt;repeated&lt;/em&gt; every time period.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can visualise the trend and seasonality using the &lt;code&gt;stl&lt;/code&gt; function. For instance lets look at the time series decomposition of monthly sales volumes for anti-diabetic drugs in Australia:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season.decomp.ts &amp;lt;- (stl(a10, s.window=&amp;quot;periodic&amp;quot;)$time.series)
season.decomp.df &amp;lt;- timetk::tk_tbl(season.decomp.ts)

#convert to long form
season.decomp.df %&amp;gt;%
     gather(key, value, -index) %&amp;gt;%
     ggplot(aes(x = index, y = value))+
     geom_line()+
     facet_grid(key~., scales = &amp;quot;free&amp;quot;)+
     theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see and upward trend and small peaks along the way. The rise and fall around the peaks represent seasons that is repeated every calender year. Seasonality may be the result of external factors that influence the time series data - such as the AT&amp;amp;T stock prices; the rise and fall of which might be affected by sentiments of customers. Another cool way to visualize the seasons within each year:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggseasonplot(a10, polar = TRUE) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Cyclicity:&lt;/strong&gt; This too has a rise and fall pattern like seasonality except cycles are &lt;em&gt;longer&lt;/em&gt; and &lt;em&gt;inconsistant&lt;/em&gt;. They are inconsistent in their magnitude as well as their repeat patterns. More often cycles are a part of a longer time series extending over decades.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;type-of-time-series-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Type of Time series data&lt;/h2&gt;
&lt;p&gt;Based on behavior, time series can be categorized in two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Frequency domain:&lt;/strong&gt; This approach assumes that the characteristics of a time series are explained by systematic or periodic movements that are observed in nature.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Time domain:&lt;/strong&gt; It is primary based on the assumption that adjacent time series objects are correlated.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This brings us to the world of ARIMA models. The &lt;em&gt;AutoRegressiveMovingAverage&lt;/em&gt; models are the outcome of landmark work by Box and Jenkins. This is why a univariate ARIMA model is also referred to as a UBJ (univariate box-jenkins) model. ARIMA models fall under the time domain. UBJ models are broadly classified into two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stationary:&lt;/strong&gt; Stationarity can of two forms - mean stationary and variance stationary. As the name suggests, a time series that has a constant mean is mean stationary and it’s variance stationary when the variance of &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;’s are independent of time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non Stationary:&lt;/strong&gt; Any time series that is either mean or variance non stationary is non stationary in nature.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, the AT&amp;amp;T stock prices is a non stationary process over the 52 week period. However, the first difference of the above series becomes a stationary (approximately). The &lt;code&gt;diff&lt;/code&gt; fucntion is used to compute the difference of a speacific order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_df &amp;lt;- data.frame(difference = diff(att$Price,1),time.index = 1:51)

ggplot(diff_df,aes(time.index,difference))+
   geom_line()+
   geom_point(col = &amp;quot;#FC4E07&amp;quot;)+
   labs(title = &amp;quot;Stationarized AT&amp;amp;T Time series&amp;quot;,y = &amp;quot;First order difference&amp;quot;,x = &amp;quot;Time&amp;quot;)+
   theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;white-noise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;White Noise&lt;/h2&gt;
&lt;p&gt;Almost all statistical concepts are not complete without defining a term for randomness that real life data exhibits. In time series the term ‘white noise’ refers to a set of uncorrelated random variables. It is also not unusual for most statistical ideas to be optimistic and consider the distribution of random variables as normal. Mathematically, they are represented as: &lt;span class=&#34;math display&#34;&gt;\[Z_t \sim iid(0,\sigma^2)\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This just means that they are independently and identically distributed with mean 0 and has a inherent variation. The source of this variation can be factors that affect the data points externally and internally. White noise is also not observed(as with any random component).Lets look at a simulated white noise:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To observe the effect of white noise take a look at these images. The top image shows a simple cosine curve. The next image shows what happens when we introduce white noise in the curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- 1:500;w &amp;lt;- rnorm(500,0,1)
c &amp;lt;- 2*cos(2*pi*t/50)
c_new &amp;lt;- c+w
tmp &amp;lt;- data.frame(t = 1:500,c,c_new)

my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

tmp %&amp;gt;%
  gather(key,value,-t)%&amp;gt;%
  ggplot(aes(x = t, y = value, col = key))+
  geom_line()+
  facet_grid(key~.)+
  scale_color_manual(values = my2cols)+
  theme_minimal()+
  labs(title = &amp;quot;Effect of WN on a time series&amp;quot;, x = &amp;quot;time&amp;quot;)+
  theme(legend.position = &amp;quot;none&amp;quot;, strip.text = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;characteristics-of-stationary-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Characteristics of Stationary time series&lt;/h2&gt;
&lt;p&gt;Stationarity ensures that our time series is at a state of equilibrium and the basic behavior does not alter with time in terms of mean or variance. Therefore, the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt; could just be represented by &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. This simplifies our calculations and gives us a good starting point to describing the characteristics of time series. It’s also the same reason why if a series is non-stationary, we try to stationarize it first (We shall discuss how very soon).&lt;/p&gt;
&lt;p&gt;Stationary time series can further be classified into two types based on variance and autocovariance of the data points:&lt;br /&gt;
- A time series is &lt;em&gt;weakly&lt;/em&gt; stationary if the variance does not change with time&lt;br /&gt;
- A time series is &lt;em&gt;strictly&lt;/em&gt; stationary if the joint distribution of the set &lt;span class=&#34;math inline&#34;&gt;\({X_{t1},X_{t2}...}\)&lt;/span&gt; is equal to the time shifted set &lt;span class=&#34;math inline&#34;&gt;\(X_{t1+h},X_{t2+h}...\)&lt;/span&gt;. Also, the mean of the two sets along with the variance is stationary.&lt;/p&gt;
&lt;p&gt;In statistics, a collection of random variables always has two aspects- their distribution and their moments. Think of these as IDs but they’re not always unique. It’s common for two different random variables to have the same distribution and the same moments. The time series random variables also have distribution and moments. It is also possible to define a joint distribution for all these variables. Generally, one would define a joint distribution function as: &lt;span class=&#34;math display&#34;&gt;\[F(c_1,c_2,c_3,...,c_n) = P(x_{t1}&amp;lt;c_1,x_{t2}&amp;lt;c_2,....,x_{tn}&amp;lt;c_n)\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Defining such a ‘combined’ distribution becomes a problem when the random variables follow different distributions. So, we make the assumption that they follow the same (normal) distribution (ie, normal iid or multinomial Gaussian). Now we can define our CDF as:&lt;span class=&#34;math display&#34;&gt;\[F(c_1,c_2,c_3,...,c_n) = \prod_{t=1}^n 1/\sqrt(2 \pi)\int_\infty^x exp(-z^2/2) dz\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can now define the PDF as:&lt;span class=&#34;math display&#34;&gt;\[f_{t_1,t_2..}(x)=\frac{dF_{t_1,t_2..}(x)}{dx}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re now at a stage to define the different attributes of a stationary time series process but before that we need to define what a &lt;strong&gt;realization&lt;/strong&gt; is. A &lt;strong&gt;realization&lt;/strong&gt; is one subset of observations coming from the underlying process. Simply put it represents a sample. And normal statistical practice dictates that we estimate parameters of the process from this realization (or sample). From now on when we are referring to a time series we refer to realization of the actual underlying process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mean&lt;/strong&gt;: As with any statistical distribution, the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; of a time series is simply: &lt;span class=&#34;math display&#34;&gt;\[E(X_t)=\int_{- \infty}^{\infty} X f_t(X) dx\]&lt;/span&gt;. It is no surprise that the sample (or realization) mean estimates the actual mean. &lt;span class=&#34;math display&#34;&gt;\[\bar{X_t} = \hat{\mu_t}= \frac{\sum_{t=1}^n X_t}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; : Following the usual definition of variance in statistical literature, the variance of a time series is given by &lt;span class=&#34;math display&#34;&gt;\[Var(X_t)=E(X_t - \mu_t)^2\]&lt;/span&gt;. While the estimated variance &lt;span class=&#34;math display&#34;&gt;\[Var(\bar{X_t})= \frac{\sigma^2}{N} \sum_{h=-(n-1)}^{n-1}(1- \frac{|k|}{n})\rho_h\]&lt;/span&gt;. where n = realization of length n.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Autocovariance function&lt;/strong&gt;: It is the second moment function. It is given by:&lt;span class=&#34;math display&#34;&gt;\[Cov(X_t,X_{t+h}) = E[(X_t-\mu_t)(X_{t+h}-\mu{t+h})]\]&lt;/span&gt; Here h is called &lt;em&gt;lag&lt;/em&gt; time which simply means that the observation &lt;span class=&#34;math inline&#34;&gt;\(X_{t+h}\)&lt;/span&gt; is ‘h’ lags ahead of &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;. This is often represented by &lt;span class=&#34;math inline&#34;&gt;\(\gamma(h)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h=h+t-t\)&lt;/span&gt; for a stationary process and does not depend on t.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is important to realize that &lt;span class=&#34;math inline&#34;&gt;\(Cov(X_t,X_t)=Var(X_t)\)&lt;/span&gt;. Notice, there is no lag here or &lt;span class=&#34;math inline&#34;&gt;\((t-t)=0\)&lt;/span&gt;. Hence, variance is also represented as &lt;span class=&#34;math inline&#34;&gt;\(\gamma(0)\)&lt;/span&gt;. The estimated ACVF is given by: &lt;span class=&#34;math display&#34;&gt;\[\hat{\gamma_h}=\frac{1}{n} \sum_{t=1}^{n-|h|} (X_t - \bar{X})(X_{t+|h|}-\bar{X_t})\]&lt;/span&gt; where h=0,&lt;span class=&#34;math inline&#34;&gt;\(\pm 1,\pm2,..,\pm(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;autocorrelation-function-acf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Autocorrelation function (ACF)&lt;/h2&gt;
&lt;p&gt;The autocorrelation function, mathematically is similar to the usual correlation function that we encounter in statistics. The intuition behind ACF is that there are correlations between &lt;em&gt;ordered pairs&lt;/em&gt; (I will explain ordered pairs in details when we discuss difference but for now think of ordered pairs as a pairs of observations that are separated by &lt;em&gt;h&lt;/em&gt; lags) of data. This is the essence of UBJ ARIMA modeling.&lt;/p&gt;
&lt;p&gt;The ACF is a very important tool during the initial model identification as well as final model selection stage. It is given by the formula:&lt;span class=&#34;math display&#34;&gt;\[Corr(X_t,X_{t+h})=\frac{Cov(X_t,X_{t+h})}{\sqrt{Var(X_t)}\sqrt{Var(X_{t+h})}}\]&lt;/span&gt; The difference in lag is &lt;span class=&#34;math inline&#34;&gt;\(t+h-t=h\)&lt;/span&gt; and therefore the above is popularly represented by &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)\)&lt;/span&gt; for a stationary process.&lt;/p&gt;
&lt;p&gt;It can be easily shown that:&lt;br /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)=\frac{\gamma(h)}{\gamma(0)}\)&lt;/span&gt;&lt;br /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)=\rho(-h)\)&lt;/span&gt;. This means that ACF of a stationary time series is symmetric about the origin.&lt;/p&gt;
&lt;p&gt;Therefore, the estimated ACF is just ratio of estimated ACVFs and is given by &lt;span class=&#34;math display&#34;&gt;\[\hat{\rho_h}=\frac{\hat{\gamma_h}}{\hat{\gamma_0}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is what the ACF (using the &lt;code&gt;ggAcf&lt;/code&gt; fucntion) of the AT&amp;amp;T stock prices looks like when plotted against lag:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(att$Price)+
  labs(title = &amp;quot;ACF PLot&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first set of numbers are the actual values of ACF and the plot below that displays the values. The first value for ACF is always one.(Think why?). The blue dashed line is cutoff that indicates that only the ACF values at the respective lags are significant. Finally, the estimated ACF is also referred to as the sample ACF (SACF).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partial-autocorrelation-function-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partial Autocorrelation Function (PACF)&lt;/h2&gt;
&lt;p&gt;Recall, when I mentioned that the ACF measures the correlation between ordered pairs drawn from a time series data we chose to ignore the effects of the &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;’s that lie between the pair. PACF does exactly that. It measures the correlation between &lt;span class=&#34;math inline&#34;&gt;\(X_t,X_{t+h}\)&lt;/span&gt; but takes into consideration all the random variable interactions between them. So, if we wanted to measure the PACF between &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{t+2}\)&lt;/span&gt;, this would also include the effects of &lt;span class=&#34;math inline&#34;&gt;\(X_{t+1}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(X_{t+2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The PACF is designated by the symbol &lt;span class=&#34;math inline&#34;&gt;\(\phi_{hh}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\phi_{hh}}\)&lt;/span&gt;. I will not go into the details of PACF calculation (it involves using the AR, MA and ARMA equations and developing recursive equations that give us fairly good estimates of PACF) since it is complicated and in reality these calculations will be done by R.&lt;br /&gt;
Then why do we study the ACF and PACF? The answer to this question will be revealed very soon when I explain how to do ARIMA modeling.&lt;/p&gt;
&lt;p&gt;Note: Just like theoretical and estimated ACF we also have the theoretical and estimated PACF. And just like it’s cousin the estimated PACF is referred to as SPACF or Sample PACF.&lt;/p&gt;
&lt;p&gt;Let’s look at the PACF (using the &lt;code&gt;ggPacf&lt;/code&gt;) for AT&amp;amp;T stock prices&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(att$Price)+
  labs(title = &amp;quot;PACF PLot&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, the first set of numbers are the actual values of PACF for the time series and the plot above is a graphical interpretation of those numbers. The blue dashed line once again indicates that only the lags for which the PACF peaks crosses the blue region are significant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-models-for-time-series&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistical Models for Time series&lt;/h1&gt;
&lt;p&gt;Recall that time series data are a collection of random variables indexed according to the sequence they are obtained in time. So, &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(t \in {0,\pm1, \pm2,..}\)&lt;/span&gt; is such a random variable and the subscript &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; represents the order or sequence of discrete points in time.&lt;/p&gt;
&lt;p&gt;The UBJ ARIMA family has 4 statistical models. They are nothing but an algebraic expression describing how these points are related.&lt;/p&gt;
&lt;div id=&#34;the-moving-average-ma-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The moving average (MA) model&lt;/h2&gt;
&lt;p&gt;The algebraic equation for the MA model is:&lt;span class=&#34;math display&#34;&gt;\[X_t = c + Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(theta_t\)&lt;/span&gt; are the parameters of the model and &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; are the white noise/error terms. &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the lag of the model and is referred to as the ‘order’ of the process. The choice of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a personal one and it depends on the person modeling and the nature of the problem at hand.&lt;/p&gt;
&lt;p&gt;The intuition behind the above equation is that the current value &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; is an average of the past &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; random shocks (which are obviously unobserved) so instead of measuring &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; as the mean and the current random shock, say that they are an average of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; random shocks.&lt;br /&gt;
There is another intuition behind the moving average equation and perhaps it’s a more mathematically accurate intuition. If you were to look at the MA equation, it appears as if it is linear regression of the current value of the series &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; against the current and past values of white noise.&lt;/p&gt;
&lt;p&gt;A key question from the MA equation is that even though I have stressed that a univariate Box-Jenkins model essentially studies the relationships between present and past random variables, the above process does not include past random variables. The answer is that the random shock terms above can be replaced by past values of the time series through manipulation. In other words, you can think of &lt;span class=&#34;math inline&#34;&gt;\(Z_t&amp;#39;s\)&lt;/span&gt; as being part of &lt;span class=&#34;math inline&#34;&gt;\(X_t&amp;#39;s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is an example of a simulated MA process with &lt;span class=&#34;math inline&#34;&gt;\(theta_1=0.5\)&lt;/span&gt; with 100 data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ma_sim &amp;lt;- arima.sim(model = list(ma=0.5),n=50)
ma_sim &amp;lt;- as.data.frame(ma_sim)
ma_sim &amp;lt;- ma_sim %&amp;gt;% mutate(t = 1:50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;bindrcpp&amp;#39; was built under R version 3.4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ma_sim,aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs(&amp;quot;Simulated MA Series&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + Z_t-\theta_1Z_{t-1}\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + Z_t-\theta_1Z_{t-1}+\theta_2Z_{t-2}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Backshift Operator:&lt;/em&gt; There exists an operator called the backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z_t = B^0Z_t,Z_{t-1}=B^1Z_t,Z_{t-2}=B^2Z_t\)&lt;/span&gt;. This gets rid of all the confusing subscripts. It is also convention to write &lt;span class=&#34;math inline&#34;&gt;\(X_t - c\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t}\)&lt;/span&gt;. Using these conventions we can write the above examples as:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t} = (1 - \theta_1B)Z_t\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t} = (1 - \theta_1B - \theta_2B^2)Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence a general MA(q) process can be represented as: &lt;span class=&#34;math display&#34;&gt;\[\tilde{X_t} = (1 - \theta_1B - \theta_2B^2 - ..... - \theta_qB^q)Z_t = \Theta_q(B)Z_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: It can be shown that variance of an MA process is independent of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. This effect trickles down to their ACF function.&lt;/p&gt;
&lt;p&gt;When I discussed ACF and PACF, I had mentioned that the importance of estimated ACF and PACF becomes paramount in the model selection steps. Here’s why:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF of an MA process:&lt;br /&gt;
For an MA process the ACF shows spikes at lags where the autocorrelation are significant. Specifically, the MA will have spikes at k &lt;span class=&#34;math inline&#34;&gt;\(\le\)&lt;/span&gt; q and then &lt;em&gt;sharply&lt;/em&gt; drops to zero. Let’s look at the ACF from our simulated MA model:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ma_sim &amp;lt;- arima.sim(model = list(ma=0.5),n=100)
ggAcf(ma_sim)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The MA(1) process has a &lt;em&gt;spike&lt;/em&gt; at lag one.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PACF of an MA process: The PACF of an MA process dampens to zero &lt;em&gt;quickly&lt;/em&gt; but not &lt;em&gt;sharply&lt;/em&gt;. The fall is gradual but relatively smooth and quick yet not a sharp drop. Lets look at the PACF of the simulated MA model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance it might be difficult to see the gradual drop as it looks like a spike but if you carefully inspect the &lt;em&gt;spikes&lt;/em&gt; on both sides of the axis, you will realise that the spikes actually change slowly from being significant to insignificant. The ability to recognise this drop versus a spike takes time and experience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-autoregressive-ar-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The autoregressive (AR) model&lt;/h2&gt;
&lt;p&gt;The algebraic equation for the MA model is:&lt;span class=&#34;math display&#34;&gt;\[X_t = c +\phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p} + Z_t\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(phi_t&amp;#39;s\)&lt;/span&gt; are the parameters of the model and &lt;span class=&#34;math inline&#34;&gt;\(X_t&amp;#39;s\)&lt;/span&gt; are the previous lagged terms. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the lag of the model and is referred to as the ‘order’ of the process. Once again, the choice of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a personal one and it depends on the person modeling and the nature of the problem at hand.&lt;/p&gt;
&lt;p&gt;The AR process is more intuitive than the MA. We can see how the present value is related to the previous value through a regression equation. Also, just like a regression equation, the &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; represents the probabilistic aspect of a linear model. Let’s look at a simulated AR time series:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ar_sim &amp;lt;- data.frame(x = arima.sim(model = list(ar=0.5),n=50))
ar_sim %&amp;gt;% 
 mutate(t = 1:50)%&amp;gt;%
 ggplot(aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs(&amp;quot;Simulated AR Series&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;br /&gt;
- AR(1): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + \phi_1X_{t-1} + Z_t\)&lt;/span&gt;&lt;br /&gt;
- AR(2): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c +\phi_1X_{t-1}+\phi_2X_{t-2}+Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Backshift Operator:&lt;/em&gt; For AR process also there exists a backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X_t = B^0X_t,X_{t-1}=B^1X_t,X_{t-2}=B^2X_t\)&lt;/span&gt;. Like the MA process, it is also convention to write &lt;span class=&#34;math inline&#34;&gt;\(X_t - \mu\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{c}{1-\phi_1}\)&lt;/span&gt;. Using these conventions we can write the above examples as:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\((1 - \theta_1B)\tilde{X_t} = Z_t\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\((1 - \theta_1B - \theta_2B^2)\tilde{X_t} = Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence a general MA(q) process can be represented as: &lt;span class=&#34;math display&#34;&gt;\[(1 - \theta_1B - \theta_2B^2 - ..... - \theta_pB^p)\tilde{X_t} = Z_t = \Phi_p(B)X_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: For an AR process the variance depends on time. This effect is visible when we discuss the ACF and PACF of an AR process next.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ACF of AR process:&lt;/strong&gt;The ACF for the AR process is exactly opposite to the ACF of the MA. The AR ACF shows a gradual decline to zero&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PACF of AR process:&lt;/strong&gt; The PACF is exactly opposite to the PACF of the MA. The AR PACF shows spikes for lags. Let’s look at the ACF and PACF plots for AR and MA process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(ar_sim)+
  labs(title = &amp;quot;ACF&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(ar_sim)+
  labs(title = &amp;quot;PACF&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is how ACF and PACF act as distinguishing factors in the identification of the appropriate statistical model that best explains the given realisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arma-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ARMA process&lt;/h2&gt;
&lt;p&gt;We have learnt that sometimes the present value depends on the past values (AR process) of the time series process and sometimes the present value depends on the past random shocks (MA process). There’s another model that uses both these features. This is called the ARMA model. In time series this process is fully represented by ARMA(p,q) where p, q carry the same meaning as before. Perhaps the mathematical representation will make the intuition more clear,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_t = c + \phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p}+Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using the backshift operator this can be represented as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Phi_p(B)\tilde{X_t} = \Theta_q(B)Z_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here the symbols have their usual meaning except &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_T}\)&lt;/span&gt;. This is &lt;span class=&#34;math inline&#34;&gt;\(X_t - \mu\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{c}{1-\phi_1-\phi_2...-\phi_p}\)&lt;/span&gt;. For instance an ARMA(1,1) process can be written as &lt;span class=&#34;math inline&#34;&gt;\((1-\phi_1B)X_t=(1-\theta_1B)Z_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ARIMA Model&lt;/h2&gt;
&lt;p&gt;This is a upgrade from the ARMA model. The ‘I’ in ARIMA stands for Integrated which indicates the time series values have been differenced. The ARIMA components can be represented in two different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the model has a &lt;em&gt;non seasonal&lt;/em&gt; component only the model is represented as &lt;strong&gt;ARIMA(p,d,q)&lt;/strong&gt; where p,d,q stands for the AR, MA and difference order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the has both &lt;em&gt;seasonal and non seasonal&lt;/em&gt; components, the model is represented by &lt;strong&gt;ARIMA(p,d,q)(P,D,Q)&lt;/strong&gt; where P,D,Q stands for the seasonal AR, MA and difference order.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Identification:&lt;/strong&gt; Identifying the order of an ARIMA can be tricky. From personal experience, when there are indications of both AR and MA process in the ACF and PACF, it could indicate the combination of both process. Signs of seasonal components is displayed by spikes or decays at the seasonal lag. Rob Hyndman’s &lt;a href=&#34;https://www.otexts.org/fpp/8/9&#34;&gt;page&lt;/a&gt; on seasonal ARIMA has nice examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-makes-a-good-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What makes a good model&lt;/h2&gt;
&lt;p&gt;When we select a time series model, there are signs that indicate how good the model is. Here is a list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stationarity &amp;amp; Invertiblity:&lt;/strong&gt; We can use the coefficients of the model to check for stationarity and invertibility by using certain algebraic inequalities. This &lt;a href=&#34;http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/TimeSeries_Stationarity.pdf&#34;&gt;resource&lt;/a&gt; outlines the rules.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Correlation:&lt;/strong&gt; The coefficients in the model should not be highly correlated. If they are, they could represent redundent terms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statistical Significance of parameters:&lt;/strong&gt; The parameters estimated should be statistically significant. We can check to see of the p-value is lower than the significance value in the output.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Diagnostic Checking:&lt;/strong&gt; Although in practice we cannot observe the random shocks, it is important that they are statistically independent. If the residuals are autocorrelated they are not white noise. We perform the Ljung-Box (Box-Pierce) test which essentially tests the autocorrelations among random shocks. The random noise should be normally distributed and the residual-ACF should resemble white noise ACF (no significant lags).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-in-arima-modelling-and-forecasting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps in ARIMA modelling and forecasting&lt;/h1&gt;
&lt;p&gt;Now we are ready to layout the steps we use to first identify a time series model parameters and then forecast future values. The data used contains 1810 observations in Bowling Green, Ohio of the maximum daily temperature in degrees Celsius, one observation every day from January 1, 2011 - December 31, 2015 (so 5 years of complete data).&lt;/p&gt;
&lt;p&gt;The Bowling Green station has a latitude and longitude of 41.3831?, -83.6111?. The data goes into the GHCN (Global Historical Climatology Network)-Daily database, maintained by the National Climactic Data Center (NCDC), part of the U.S. Department of Commerce.The data was aggregated over every week and the mean maximum weekly temperature was obtained.The final 51 weeks were used as test set, and the rest was used to train the ARIMA model.&lt;/p&gt;
&lt;div id=&#34;step-1-visualize-the-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Visualize the time series&lt;/h2&gt;
&lt;p&gt;The first step is to visualize the time series to gain insights into the behavior (such as seasonality) of the time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;%
  ggplot(aes(x = week, y = weekly_mean))+
  geom_line(col = my2cols[1])+
  geom_point(col = my2cols[2])+
  labs(title = &amp;quot;Bowling Green Weekly Average Maximum&amp;quot;,y = &amp;quot;Temperture&amp;quot;,x = &amp;quot;Week ID&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The time series plot for the first four years of data is clearly seasonal with a period of length s = 52 (weeks).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-identify-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Identify the model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ACF Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(train$weekly_mean, lag.max = 200)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PACF Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(train$weekly_mean, lag.max = 200)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The inferences can be summarized:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looking at the ACF, the early lags’ decay is possibly quick enough that a differencing of the original series will not be necessary (d = 0), but we may consider it as well (d = 1) since it is a rather grey area. (&lt;em&gt;NOT SHOWN: A first order difference ACF plot showed the possibility of an MA component&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;The seasonal spikes at 52, 104, and 156 are decreasing rather slowly but are within the 5% significance limits, so on the seasonal side there is also a grey area as to whether D = 0 or D = 1 needs to be used.&lt;/li&gt;
&lt;li&gt;Looking at the PACF, we have spikes for lags 1 and 2, especially for 1 (meaning that, if we use d = 0, may start with an AR1 model)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the above information lets fit two different models - &lt;em&gt;ARIMA(0,1,1)(0,1,1)&lt;/em&gt; and &lt;em&gt;ARIMA(1,0,1)(1,0,1)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-estimation-of-model-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Estimation of model parameters&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Arima&lt;/code&gt; function from &lt;code&gt;forecast&lt;/code&gt; package is used to fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a time series object
y &amp;lt;- ts(train$weekly_mean, frequency = 52, start = c(2011, 1))

first.model &amp;lt;- forecast::Arima(y = y,
                               order = c(0,1,1),
                               seasonal = list(order = c(0,1,1), period = 52))
  

summary(first.model) # summary of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: y 
## ARIMA(0,1,1)(0,1,1)[52]                    
## 
## Coefficients:
##           ma1     sma1
##       -0.8196  -0.9772
## s.e.   0.0608   1.1281
## 
## sigma^2 estimated as 12.43:  log likelihood=-448.97
## AIC=903.94   AICc=904.1   BIC=913.07
## 
## Training set error measures:
##                      ME     RMSE      MAE       MPE    MAPE      MASE
## Training set -0.1625666 3.024313 2.109829 -106.9443 152.265 0.5313132
##                    ACF1
## Training set 0.01856747&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second.model &amp;lt;- forecast::Arima(y = y,
                               order = c(1,0,1),
                               seasonal = list(order = c(1,0,1), period = 52))
  

summary(second.model) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: y 
## ARIMA(1,0,1)(1,0,1)[52] with non-zero mean 
## 
## Coefficients:
##          ar1      ma1    sar1     sma1     mean
##       0.9527  -0.4237  0.9915  -0.9215  14.6272
## s.e.  0.0220   0.0638  0.0750   0.3446   4.7782
## 
## sigma^2 estimated as 14.57:  log likelihood=-589.01
## AIC=1190.03   AICc=1190.45   BIC=1210.05
## 
## Training set error measures:
##                     ME     RMSE      MAE       MPE     MAPE      MASE
## Training set 0.1306786 3.770671 3.051861 -35.31861 103.6716 0.7685429
##                     ACF1
## Training set -0.06111836&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-other-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Other tests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Significance test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(first.model) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##       Estimate Std. Error  z value Pr(&amp;gt;|z|)    
## ma1  -0.819648   0.060812 -13.4785   &amp;lt;2e-16 ***
## sma1 -0.977199   1.128073  -0.8663   0.3864    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(second.model) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##            Estimate Std. Error z value  Pr(&amp;gt;|z|)    
## ar1        0.952665   0.022011 43.2818 &amp;lt; 2.2e-16 ***
## ma1       -0.423745   0.063845 -6.6371   3.2e-11 ***
## sar1       0.991536   0.075026 13.2159 &amp;lt; 2.2e-16 ***
## sma1      -0.921538   0.344585 -2.6743  0.007488 ** 
## intercept 14.627228   4.778240  3.0612  0.002204 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that one of the parameters in the first model is insignificant.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Correlation test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(first.model$var.coef) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              ma1        sma1
## ma1   1.00000000 -0.02333284
## sma1 -0.02333284  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(second.model$var.coef) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ar1          ma1        sar1        sma1    intercept
## ar1        1.00000000 -0.250681177 -0.02601634  0.01704827 -0.118705291
## ma1       -0.25068118  1.000000000 -0.04030825  0.01638146  0.003743894
## sar1      -0.02601634 -0.040308250  1.00000000 -0.99878199  0.039394253
## sma1       0.01704827  0.016381464 -0.99878199  1.00000000 -0.036329713
## intercept -0.11870529  0.003743894  0.03939425 -0.03632971  1.000000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extremely high correlation between the SAR and SMA term in the second model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error measures&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first.model$sigma2 # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12.43442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second.model$sigma2 # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.56816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model has better (lower) error measure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assumption Checking&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;checkresiduals(first.model) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(0,1,1)(0,1,1)[52]
## Q* = 126.13, df = 102, p-value = 0.05286
## 
## Model df: 2.   Total lags used: 104&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;checkresiduals(second.model) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,0,1)(1,0,1)[52] with non-zero mean
## Q* = 262.02, df = 99, p-value &amp;lt; 2.2e-16
## 
## Model df: 5.   Total lags used: 104&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assumptions for both models seem to be satisfied.&lt;/p&gt;
&lt;p&gt;At this point we make a decision to move forward with one of the model for forecasting. We could go forward with both models and further check prediction errors on both models. But this process would be unrealistic if we had plenty of models. I will pick the first model because it is simpler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Forecasting&lt;/h2&gt;
&lt;p&gt;We can forecast using the &lt;code&gt;forecast&lt;/code&gt; function using an appropriate forecast window.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts.object &amp;lt;- forecast(first.model, h = 51)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets visualize the forecasts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# obtain the forecast means and confidence intervals

forecast.df &amp;lt;- data.frame(
                 week = test$week,
                 forecasts = as.matrix(forecasts.object$mean),
                 upper = as.matrix(forecasts.object$upper),
                 lower = as.matrix(forecasts.object$lower),
                 actual = test$weekly_mean
)


forecast.df %&amp;gt;%
  ggplot()+
    geom_line(mapping = aes(x = week, y = forecasts), col = my2cols[1], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = actual), col = my2cols[2], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = lower.95.), lty = 2)+
    geom_line(mapping = aes(x = week, y = upper.95.), lty = 2)+
    labs(y = &amp;quot;Temperature&amp;quot;,x = &amp;quot;Week&amp;quot;,title = &amp;quot;Actual vs Forecasted Value&amp;quot;)+
    theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;),legend.position=&amp;quot;top&amp;quot;)+
    theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line represent the forecasted values while the red one represents the actual ones. The dotted lines on the other hand represent the upper and lower confidence intervals. We can see the model does a pretty good job at forecasting the values for the test data set.&lt;/p&gt;
&lt;p&gt;Finally, let’s calculate the accuracy of the forecasted values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error = function(actual, pred){
  mse = sqrt(mean((actual - pred)^2))
  mad =  mean(abs(actual - pred))
  mape = mean(abs((actual - pred)/actual))
  list(mse = mse, mape = mape, mad = mad)
}

error(forecast.df$actual, forecast.df$forecasts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $mse
## [1] 3.533256
## 
## $mape
## [1] 0.3449951
## 
## $mad
## [1] 2.630848&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The error function computes the standard error measures (Mean Square Error, Mean Absolute Deviation, Mean Absolute Percentage Error). The error values seem to be sufficiently small.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Carbon Dioxide Emmissions</title>
      <link>/post/carbon-dioxide-emmissions/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/carbon-dioxide-emmissions/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Global warming is real. 100%. NASA came out with a &lt;a href=&#34;https://www.nasa.gov/press/goddard/2014/november/nasa-computer-model-provides-a-new-portrait-of-carbon-dioxide/&#34;&gt;beautiful simulation&lt;/a&gt; of carbon dioxide emissions for countries all around the world. Ever since then, I was eager to replicate this in R. Of course I couldn’t make it as beautiful as theirs but nevertheless, the final picture I made came out great by R standards. I found some good data on carbon dioxide emissions from the WRI &lt;a href=&#34;http://datasets.wri.org/dataset/cait-country&#34;&gt;website&lt;/a&gt;. I was able to use the data to create an animation with &lt;code&gt;ggplot2&lt;/code&gt;. A great way to perform animations with &lt;code&gt;ggplot2&lt;/code&gt; is to use the &lt;code&gt;gganimate&lt;/code&gt; package. But the package is filled with bugs and fails to run on many occasions. The &lt;code&gt;gganimate&lt;/code&gt; function is just a wrapper around image magick. You can easily create the frames, save them in a drive and just run image magick from your console. But if you do manage to make &lt;code&gt;gganimate&lt;/code&gt; run on your R-studio then go ahead use that option instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;div id=&#34;carbon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Carbon&lt;/h2&gt;
&lt;p&gt;This is the carbon dioxide emission data by country and Year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon &amp;lt;- read.csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/Projects/Carbon Emmissions/carbon.csv&amp;quot;)
colnames(carbon)[3] &amp;lt;- &amp;quot;emission&amp;quot;

knitr::kable(tail(carbon,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;emission&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Vietnam&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;30829&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;World&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34389.5959&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30830&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yemen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.9220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;30831&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Zambia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4472&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30832&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Zimbabwe&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.3572&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rename USA and RUSSIA
carbon$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, carbon$Country)
carbon$Country &amp;lt;- gsub(&amp;quot;Russian Federation&amp;quot;, &amp;quot;Russia&amp;quot;, carbon$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;world-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;World Map&lt;/h2&gt;
&lt;p&gt;World data with latitude and longitude values is easily available using the &lt;code&gt;map_data&lt;/code&gt; function. You need to convert it into a data frame. I removed Antarctica because data for carbon dioxide emissions was not available for Antarctica.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;world &amp;lt;- map_data(&amp;#39;world&amp;#39;) %&amp;gt;% 
                   data.frame() %&amp;gt;%
                   select(1:3,5) %&amp;gt;%
                   filter(region != &amp;quot;Antarctica&amp;quot;) %&amp;gt;%
                   rename(Country = region)

knitr::kable(head(world,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;long&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;group&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.89912&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.45200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.89571&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.42300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.94219&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.43853&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-70.00415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.50049&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-70.06612&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.54697&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Transformations&lt;/h1&gt;
&lt;p&gt;I noticed there was a lot of missing values for the years before 1950. So I filtered those years out. I also converted &lt;code&gt;Country&lt;/code&gt; into factors. &lt;em&gt;Please don’t convert the years into factors&lt;/em&gt;. We would take advantage of the numeric nature to loop through the years and create separate images.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon &amp;lt;- carbon %&amp;gt;%
              filter(Year &amp;gt; 1949) %&amp;gt;%
              mutate(Country = as.factor(Country))
              

carbon[is.na(carbon)] &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualization&lt;/h1&gt;
&lt;p&gt;The following codes would create 64 frame (or plots), one for each Year and save it to the current working directory. It’s pretty straight forward. The first few lines is just a fancy way to create the names for the &lt;code&gt;jpeg&lt;/code&gt;s as they are created. Then I extract the &lt;code&gt;Year&lt;/code&gt; one by one and combine it with the world data. The penultimate step is to create the plot in &lt;code&gt;ggplot2&lt;/code&gt;. The final trick is to use the &lt;code&gt;ggsave&lt;/code&gt; function to save the &lt;code&gt;jpegs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After you have saved the plots in the current working directory, you need to download &lt;a href=&#34;https://www.imagemagick.org/script/download.php&#34;&gt;imagemagick&lt;/a&gt; on your device. Then you need to go your command line (cmd for windows users) and after that your need to navigate to the working directory using &lt;code&gt;cd&lt;/code&gt;. After you change the directory, you need to type this:&lt;br /&gt;
&lt;code&gt;magick convert *.jpeg -delay 10 -loop 0 carbon.gif&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;frames &amp;lt;- length(unique(carbon$Year))

windowsFonts(Times=windowsFont(&amp;quot;Times New Roman&amp;quot;))

for(i in 1:frames){
  # creating a name for each plot file with leading zeros
  if (i &amp;lt; 10) {name = paste(&amp;#39;000&amp;#39;,i,&amp;#39;.jpeg&amp;#39;,sep=&amp;#39;&amp;#39;)}
  if (i &amp;lt; 100 &amp;amp;&amp;amp; i &amp;gt;= 10) {name = paste(&amp;#39;00&amp;#39;,i,&amp;#39;.jpeg&amp;#39;, sep=&amp;#39;&amp;#39;)}
  if (i &amp;gt;= 100) {name = paste(&amp;#39;0&amp;#39;, i,&amp;#39;.jpeg&amp;#39;, sep=&amp;#39;&amp;#39;)}
  
  data &amp;lt;- carbon %&amp;gt;%
               filter(Year == (1949+i))
  
  combine &amp;lt;- left_join(world,data,&amp;quot;Country&amp;quot;)
  
  # create plot
  g &amp;lt;- ggplot(combine, aes(x = long,y = lat,group = group))+
           geom_polygon(aes(fill = emission))+
           geom_path()+ 
           scale_fill_gradientn( name = &amp;quot;Carbon Emmissions Level&amp;quot;,
                                 colours = rev(heat.colors(10)),
                                 na.value = &amp;quot;grey90&amp;quot;,
                                 limits = c(0, 7e3),
                                 guide = guide_legend( keyheight = unit(3, units = &amp;quot;mm&amp;quot;), 
                                                       keywidth = unit(12, units = &amp;quot;mm&amp;quot;),
                                                       label.position = &amp;quot;bottom&amp;quot;, 
                                                       title.position = &amp;#39;top&amp;#39;, 
                                                       nrow = 1))+
           labs(title = paste(&amp;#39;Emissions in the year&amp;#39;,1949+i),
                subtitle = &amp;quot;Carbon Dioxide Emmission since 1950&amp;quot;, 
                caption = &amp;quot;World Resources Institute&amp;quot;,
                x = &amp;quot;Longitude&amp;quot;,y = &amp;quot;Latitude&amp;quot;)+
           theme(text = element_text(color = &amp;quot;#22211d&amp;quot;), 
                 plot.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA), 
                 panel.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA), 
                 legend.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA),
                 plot.title = element_text(family = &amp;quot;Times&amp;quot;,
                                           size = 18, 
                                           hjust = 0.01, 
                                           color = &amp;quot;#4e4d47&amp;quot;, 
                                           margin = margin(b = -0.1, t = 0.4, l = 2, unit = &amp;quot;cm&amp;quot;)),
                plot.subtitle = element_text(size= 15, 
                                             hjust = 0.01, 
                                             color = &amp;quot;#4e4d47&amp;quot;, 
                                             margin = margin(b = -0.1, t = 0.43, l = 2, unit = &amp;quot;cm&amp;quot;)),
                plot.caption = element_text( size = 12, 
                                             color = &amp;quot;#4e4d47&amp;quot;, 
                                             margin = margin(b = 0.3, r=-99, unit = &amp;quot;cm&amp;quot;) ),
                legend.position = c(0.7, 0.09))
  
  # print and save
  # ggsave(name,width = 40, height = 20, units = &amp;quot;cm&amp;quot;) You would need to uncomment this
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find the gif &lt;a href=&#34;https://gfycat.com/gifs/detail/ImaginaryDistortedAlaskajingle&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Beautiful!&lt;/p&gt;
&lt;p&gt;Its not as fancy as the NASA plot but nonetheless, it’s pretty accurate. Notice the colors for USA and China. Right towards the end USA manages to reduce the emission levels but China’s emission levels keep on rising!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nobel Laureates</title>
      <link>/post/nobel-laureates/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nobel-laureates/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I was always curious to know if there was any relationship between the Intelligent Quotient (IQ) levels and the number of number laureates a country has produced. After I found the &lt;a href=&#34;https://www.kaggle.com/nobelfoundation/nobel-laureates&#34;&gt;Nobel Prize&lt;/a&gt; data set on Kaggle, I was eager to check how the numbers matched up against the IQ levels of a country. I found some average IQ data (from 1990s to 2010) for several country from this &lt;a href=&#34;https://www.worlddata.info/iq-by-country.php&#34;&gt;website&lt;/a&gt; and I was able to scrap it out to match it up with the nobel prize data. The results were quite surprising (especially for me and I will explain why). I decided to extend the plot to include more features. Surely the number of nobel laureates a country produces has to depend on how advanced the country is. A few months ago when I was working to deliver a lecture on &lt;code&gt;ggplot2&lt;/code&gt; at my school I had stumbled upon a &lt;strong&gt;Human Development Index&lt;/strong&gt; data set from this website. The HDI is a good indicator of how advanced the country is. When I added this aesthetic to the plot the results finally made more sense. More after the plot below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
library(readr)
library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The data&lt;/h1&gt;
&lt;div id=&#34;iq-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IQ data&lt;/h2&gt;
&lt;p&gt;Here’s a look at the IQ data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iq &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Visuals/Nobel Prize/iq.csv&amp;quot;, col_types = cols(`Daily max temperature` = col_skip(), `Education expenditures per capita` = col_number(), Income = col_number()))

# rename the 5th column
colnames(iq)[5] &amp;lt;- &amp;quot;exp_per_capita&amp;quot;

knitr::kable(head(iq,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Rank&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IQ&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Income&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exp_per_capita&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Singapore&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hong Kong&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26057&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;South Korea&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14077&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;538&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Taiwan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Japan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37244&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1263&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;nobel-prize-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nobel Prize Data&lt;/h2&gt;
&lt;p&gt;The Nobel Prize Data has several variables. I was interested only in the &lt;code&gt;bornCountry&lt;/code&gt; variable. I made the assumption that for a nobel prize winner who was born in a country, the credits of the prize went to that country even if he/she had moved out. Also if the name of the country changed over time (some cities belong to different countries now), I preserved the older country since the person was originally born in that country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nobel &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Visuals/Nobel Prize/nobel_prize_by_winner.csv&amp;quot;)
glimpse(nobel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 972
## Variables: 20
## $ id                &amp;lt;int&amp;gt; 846, 846, 783, 230, 918, 428, 773, 597, 615,...
## $ firstname         &amp;lt;chr&amp;gt; &amp;quot;Elinor&amp;quot;, &amp;quot;Elinor&amp;quot;, &amp;quot;Wangari Muta&amp;quot;, &amp;quot;Dorothy...
## $ surname           &amp;lt;chr&amp;gt; &amp;quot;Ostrom&amp;quot;, &amp;quot;Ostrom&amp;quot;, &amp;quot;Maathai&amp;quot;, &amp;quot;Hodgkin&amp;quot;, &amp;quot;T...
## $ born              &amp;lt;chr&amp;gt; &amp;quot;8/7/1933&amp;quot;, &amp;quot;8/7/1933&amp;quot;, &amp;quot;4/1/1940&amp;quot;, &amp;quot;5/12/19...
## $ died              &amp;lt;chr&amp;gt; &amp;quot;6/12/2012&amp;quot;, &amp;quot;6/12/2012&amp;quot;, &amp;quot;9/25/2011&amp;quot;, &amp;quot;7/29...
## $ bornCountry       &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;Kenya&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;US...
## $ bornCountryCode   &amp;lt;chr&amp;gt; &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;KE&amp;quot;, &amp;quot;EG&amp;quot;, &amp;quot;CN&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;IR&amp;quot;, &amp;quot;I...
## $ bornCity          &amp;lt;chr&amp;gt; &amp;quot;Los Angeles, CA&amp;quot;, &amp;quot;Los Angeles, CA&amp;quot;, &amp;quot;Nyeri...
## $ diedCountry       &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;Kenya&amp;quot;, &amp;quot;United Kingdom&amp;quot;, NA,...
## $ diedCountryCode   &amp;lt;chr&amp;gt; &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;KE&amp;quot;, &amp;quot;GB&amp;quot;, NA, &amp;quot;US&amp;quot;, NA, &amp;quot;IT&amp;quot;, ...
## $ diedCity          &amp;lt;chr&amp;gt; &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Nairo...
## $ gender            &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;fem...
## $ year              &amp;lt;int&amp;gt; 2009, 2009, 2004, 1964, 2015, 1983, 2003, 19...
## $ category          &amp;lt;chr&amp;gt; &amp;quot;economics&amp;quot;, &amp;quot;economics&amp;quot;, &amp;quot;peace&amp;quot;, &amp;quot;chemistr...
## $ overallMotivation &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ share             &amp;lt;int&amp;gt; 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,...
## $ motivation        &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;for her analysis of economic governance, ...
## $ name              &amp;lt;chr&amp;gt; &amp;quot;Indiana University&amp;quot;, &amp;quot;Arizona State Univers...
## $ city              &amp;lt;chr&amp;gt; &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Tempe, AZ&amp;quot;, NA, &amp;quot;Oxford&amp;quot;...
## $ country           &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, NA, &amp;quot;United Kingdom&amp;quot;, &amp;quot;China&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;human-development-index&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Human Development Index&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HDI &amp;lt;- read_excel(&amp;quot;C:/Users/routh/Desktop/Study Materials/Projects/Basic Workshop/HDI.xlsx&amp;quot;, col_types = c(&amp;quot;text&amp;quot;, &amp;quot;numeric&amp;quot;, &amp;quot;blank&amp;quot;))

knitr::kable(head(HDI,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;HDI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Norway&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9494228&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Australia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9386795&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Switzerland&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9391309&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Germany&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9256689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Denmark&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9246494&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;No explanation needed here!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-munging&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Munging&lt;/h1&gt;
&lt;p&gt;I need to take exactly what I need from each of the data set and merge them to make the final plot. From the &lt;code&gt;nobel&lt;/code&gt; data set, I extracted the &lt;code&gt;bornCountry&lt;/code&gt; column to and found the total count by country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nobel.prize &amp;lt;- nobel %&amp;gt;%
                 mutate(bornCountry = gsub(&amp;quot;\\s*\\([^\\)]+\\)&amp;quot;,&amp;quot;&amp;quot;,as.character(bornCountry)))%&amp;gt;%
                 group_by(bornCountry) %&amp;gt;%
                 summarise(count = n()) %&amp;gt;%
                 na.omit() %&amp;gt;%
                 rename(Country = bornCountry)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I extracted all the columns from the IQ data set and changed United States to USA to match it up with the other data sets. The &lt;code&gt;exp_per_capita&lt;/code&gt; stands for education expenditure per capita for all the countries in dollars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iq.country &amp;lt;- iq %&amp;gt;%
                 select(Country,IQ,exp_per_capita)
iq.country$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, iq.country$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, all the columns were used from the HDI data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HDI$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, HDI$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualization&lt;/h1&gt;
&lt;p&gt;Finally we’re ready to merge the frames. A few things to note before you move on to the plots. I rescaled the x and y variable on to a logarithmic scale to make the points closer and easier to interpret.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combine &amp;lt;- list(nobel.prize,iq.country,HDI)%&amp;gt;%
                      purrr::reduce(left_join,&amp;quot;Country&amp;quot;) %&amp;gt;%
                      na.omit() %&amp;gt;%
                      mutate(log.count = log(count+1),log.iq = log(IQ+1),
                             rank = cut(HDI,c(0.61,0.84,0.89,0.95),c(&amp;quot;Low&amp;quot;,&amp;quot;Medium&amp;quot;,&amp;quot;High&amp;quot;))) %&amp;gt;%
                      filter(log.iq &amp;gt; 4.35 &amp;amp; log.count &amp;gt; 1)



my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
windowsFonts(Times=windowsFont(&amp;quot;Times New Roman&amp;quot;))

ggplot(combine, aes(x = log.count, y = log.iq, size = exp_per_capita, fill = rank))+
  geom_point(pch = 21)+
  ggrepel::geom_text_repel(aes(label = Country),
                           size = 3,
                           segment.color = &amp;quot;black&amp;quot;,
                           segment.size = 1,
                           force = 2,
                           arrow = arrow(angle = 4, length = unit(0.09,&amp;quot;lines&amp;quot;)))+
  theme_minimal(base_size = 10)+
  scale_fill_manual(&amp;quot;HDI\nRank&amp;quot;,values = my3cols)+
  guides(fill = guide_legend(override.aes = list(size=7)))+
  scale_size_continuous(name = &amp;quot;Education Expenditure\nPer Capita&amp;quot; ,range = c(3,6))+
  scale_x_continuous(name = &amp;quot;Logarithm Nobel Prize Count&amp;quot;,breaks = seq(0,6,0.5))+
  scale_y_continuous(name = &amp;quot;Logarithm Country IQ&amp;quot;,breaks = seq(4.2,5,0.05))+
  theme(legend.position = &amp;quot;top&amp;quot;,
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(family = &amp;quot;Times&amp;quot;,size = 18))+
  labs(title = &amp;quot;Intelligent Quotient vs Nobel Prize Count&amp;quot;, 
       subtitle = &amp;quot;Relationship between IQ levels,Nobel Prize count and the Human Development Index by country&amp;quot;,
       caption = &amp;quot;Source : Multiple&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-nobel-laureates_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows IQ of a country versus the number of nobel prize won. The points are sized according to the expenditure per capita and the countries are colored by how high or low the Human Development Index is.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I had initially thought India would be ahead in terms of IQ but the lower position is justified by the low HDI level.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turns out there is a relationship between the number of nobel prizes and the IQ level.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some countries however don’t exhibit the relationship for instance Luxembourg and New Zealand.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a strong relationship however between HDI and IQ levels. We can see 3 clear strata. This is also true for expenditure per capita. Countries higher up in the IQ levels have higher expenditures for expenditure.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
