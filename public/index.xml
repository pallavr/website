<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Home</title>
    <link>/</link>
    <description>Recent content in Home on Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Framework for Estimating Customer Worth Under Competing Risk</title>
      <link>/publication/a-framework-for-estimating-customer-worth-under-competing-risk/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/a-framework-for-estimating-customer-worth-under-competing-risk/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Here is a sneak peak into my current working paper. Our research has potential in business settings where customers face multiple causes of churn.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt;Dr. Jeff Meyer,Dr. Arkajyoti Roy, Pallav Routh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;In the presence of multiple causes of exit, a customer’s survival propensity due to a specific cause is affected by all other competing causes of churn. Consequently, the worth of a customer within such a firm would also be affected by these competing forces. Therefore, the lifetime value for such customers must incorporate this competing information. In this paper, we implement a Competing Risk model for estimating the worth of a customer who faces simultaneous risks of temporary or permanent disengagement of membership from the firm. To overcome the limitations of traditional semi-parametric modeling approaches we implement a Random Survival Forest model to predict the survival probability of customer influenced by competing causes of churn. In doing so, we extend the literature on prediction of survival probabilities beyond non-competing scenarios. By the same token, we also extend the literature on customer relationship valuation under contractual business settings in the presence of multiple causes of churn. We demonstrate analytical insights that (i) investigate factors that contribute to a specific cause of churn (ii) reflect the trade-offs between predicted risk and customer value (iii) segment the predicted customer valuation curves over time (iv) study the behavior of predictor variables within such segments. Finally we utilize the prediction and analytical results to optimize the marketing-budget allocation within these customer valuation segments thereby bridging the gap between predictive and prescriptive analytics.&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>From Subjectivity to Objectivity: Measuring Customer Lifetime Value Constructs</title>
      <link>/publication/from-subjectivity-to-objectivity-measuring-customer-lifetime-value-constructs/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/from-subjectivity-to-objectivity-measuring-customer-lifetime-value-constructs/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;This post is about my first publication in a local journal on outlining the contributions of Customer Lifetime Value (CLV) to marketing within a firm. Customer Lifetime Value is where my infatuation towards data science and likeness towards data science all began. I didn’t even know about CLV until I was asked to present on it in my marketing strategy class by my professor, Dr. Banerjee. That lead me to an article called “Modeling Customer Lifetime Value” by Gupta et al (2006). That article had opened my eyes to a fascinating world of quantitative marketing and customer relationship management (CRM).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here is a link to my &lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.741.7971&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;article&lt;/a&gt;. It gives a gentle introduction to how CLV fits into the marketing machinery. Then the article focuses on the issue of balancing acquisition and retention expenses and how CLV easily helps to address the issue. I even go on to prove this with a hypothetical example that shows how CLV, even in its most simplest form, can capture the problem of balancing cost and guide managers in decision making. This is just one of the several uses of CLV as a marketing metric. CLV has evolved tremendously evolved over the years.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Authors&lt;/strong&gt; Dr. Shivaji Banerjee, Pallav Routh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As data drives decision making in business more and more, it becomes essential to objectify key market metrics such as Customer Lifetime Value (CLV). CLV is the value added by a customer to a firm’s profits during its association with the firm’s products or services. Therefore it becomes important for firms to employ suitable strategies to prolong a customer’s lifetime. In this paper we have taken this subjective aspect of CLV and given it a measurement. We have used this measurement to establish a connection between customer satisfaction and maximization of ROI. Next we have again used this measurement to compare three different marketing strategies in a market driven firm.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Death Note (Part I)</title>
      <link>/post/death-note-part-i/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/death-note-part-i/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;&lt;em&gt;Death Note&lt;/em&gt; is a Japanese manga series depicting an evil book that would kill people if their names were written on it. Now, there are no such books in the real world, but a field of study called &lt;strong&gt;Survival Analysis&lt;/strong&gt; allows statisticians to measure and predict the risk(s) individuals will face. It also allows statisticians to measure the duration of life until an event(s) occurs. The earliest forms of survival analysis originated in the 17th century in the form of &lt;strong&gt;life tables&lt;/strong&gt;. It was popularized by the work of the infamous &lt;em&gt;D.R.Cox&lt;/em&gt; and his &lt;strong&gt;semi-parametric&lt;/strong&gt; models.&lt;/p&gt;
&lt;div id=&#34;the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(invGauss) # has the data
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # data viz
library(ggfortify)
library(survival) # survival analysis&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;It’s very difficult to obtain time-to-event data on the internet. One interesting data set I found is shipped with the &lt;code&gt;invGauss&lt;/code&gt; package. The data set is about cancer in the oropharynx. This data set was given by Kalbfleisch and Prentice and has 192 observations or patients with carcinoma of the oropharynx carried out by the Radiation Therapy Oncology Group in the United States.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CASE:&lt;/strong&gt; Case Number&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;INST:&lt;/strong&gt; Participating Institution&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SEX:&lt;/strong&gt; 1=male, 2=female&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TX:&lt;/strong&gt; Treatment: 1=standard, 2=test&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRADE:&lt;/strong&gt; 1=well differentiated, 2=moderately differentiated,3=poorly differentiated, 9=missing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AGE:&lt;/strong&gt; In years at time of diagnosis&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;COND:&lt;/strong&gt; Condition: 1=no disability, 2=restricted work, 3=requires assistance with self care, 4=bed confined, 9=missing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SITE:&lt;/strong&gt; 1=facial arch, 2=tonsillar fossa, 3=posterior pillar,4=pharyngeal tongue, 5=posterior wall&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;T_STAGE:&lt;/strong&gt; 1=primary tumor measuring 2 cm or less in largest diameter,2=primary tumor measuring 2 cm to 4 cm in largest diameter with minimal infiltration in depth, 3=primary tumor measuring more than 4 cm, 4=massive invasive tumor&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;N_STAGE:&lt;/strong&gt; 0=no clinical evidence of node metastases, 1=single positive node 3 cm or less in diameter, not fixed, 2=single positive node more than 3 cm in diameter, not fixed, 3=multiple positive nodes or fixed positive nodes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ENTRY_DT:&lt;/strong&gt; Date of study entry: Day of year and year, dddyy&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;STATUS:&lt;/strong&gt; 0=censored, 1=dead&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TIME:&lt;/strong&gt; Survival time in days from day of diagnosis&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;d.oropha.rec&amp;quot;)
data &amp;lt;- d.oropha.rec&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;elements-of-survival-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Elements of Survival analysis&lt;/h1&gt;
&lt;div id=&#34;survival-time-and-event&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Survival Time and Event&lt;/h2&gt;
&lt;p&gt;The two most important measures in survival studies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;time to death&lt;/strong&gt; is a number (in days, week or years) that reflects the number of time intervals until which the person experiences the event. Let this be represented by $ t_i $ or &lt;em&gt;event time&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;strong&gt;status&lt;/strong&gt; of the patient is usually a number that identifies whether or not the person has experienced the event. Typically, “0” is used to represent observations that are active (or censored) while “1” is used to represent patients who has experienced the event. Let this be represented by $ _i $&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survivial_object &amp;lt;- with(data,Surv(time = time, event = status, type = c(&amp;quot;right&amp;quot;)))

(survivial_object)[1:30]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 1.7287671  0.7397260  0.8958904  0.6657534  2.5095890  4.9945205+
##  [7] 1.7452055  0.6438356  0.6986301  0.5041096  2.9150685  1.1342466 
## [13] 0.5917808  0.8876712  1.3150685  0.6712329  4.2876712+ 1.5342466 
## [19] 1.0301370  2.4958904  0.7643836  0.3945205  2.9917808  0.2575342 
## [25] 0.4849315  4.0328767+ 1.4410959  0.4739726  1.5753425  0.6082192&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One can create a survival object in R using the &lt;code&gt;Surv&lt;/code&gt; function. A call to the object shows the observations coded as censored (with the &lt;code&gt;+&lt;/code&gt;) and non censored. The &lt;code&gt;time&lt;/code&gt; column in the data represents the time to death or censoring time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;censoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Censoring&lt;/h2&gt;
&lt;p&gt;For individuals who have not experienced the event, (called the &lt;strong&gt;censored observations&lt;/strong&gt;), we only know the &lt;em&gt;censoring time&lt;/em&gt; (represented by $ c_i $) or the time for which the particular observation was under study. Therefore, for such observations we do not know the actual event time. Now imagine any individual &lt;em&gt;i&lt;/em&gt; there is a pair of times $ (,) $ but depending on the status we choose either one of them. If the individual experiences the event and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i=1\)&lt;/span&gt; we see &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; but if there is no event and the individual is censored, we see &lt;span class=&#34;math inline&#34;&gt;\(c_i\)&lt;/span&gt;. This information can be compactly represented by &lt;span class=&#34;math inline&#34;&gt;\((min(t_i,c_i),\delta_i)\)&lt;/span&gt;. The &lt;code&gt;status&lt;/code&gt; column in the data reflects the status of the individual until event time or censoring time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kaplan-meier-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Kaplan Meier Analysis&lt;/h1&gt;
&lt;p&gt;Kaplan-Meier (KM) estimation is a non parametric method of estimating the survival at time &lt;em&gt;t&lt;/em&gt;. If you were to order the time in ascending order by the event times &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; and if &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; was the number of deaths and let &lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt; be the number alive just before &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;, then the KM or &lt;strong&gt;product limit estimate&lt;/strong&gt; of the survivor function is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[S(t) = S(t-1)(1 - \frac{d_t}{n_t} )\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s understand where this comes from. Let &lt;span class=&#34;math inline&#34;&gt;\(t_1,t_2,t_3...\)&lt;/span&gt; be the actual times of death of the &lt;em&gt;n&lt;/em&gt; individuals and let &lt;span class=&#34;math inline&#34;&gt;\(d_1,d_2,d_3....\)&lt;/span&gt; represent the number of deaths during those times and let &lt;span class=&#34;math inline&#34;&gt;\(n_1,n_2,...\)&lt;/span&gt; be the people remaining. Now, &lt;span class=&#34;math inline&#34;&gt;\(n_2 = n_1 - d_1\)&lt;/span&gt; and so on. So, intuitively the proportion surviving at time &lt;span class=&#34;math inline&#34;&gt;\(t=2\)&lt;/span&gt; depends on the proportion surviving at &lt;span class=&#34;math inline&#34;&gt;\(t=1\)&lt;/span&gt;. Now for any time interval say $ t [t_1,t_2) $ the survival is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  S(t) = P(T \geq t) = P(\text{survive in}[0,t_1))* P(\text{survive in}[t_1,t]|\text{survive in}[0,t_1))            \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  S(t) = 1 * \frac{n_1-d_1}{n_1}  \]&lt;/span&gt;. This bring us to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ S(t) = \prod_{i=1}^{n}(1-\frac{d_i}{n_i}) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Kaplan-Meier estimate is a step function which &lt;strong&gt;discontinuities or jumps&lt;/strong&gt; at the observed death times. If there is no censoring, the K-M estimate coincides with the empirical survival function. The KM estimator can also be nicely interpreted as a NPML estimator. The &lt;code&gt;survfit&lt;/code&gt; function in the &lt;code&gt;survival&lt;/code&gt; package is used to find out the KM estimates of survival. We can use the &lt;code&gt;summary&lt;/code&gt; call to the &lt;code&gt;km&lt;/code&gt; object to get the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;n:&lt;/strong&gt; the total number of observations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;time:&lt;/strong&gt; the time points at which the &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is calculated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n.risk:&lt;/strong&gt; the number of observations at risk at time t&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n.event:&lt;/strong&gt; the number of events that occurred at time t.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;n.censor:&lt;/strong&gt; the number of censored observations, without an event, at time t.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lower,upper:&lt;/strong&gt; lower and upper confidence limits for &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;, respectively.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;strata:&lt;/strong&gt; indicates stratification of survival curves. If strata is specified, there are multiple curves in the result. There’s a demonstration later on.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.fit &amp;lt;- survfit(survivial_object ~ 1, # for a single survival curve
                  data = data)

time &amp;lt;- km.fit$time
summary(km.fit, times = time[1:20])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call: survfit(formula = survivial_object ~ 1, data = data)
## 
##    time n.risk n.event survival std.err lower 95% CI upper 95% CI
##  0.0301    192       2    0.990 0.00733        0.975        1.000
##  0.0411    190       1    0.984 0.00895        0.967        1.000
##  0.1041    189       1    0.979 0.01031        0.959        1.000
##  0.2027    188       1    0.974 0.01149        0.952        0.997
##  0.2219    187       1    0.969 0.01256        0.944        0.994
##  0.2438    186       1    0.964 0.01353        0.937        0.990
##  0.2466    185       0    0.964 0.01353        0.937        0.990
##  0.2575    184       1    0.958 0.01443        0.930        0.987
##  0.2712    183       2    0.948 0.01606        0.917        0.980
##  0.2877    181       1    0.943 0.01680        0.910        0.976
##  0.2932    180       1    0.937 0.01751        0.904        0.972
##  0.3068    179       3    0.922 0.01942        0.884        0.961
##  0.3479    176       1    0.916 0.02001        0.878        0.956
##  0.3507    175       1    0.911 0.02057        0.872        0.952
##  0.3562    174       1    0.906 0.02110        0.866        0.948
##  0.3671    173       1    0.901 0.02162        0.859        0.944
##  0.3945    172       1    0.895 0.02212        0.853        0.940
##  0.4027    171       2    0.885 0.02307        0.841        0.931
##  0.4219    169       1    0.880 0.02352        0.835        0.927
##  0.4356    168       1    0.875 0.02395        0.829        0.923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a neat plot of risk and survival estimated by the &lt;code&gt;survfit&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

km.df &amp;lt;- data.frame(
  time = time,
  survival = km.fit$surv,
  dead = km.fit$n.event
)

km.df %&amp;gt;%
  mutate(deaths = cumsum(dead)) %&amp;gt;%
  select(time,survival,deaths) %&amp;gt;%
  gather(key, value, -time) %&amp;gt;%
  ggplot(aes(x = time, y = value))+
  geom_point(col = my2cols[2])+
  geom_line(col = my2cols[2])+
  facet_wrap(~key, scales = &amp;quot;free&amp;quot;)+
  labs(title = &amp;quot;Survival and Event vs Time&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;bindrcpp&amp;#39; was built under R version 3.4.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;km-by-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;KM by groups&lt;/h2&gt;
&lt;p&gt;As I mentioned before, we can also create these KM estimates by groups. In R the best way to summarise all the information is to use the &lt;code&gt;ggsurvplot&lt;/code&gt; function from the &lt;code&gt;survminer&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.sex.fit &amp;lt;- survfit(survivial_object ~ sex,
                      data = data)

# Using ggsurv package

survminer::ggsurvplot(
   km.sex.fit,               # survfit object with calculated statistics.
   data = data,
   pval = TRUE,              # show p-value of log-rank test.
   conf.int = TRUE,          # show confidence intervals for 
                             # point estimaes of survival curves.
   conf.int.style = &amp;quot;step&amp;quot;,  # customize style of confidence intervals
   xlab = &amp;quot;Time in years&amp;quot;,    # customize X axis label.
   ggtheme = theme_light(),  # customize plot and risk table with a theme.
   risk.table = &amp;quot;abs_pct&amp;quot;,   # absolute number and percentage at risk.
   risk.table.y.text.col = T,# colour risk table text annotations.
   risk.table.y.text = FALSE,# show bars instead of names in text annotations
   ncensor.plot = TRUE,      # plot the number of censored subjects at time t
   surv.median.line = &amp;quot;hv&amp;quot;,  # add the median survival pointer.
   legend.labs = c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;),     # change legend labels.
   palette = my2cols         # custom color palettes.
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Kaplan-Meier plot can be interpreted as follow:&lt;/p&gt;
&lt;p&gt;The x axis represents time in years, and the vertical axis (y-axis) shows the proportion of people surviving. The lines represent survival curves of the two groups. &lt;strong&gt;A vertical drop in the curves indicates an event.&lt;/strong&gt; The vertical tick mark on the last plot means that a observation was censored at this time and the group it belongs to.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At time zero, the survival probability is 1.0 (or 100% of the participants are alive).&lt;/li&gt;
&lt;li&gt;At time 1.5 years, the probability of survival is approximately 0.50 (or 55%) for males. It is slightly higher for females.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;interpret-the-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpret the output&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(km.sex.fit)$table&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       records n.max n.start events   *rmean *se(rmean)   median   0.95LCL
## sex=1     147   147     147    108 1.854084  0.1360870 1.263014 0.9616438
## sex=2      45    45      45     31 2.096910  0.2490365 1.364384 0.9479452
##        0.95UCL
## sex=1 1.493151
## sex=2       NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The median survival time for female is indeed higher. The &lt;code&gt;rmean&lt;/code&gt; stands for restricted mean survival time. The restricted mean is a measure of average survival from time 0 to a specified time point, and may be estimated as the area under the survival curve up to that point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Complicated Groups&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One can also use facet grids to compare survival curves across complex groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;km.sex.treat.fit &amp;lt;- survfit(survivial_object ~ sex + treatm,
                      data = data)

ggsurv &amp;lt;- survminer::ggsurvplot(km.sex.treat.fit, data = data,fun = &amp;quot;cumhaz&amp;quot;, conf.int = TRUE)
   
ggsurv$plot +
  theme_minimal() + 
  theme (legend.position = &amp;quot;right&amp;quot;)+
  facet_grid(sex ~ treatm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hazard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hazard&lt;/h2&gt;
&lt;p&gt;In survival literature, the opposite of survival is hazard. Hazard refers to the instantaneous rate of death of the subject at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; conditional on survival to that time. Mathematically,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \lambda(t) = \lim_{\Delta t \to 0} \frac{ \text{Pr}\{(t \leq T \leq t+\Delta t)|T \geq t\}   }{\Delta t} = \frac{f(t)}{S(t)}         \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The numerator in the equation represents the conditional probability that an event will occur within the time interval of &lt;span class=&#34;math inline&#34;&gt;\([t,t+\Delta t)\)&lt;/span&gt; given that it has not occurred before (that is $ T t $). This conditional probability may be represented by a joint probability (Using the axioms of probability: P(AB) = P(A)P(B|A)). The former is represented by $ f(t) $ in the above equation while the latter is $ S(t) $ by definition. Dividing this by the width of the interval gives us a rate (of death) per unit time. Therefore, Hazard and survival have a one to one relationship (and it’s definitely not &lt;strong&gt;1-Survival&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;One can calculate hazard non parametrically using the &lt;strong&gt;Nelson Aalen Estimator&lt;/strong&gt;. Mathematically the NE estimator is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \hat{H(t)} = \sum_{t_i \leq T}\frac{d_i}{n_i}      \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with $ d_{i} $ the number of events at $ t_{i} $ and $ n_{i} $ the total individuals at risk at $ t_{i} $. You can also visualize the hazard using the &lt;code&gt;ggsurvplot&lt;/code&gt; function by specifying the value of &lt;code&gt;fun&lt;/code&gt; as &lt;code&gt;cumhaz&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggsurvplot(
   km.sex.fit,               # survfit object with calculated statistics.
   data = data,
   fun = &amp;quot;cumhaz&amp;quot;,
   pval = TRUE,              # show p-value of log-rank test.
   conf.int = TRUE,          # show confidence intervals for 
                             # point estimaes of survival curves.
   conf.int.style = &amp;quot;step&amp;quot;,  # customize style of confidence intervals
   xlab = &amp;quot;Time in years&amp;quot;,    # customize X axis label.
   ggtheme = theme_light(),  # customize plot and risk table with a theme.
   risk.table = &amp;quot;abs_pct&amp;quot;,   # absolute number and percentage at risk.
   risk.table.y.text.col = T,# colour risk table text annotations.
   risk.table.y.text = FALSE,# show bars instead of names in text annotations
   ncensor.plot = TRUE,      # plot the number of censored subjects at time t
   legend.labs = c(&amp;quot;Male&amp;quot;, &amp;quot;Female&amp;quot;),     # change legend labels.
   palette = my2cols         # custom color palettes.
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-survival-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing Survival Curves&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Log Rank Test&lt;/strong&gt; is the most popular hypothesis test to distinguish two or more survival curves. Basically the LR test compares the Hazard of the two survival curves at every point in time. If you’re interested in the formulation and the derivation of the test statistic, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Log-rank_test&#34;&gt;wikipedia&lt;/a&gt; page has a nice explanation.&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;survdiff()&lt;/code&gt; [in survival package] can be used to compute log-rank test comparing two or more survival curves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survdiff(survivial_object ~ sex,data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## survdiff(formula = survivial_object ~ sex, data = data)
## 
##         N Observed Expected (O-E)^2/E (O-E)^2/V
## sex=1 147      108    103.8     0.174     0.688
## sex=2  45       31     35.2     0.511     0.688
## 
##  Chisq= 0.7  on 1 degrees of freedom, p= 0.407&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function returns a list of components, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;n:&lt;/strong&gt; the number of subjects in each group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;obs:&lt;/strong&gt; the weighted observed number of events in each group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;exp:&lt;/strong&gt; the weighted expected number of events in each group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;chisq:&lt;/strong&gt; the chi square statistic for a test of equality.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;strata:&lt;/strong&gt; optionally, the number of subjects contained in each stratum.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The log rank test for difference in survival gives a p-value of p = 0.4, indicating that the sex groups do not differ significantly in survival.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;cox-proportional-survival-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cox Proportional Survival Analysis&lt;/h1&gt;
&lt;p&gt;The proportional hazards model (introduced by Cox, 1972) is a family of survival models that allow for covariates to be linked to the hazard directly. The first key to this relationship is that its &lt;strong&gt;multiplicative&lt;/strong&gt;. This means that we can only allow the logarithm of the hazard to be linked to the covariates by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \text{log} \lambda_i(t) = \alpha + \beta_1x_{i1} + ...                   \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or equivalently,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \lambda_i(t) = \text{exp}(\alpha + \beta_1x_{i1} + ...)   \]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in this multiplicative version represents some sort a &lt;em&gt;baseline&lt;/em&gt;. If all &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;’s are zero, then the logarithm of hazard equals &lt;span class=&#34;math inline&#34;&gt;\(e^\alpha\)&lt;/span&gt;. The good part of this family of models is it allows for this baseline to remain unspecified (or we don’t necessarily need to compute the coefficient for this intercept-like term).This makes the model &lt;em&gt;semi-parametric&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The second key aspect of the Cox model is the &lt;strong&gt;proportionality&lt;/strong&gt;. This means that &lt;em&gt;hazard ratios&lt;/em&gt; of say two observations are &lt;em&gt;independent of time&lt;/em&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s are estimated using proportionalily. This means that the coefficients are interpreted differently (see below). A value &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; greater than zero, or equivalently a hazard ratio greater than one, indicates that as the value of the ith covariate increases, the event hazard increases and thus the length of survival decreases. Put another way, a hazard ratio above 1 indicates a covariate that is positively associated with the event probability, and thus negatively associated with the length of survival.&lt;/p&gt;
&lt;p&gt;In summary,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HR = 1: No effect&lt;/li&gt;
&lt;li&gt;HR &amp;lt; 1: Reduction in the hazard&lt;/li&gt;
&lt;li&gt;HR &amp;gt; 1: Increase in Hazard&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;fitting-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the model&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;coxph&lt;/code&gt; function is used to estimate the coefficients of the parameters. The &lt;code&gt;summary&lt;/code&gt; call then shows all the estimates along with the following output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cox.fit &amp;lt;- coxph(formula = Surv(time,status)~.,
                 data = data[,-1],
                 ties = &amp;quot;efron&amp;quot;)

summary(cox.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## coxph(formula = Surv(time, status) ~ ., data = data[, -1], ties = &amp;quot;efron&amp;quot;)
## 
##   n= 192, number of events= 139 
## 
##              coef  exp(coef)   se(coef)      z Pr(&amp;gt;|z|)    
## inst   -0.0407864  0.9600342  0.0555278 -0.735   0.4626    
## sex    -0.3031563  0.7384836  0.2077356 -1.459   0.1445    
## treatm  0.1153139  1.1222257  0.1763759  0.654   0.5132    
## grade  -0.1697348  0.8438886  0.1348991 -1.258   0.2083    
## age    -0.0009324  0.9990680  0.0086164 -0.108   0.9138    
## cond    0.9144639  2.4954371  0.1523929  6.001 1.96e-09 ***
## site   -0.0506363  0.9506244  0.0740743 -0.684   0.4942    
## tstage  0.2650927  1.3035517  0.1198984  2.211   0.0270 *  
## nstage  0.1546905  1.1672966  0.0774096  1.998   0.0457 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
##        exp(coef) exp(-coef) lower .95 upper .95
## inst      0.9600     1.0416    0.8610     1.070
## sex       0.7385     1.3541    0.4915     1.110
## treatm    1.1222     0.8911    0.7942     1.586
## grade     0.8439     1.1850    0.6478     1.099
## age       0.9991     1.0009    0.9823     1.016
## cond      2.4954     0.4007    1.8511     3.364
## site      0.9506     1.0519    0.8222     1.099
## tstage    1.3036     0.7671    1.0306     1.649
## nstage    1.1673     0.8567    1.0030     1.359
## 
## Concordance= 0.691  (se = 0.027 )
## Rsquare= 0.208   (max possible= 0.999 )
## Likelihood ratio test= 44.83  on 9 df,   p=9.94e-07
## Wald test            = 52.47  on 9 df,   p=3.69e-08
## Score (logrank) test = 56.79  on 9 df,   p=5.556e-09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Cox regression results can be interpreted as follow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statistical significance:&lt;/strong&gt; The is the column under “z” and it corresponds to the ratio of each regression coefficient to its standard error (z = coef/se(coef)). This signifies, whether the coefficient of a given variable is statistically significantly different from 0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The regression coefficients signs:&lt;/strong&gt; A positive sign means that the hazard is higher, and this suggests that with the increase of the variable the hazard increases. For categorical variables the Cox model gives the hazard ratio (HR) for the one group relative to the other group. The beta coefficient for sex = -0.3 indicates that females have &lt;em&gt;lower&lt;/em&gt; risk of death (lower survival rates) than males.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hazard ratios:&lt;/strong&gt; The exponentiated coefficients in the second column of the first panel (and in the first column of the second panel) of the output are interpretable as multiplicative effects on the hazard. Thus, for example, holding the other covariates constant, an additional year of age reduces the yearly hazard of rearrest by 73 percent.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Confidence intervals:&lt;/strong&gt; of the hazard ratios. The summary output also gives upper and lower 95% confidence intervals for the hazard ratio (exp(coef))&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Global statistical significance of the model:&lt;/strong&gt; The likelihood-ratio, Wald, and score chi-square statistics at the bottom of the output are asymptotically (for large enough N, they will give similar results. For small N, they may differ somewhat) equivalent tests of the omnibus null hypothesis that all of the beta’s are zero. In this instance, the test statistics are in close agreement, and the hypothesis is soundly rejected.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;survival-curves&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Survival curves&lt;/h2&gt;
&lt;p&gt;One can extract the survival probabilities from the cox object in R and simultaneously plot it using the &lt;code&gt;ggsurvplot&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggsurvplot(survfit(cox.fit), color = &amp;quot;#2E9FDF&amp;quot;,ggtheme = theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;assumption-checking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumption Checking&lt;/h2&gt;
&lt;p&gt;In order to check these model assumptions, Residuals method are used. The common residuals for - the Cox model include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Schoenfeld residuals&lt;/strong&gt; to check the proportional hazards assumption&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Martingale residual&lt;/strong&gt; to assess nonlinearity&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Deviance residuals&lt;/strong&gt; (symmetric transformation of the Martingale residuals), to examine influential observations&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Proportionality:&lt;/strong&gt; The proportional hazards (PH) assumption can be checked using statistical tests and graphical diagnostics based on the scaled Schoenfeld residuals. In principle, the Schoenfeld residuals are independent of time. A plot that shows a non-random pattern against time is evidence of violation of the PH assumption. The function &lt;code&gt;cox.zph&lt;/code&gt; provides a convenient solution to test the proportional hazards assumption for each covariate included in a Cox regression model fit. For each covariate, the function &lt;code&gt;cox.zph&lt;/code&gt; correlates the corresponding set of scaled Schoenfeld residuals with time, to test for independence between residuals and time. Additionally, it performs a global test for the model as a whole. The proportional hazard assumption is supported by a non-significant relationship between residuals and time, and refuted by a significant relationship.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cox.fit &amp;lt;- coxph(formula = Surv(time,status)~ sex + treatm,
                 data = data[,-1],
                 ties = &amp;quot;efron&amp;quot;)

cox.zph(cox.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            rho  chisq     p
## sex    -0.0227 0.0734 0.786
## treatm -0.0961 1.2965 0.255
## GLOBAL      NA 1.3270 0.515&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output above, the test is not statistically significant for each of the covariates, and the global test is also not statistically significant. Therefore, we can assume the proportional hazards. It’s possible to do a graphical diagnostic using the function &lt;code&gt;ggcoxzph&lt;/code&gt;, which produces, for each covariate, graphs of the scaled Schoenfeld residuals against the transformed time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggcoxzph(cox.zph(cox.fit))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, the solid line is a smoothing spline fit to the plot, with the dashed lines representing a +/- 2-standard-error band around the fit. Note that, systematic departures from a horizontal line are indicative of non-proportional hazards, since proportional hazards assumes that estimates do not vary much over time. From the graphical inspection, there is no pattern with time. The assumption of proportional hazards appears to be supported for the covariates sex (which is, recall, a two-level factor, accounting for the two bands in the graph), wt.loss and age.&lt;/p&gt;
&lt;p&gt;A violations of proportional hazards assumption can be resolved by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adding covariate*time interaction&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Stratification&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Stratification is useful for “nuisance” con founders, where you do not care to estimate the effect. You cannot examine the effects of the stratification variable (John Fox &amp;amp; Sanford Weisberg).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Influential Observations:&lt;/strong&gt; To test influential observations or outliers, we can visualize either the deviance residuals or the dfbeta values. The function &lt;code&gt;ggcoxdiagnostics&lt;/code&gt; provides a convenient solution for checking influential observations. The simplified format is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggcoxdiagnostics(cox.fit, type = &amp;quot;martingale&amp;quot; ,linear.predictions = F, ggtheme = ggplot2::theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Specifying the argument type = “dfbeta”, plots the estimated changes in the regression coefficients upon deleting each observation in turn; likewise, type=“dfbetas” produces the estimated changes in the coefficients divided by their standard errors.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggcoxdiagnostics(cox.fit, type = &amp;quot;dfbeta&amp;quot; ,linear.predictions = F, ggtheme = ggplot2::theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above index plots show that comparing the magnitudes of the largest dfbeta values to the regression coefficients suggests that none of the observations is terribly influential individually&lt;/p&gt;
&lt;p&gt;It’s also possible to check outliers by visualizing the deviance residuals. The deviance residual is a normalized transform of the martingale residual. These residuals should be roughly symmetrically distributed about zero with a standard deviation of 1. Positive values correspond to individuals that “died too soon” compared to expected survival times. Negative values correspond to individual that “lived too long”. Very large or small values are outliers, which are poorly predicted by the model. Example of deviance residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggcoxdiagnostics(cox.fit, type = &amp;quot;deviance&amp;quot; ,linear.predictions = F, ggtheme = ggplot2::theme_minimal())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Non-linearity:&lt;/strong&gt;Often, we assume that continuous covariates have a linear form. However, this assumption should be checked. Plotting the Martingale residuals against continuous covariates is a common approach used to detect nonlinearity or, in other words, to assess the functional form of a covariate. For a given continuous covariate, patterns in the plot may suggest that the variable is not properly fit. Nonlinearity is not an issue for categorical variables, so we only examine plots of martingale residuals and partial residuals against a continuous variable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Martingale residuals may present any value in the range (-INF, +1) and a value of martingale residuals near 1 represents individuals that “died too soon”, and large negative values correspond to individuals that “lived too long”. To assess the functional form of a continuous variable in a Cox proportional hazards model, we’ll use the function &lt;code&gt;ggcoxfunctional&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function displays graphs of continuous covariates against martingale residuals of null cox proportional hazards model. This might help to properly choose the functional form of continuous variable in the Cox model. Fitted lines with lowess function should be linear to satisfy the Cox proportional hazards model assumptions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survminer::ggcoxfunctional(formula = Surv(time,status)~ age + log(age) + sqrt(age), data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_test_split &amp;lt;- function(data,percent,seed){
  set.seed(seed)
  rows = sample(nrow(data))
  data &amp;lt;- data[rows,]
  split = round(nrow(data)*percent)
  list(train = data[1:split,], test =  data[(split+1):nrow(data),])
}

list &amp;lt;- train_test_split(data,0.6,66)
train &amp;lt;- list$train; test &amp;lt;- list$test

cox.fit2 &amp;lt;- coxph(formula = Surv(time,status)~.,data = train)

# predict the survival
predictions &amp;lt;- survfit(cox.fit2, newdata = test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-the-predictions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize the predictions&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# transponse the frame
survival.prediction &amp;lt;- data.frame(t(predictions$surv))
# add IDs
survival.prediction &amp;lt;- cbind(id = test$case,survival.prediction)
# rename column
colnames(survival.prediction) &amp;lt;- c(&amp;quot;id&amp;quot;,round(predictions$time,3))

survival.prediction %&amp;gt;%
        gather(key,value,-id) %&amp;gt;%
        mutate(key = as.numeric(key)) %&amp;gt;%
        ggplot(aes(x = key, y = value, group = id))+
        geom_line(alpha = 0.25)+
        theme_minimal()+
        labs(y = &amp;quot;Probability&amp;quot;,x = &amp;quot;Time&amp;quot;, title = &amp;quot;Survival vs Time&amp;quot;)+
        theme(axis.text.x = element_blank())+
        scale_x_continuous(breaks = c(0.203,3.608,0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using pec

predictions.pec &amp;lt;- pec::predictSurvProb(cox.fit2, newdata = test, times = km.fit$time)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Cumulative Hazard&lt;/strong&gt; at time &lt;em&gt;t&lt;/em&gt; can also be extracted using the &lt;code&gt;cumhaz&lt;/code&gt; component.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hazard.prediction &amp;lt;- predictions$cumhaz # hazard&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;validating-the-predicton&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Validating the predicton&lt;/h2&gt;
&lt;p&gt;A measure used to assess the strength of a risk classification system is discrimination, and when the outcome is survival time, the most commonly applied global measure of discrimination is the concordance probability. The concordance probability represents the pairwise probability of lower patient risk given longer survival time. It is the fraction of pairs in your data, where the observation with the higher survival time has the higher probability of survival predicted by your model.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Brier score&lt;/strong&gt; is a proper score function that measures the accuracy of probabilistic predictions (&lt;a href=&#34;https://en.wikipedia.org/wiki/Brier_score&#34;&gt;wikipedia&lt;/a&gt;). Evaluating the performance of risk prediction models in survival analysis. The Brier score is a weighted average of the squared distances between the observed survival status and the predicted survival probability of a model. Roughly the weights correspond to the probabilities of not being censored. The weights can be estimated depend on covariates. Prediction error curves are obtained when the Brier score is followed over time. Cross-validation based on bootstrap resampling or bootstrap subsampling can be applied to assess and compare the predictive power of various regression modelling strategies on the same set of data.. We can calculate the Brier Scores for our model and compare it against a reference model (without covariates) and compare the performance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pec)
library(prodlim)
library(riskRegression)
set.seed(17)

cindex &amp;lt;- cindex(list(cox.fit2),
                   formula = Hist(time,status) ~ 1,
                   data = data,
                   splitMethod=&amp;quot;bootcv&amp;quot;,
                   M=round(NROW(data)*.6),
                   B=100,
                   cens.model=&amp;quot;marginal&amp;quot;)

(cindex$BootCvCindex$coxph)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6258364&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Brier  &amp;lt;- pec(list(cox.fit2),
                   formula = Hist(time,status) ~ 1,
                   data = data,
                   splitMethod=&amp;quot;bootcv&amp;quot;,
                   M=round(NROW(data)*.6),
                   B=100,
                   cens.model=&amp;quot;marginal&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;reference &amp;lt;- Brier$BootCvErr$Reference
cox &amp;lt;- Brier$BootCvErr$coxph
err.frame &amp;lt;- data.frame(reference,cox,time=Brier$time)
err.frame &amp;lt;- err.frame[1:171,]
err.frame.long &amp;lt;- gather(err.frame,key,value,-time)
ggplot(err.frame.long, aes(x = time, y = value, col = key))+
  geom_line()+
  scale_color_manual(&amp;quot;Metric&amp;quot;,values =c(&amp;quot;cox&amp;quot;=&amp;quot;red2&amp;quot;,&amp;quot;reference&amp;quot;=&amp;quot;navyblue&amp;quot;))+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly the model with all covariates have lower error rates than our reference model with no covariates. One can also check the accuracy of such models by examining the area under the curve by using ROC style analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;additive-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Additive models&lt;/h1&gt;
&lt;p&gt;Additive hazard or Aalen’s models are an alternative to proportional hazard models. Here the hazard model is more directly similar to a linear model. Here the covariates are allowed to vary over time. We can also allow the covariates to vary over time.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \lambda(t|x(t)) = \beta_0(t) + \sum \beta_k(t)x_k(t)             \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can fit the additive model in R using the &lt;code&gt;aareg&lt;/code&gt; function. Then we can use &lt;code&gt;autoplot&lt;/code&gt; from ggfortify to investigate which coefficients may vary over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;additive &amp;lt;- aareg(Surv(time,status)~., data = data)

additive&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## aareg(formula = Surv(time, status) ~ ., data = data)
## 
##   n= 192 
##     124 out of 128 unique event times used
## 
##              slope      coef se(coef)      z        p
## Intercept -0.66200 -4.50e-03 6.56e-03 -0.686 4.93e-01
## case       0.00124  4.32e-06 1.38e-05  0.314 7.53e-01
## inst      -0.03510 -1.81e-04 4.72e-04 -0.383 7.02e-01
## sex       -0.31400 -3.08e-03 1.53e-03 -2.020 4.38e-02
## treatm     0.13700  1.19e-03 1.47e-03  0.812 4.17e-01
## grade     -0.19100 -1.70e-03 1.02e-03 -1.680 9.31e-02
## age       -0.00608 -1.64e-05 6.94e-05 -0.236 8.13e-01
## cond       2.59000  1.04e-02 2.39e-03  4.340 1.45e-05
## site      -0.03200 -3.25e-04 6.16e-04 -0.528 5.97e-01
## tstage     0.25100  2.16e-03 9.87e-04  2.180 2.89e-02
## nstage     0.22300  1.58e-03 6.89e-04  2.290 2.21e-02
## 
## Chisq=35.19 on 10 df, p=0.00012; test weights=aalen&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(additive)+
  theme_minimal()+
  theme(legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-14-death-note-part-i_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Sex might vary over time. This is evident from the steep slope in comparison with the rest.&lt;/p&gt;
&lt;p&gt;There are several other survival models for instance, accelerated failure time (AFT) model that model survival information using distributions. Part 2 of this tutorial is coming up next where I will demonstrate how to construct survival trees and forests.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Jack and Rose</title>
      <link>/post/jack-and-rose/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/jack-and-rose/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Jack and Rose from &lt;em&gt;Titanic&lt;/em&gt; epitomized the concept of romance in the late 1990s. If you watched this movie you would realize how Jack and Rose are &lt;strong&gt;dependent&lt;/strong&gt; on one another for surviving the sinking ship. Sometimes these dependencies can be represented by a straight line. This is what regression represents. The “best fit” regression line quantifies the relationship between two or more variables.&lt;/p&gt;
&lt;p&gt;Multiple regression is one of the oldest ML techniques. The simple yet formidable OLS estimation algorithm dates way back to the &lt;span class=&#34;math inline&#34;&gt;\(18^{th}\)&lt;/span&gt; century. The algorithm works by minimizing the sum of squares between the best fit line and the response variable. Today, the OLS regression is not the most accurate algorithm and therefore has been replaced by more sophisticated ML techniques. Yet, the weakness of this technique is also its strength. No other algorithm is as tractable as regression. Therefore, regression is almost always a starting point in any data analysis task.&lt;/p&gt;
&lt;p&gt;In this tutorial, I will implement this technique in R on a data set (link on this page) and discuss how to interpret the outputs. Finally I will use the famous &lt;strong&gt;Titanic&lt;/strong&gt; data set from Kaggle to demonstrate the steps one should take when implementing this technique for prediction purposes.&lt;/p&gt;
&lt;div id=&#34;get-the-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get the packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr) # read in a csv/txt file
library(MASS) # has the data set
library(dplyr) # for data manipulation
library(tidyr) # for data manipulation
library(ggplot2) # for data viz
library(purrr) # for functional programming&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;birthweights-and-babies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Birthweights and Babies&lt;/h1&gt;
&lt;p&gt;To demonstrate the key codes and concepts for regression I will use the &lt;code&gt;birthwt&lt;/code&gt; package from the &lt;code&gt;MASS&lt;/code&gt; package. The Birthwt data contains 189 observations, 16 predictors, and an outcome, birth weight, available both as a continuous measure and a binary indicator for low birth weight. The data were collected at Bay state Medical Center, Springfield, Mass during 1986.&lt;/p&gt;
&lt;p&gt;Let’s load the data set and examine the variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;birthwt&amp;quot;)
glimpse(birthwt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 189
## Variables: 10
## $ low   &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ age   &amp;lt;int&amp;gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, ...
## $ lwt   &amp;lt;int&amp;gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 15...
## $ race  &amp;lt;int&amp;gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3,...
## $ smoke &amp;lt;int&amp;gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,...
## $ ptl   &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...
## $ ht    &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,...
## $ ui    &amp;lt;int&amp;gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,...
## $ ftv   &amp;lt;int&amp;gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0,...
## $ bwt   &amp;lt;int&amp;gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 26...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(birthwt,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lwt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;race&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;smoke&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ptl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ht&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ui&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ftv&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;bwt&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;182&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2523&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;155&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2551&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;87&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2557&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;88&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2594&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2600&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;low:&lt;/strong&gt;Indicator of birth weight less than 2.5kg&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;age:&lt;/strong&gt;Mother’s age in years&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;lwt:&lt;/strong&gt;Mother’s weight in pounds at last menstrual period&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;race:&lt;/strong&gt;Indicator functions for mother’s race; “3” is reference group&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;smoke:&lt;/strong&gt;Smoking status during pregnancy&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ptl:&lt;/strong&gt;Indicator functions for one or for two or more previous premature labors&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ht:&lt;/strong&gt;History of hypertension&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ui:&lt;/strong&gt;Presence of uterine irritability&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ftv:&lt;/strong&gt;Indicator functions for one, for two, or for three or more physician visits during the first trimester, respectively. No visits is the reference category&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bwt:&lt;/strong&gt;Birth weight in kilograms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lets visualize the variable distributions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GGally::ggpairs(birthwt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;ggpairs&lt;/code&gt; function is a nice way to visualize the inter-relationships between variables. It allows for early detection of correlated variables and irregular distributions. In this data set I don’t see any variables that are extremely heavily correlated or any distributions that are severely skewed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple Linear Regression&lt;/h1&gt;
&lt;p&gt;Linear regression in R is performed with the &lt;code&gt;lm&lt;/code&gt; function. The set-up is very intuitive.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.lm &amp;lt;- lm(formula = bwt ~ lwt,  # regression formulae
             data = birthwt) # data set

# summarise the model
summary(sim.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ lwt, data = birthwt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2192.12  -497.97    -3.84   508.32  2075.60 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2369.624    228.493  10.371   &amp;lt;2e-16 ***
## lwt            4.429      1.713   2.585   0.0105 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 718.4 on 187 degrees of freedom
## Multiple R-squared:  0.0345, Adjusted R-squared:  0.02933 
## F-statistic: 6.681 on 1 and 187 DF,  p-value: 0.0105&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Inference&lt;/h2&gt;
&lt;p&gt;Often we are interested in finding the confidence interval for the coefficients of the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(sim.lm, level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   2.5 %     97.5 %
## (Intercept) 1918.867879 2820.37916
## lwt            1.048845    7.80937&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-linear-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple Linear Regression&lt;/h1&gt;
&lt;p&gt;The multiple linear regression set up is very similar to the simple regression. The predictors are connected with the &lt;code&gt;+&lt;/code&gt; operator within the formula argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutl.lm &amp;lt;- lm(formula = bwt ~ lwt + age + race + ftv,
              data = birthwt)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;interpret-the-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpret the output&lt;/h2&gt;
&lt;p&gt;The output of the lm function can be summarized with a &lt;code&gt;summary&lt;/code&gt; call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mutl.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ lwt + age + race + ftv, data = birthwt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2070.82  -458.44    21.08   526.18  1862.93 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2605.812    344.367   7.567 1.77e-12 ***
## lwt            3.604      1.755   2.054   0.0414 *  
## age            4.391     10.272   0.427   0.6696    
## race        -129.322     58.144  -2.224   0.0274 *  
## ftv            9.772     50.640   0.193   0.8472    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 713.3 on 184 degrees of freedom
## Multiple R-squared:  0.06349,    Adjusted R-squared:  0.04313 
## F-statistic: 3.118 on 4 and 184 DF,  p-value: 0.01638&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The five stat summary of the residuals are displayed. We see the minimum and maximum deviations are close to each other but are very large. This is an early indication of weak model performance in terms of fitting the best fit line.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;The coefficient table:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Estimate:&lt;/strong&gt; Gives the estimates of the coefficients of the predictors that when multiplied with the values of the predictors give the fitted values of response value. These are also called the &lt;em&gt;slopes&lt;/em&gt; while the first term is the &lt;em&gt;intercept&lt;/em&gt;. A &lt;em&gt;3.6&lt;/em&gt; coefficient for &lt;code&gt;lwt&lt;/code&gt; means for every one unit rise of lwt, birth weight would &lt;em&gt;increase&lt;/em&gt; by 3.6 units &lt;em&gt;on average keeping everything else constant&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Std. Error:&lt;/strong&gt; It is the standard error when computing the coefficients. The computation arises because the coefficients are essentially expected values for the predictors. The standard error therefore, tells us the variance or standard deviation when computing the expected values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;t value:&lt;/strong&gt; Indicates how far the coefficient estimate is from the ‘0’ on the standardized scale. The further we are the better. That would translate into a low p-value.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pr(&amp;gt;|t|):&lt;/strong&gt; It is the probability of observing a value larger than &lt;em&gt;t&lt;/em&gt;. A small p value indicates that it is unlikely that there is any relationship between predictor and response variable. The &lt;strong&gt;significance codes&lt;/strong&gt; at the bottom of the table indicates the level of significance in comparison to the standard cutoff of 5%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Residual Standard Error:&lt;/strong&gt; Roughly speaking, RSE is the measure of how far the fitted line will deviate from the fitted best fit regression line. Consequently, a smaller value is preferred.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R-square and Adjusted R-square:&lt;/strong&gt; It is measure of how well the variation in the response variables are modeled by the existing predictor variables. The adjusted version adjusts for the increase in the number of predictors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;F-statistic:&lt;/strong&gt; The F-stat is test statistic computed for measuring the overall stability of the model. The p-value next to the F-stat needs to be lower than the cutoff to ensure the model fit is legitimate.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;assumtions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Assumtions&lt;/h2&gt;
&lt;p&gt;While developing the least squares algorithm, the creators made several assumptions. We need to check for the correctness of these assumptions religiously every time we build a model especially when the aim is to predict. This also contributes to the disadvantages of using regression.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The parameters should be &lt;strong&gt;linear&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is true because our regression model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  birthweight = \beta_0 + \beta_1lwt + \beta_2age + \beta_3race + \beta_4ftv    \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;All dependent variables have singular powers.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Mean of residuals&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The mean of residuals should be close to zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mutl.lm$residuals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.723095e-14&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Equal variance&lt;/strong&gt; of residuals&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We need to check the residuals vs. fitted values for signs of patterns (specifically a “cone” shaped pattern)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- data.frame(residual = mutl.lm$residuals, fitted = mutl.lm$fitted.values)

ggplot(tmp, aes(x = fitted, y = residual))+
  geom_point(col = &amp;quot;#2E9FDF&amp;quot;)+
  labs(title = &amp;quot;Residuals vs Fitted Values&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Normality assumption&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All residuals should be normally distributed. Visually it can be checked using the Q-Q plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qqplot.data &amp;lt;- function (vec) # argument: vector of numbers
{
  # following four lines from base R&amp;#39;s qqline()
  y &amp;lt;- quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x &amp;lt;- qnorm(c(0.25, 0.75))
  slope &amp;lt;- diff(y)/diff(x)
  int &amp;lt;- y[1L] - slope * x[1L]

  d &amp;lt;- data.frame(resids = vec)

  ggplot(d, aes(sample = resids)) + 
    stat_qq() + 
    geom_abline(slope = slope, intercept = int) +
    labs(title = &amp;quot;Normal Q-Q plot&amp;quot;)+
    theme_minimal()

}

resid &amp;lt;- mutl.lm$residuals
qqplot.data(resid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of the residuals are on the straight line. This indicates normality is met.&lt;/p&gt;
&lt;p&gt;A test statistic that confirms normality is the Durbin Watson Test Stat.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::dwtest(mutl.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Durbin-Watson test
## 
## data:  mutl.lm
## DW = 0.28675, p-value &amp;lt; 2.2e-16
## alternative hypothesis: true autocorrelation is greater than 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing Models&lt;/h2&gt;
&lt;p&gt;Does addition of extra variables improve the model? This can be checked with an ANOVA test. The low p value indicates the second model is ‘significant’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(sim.lm,mutl.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Model 1: bwt ~ lwt
## Model 2: bwt ~ lwt + age + race + ftv
##   Res.Df      RSS Df Sum of Sq      F Pr(&amp;gt;F)
## 1    187 96521017                           
## 2    184 93622816  3   2898201 1.8986 0.1314&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-inflation-factor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance Inflation Factor&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;vif&lt;/code&gt; checks for multi-collinearity. MC can be a real villain for regressional fits. It could render variables to be insignificant and hamper the R-squared values. One way to counter VIFs are using the variables in an interaction terms. Informally speaking, this would also require extensive domain knowledge to understand why an interaction would be valid. A VIF around 1 is good while if it is greater than 10, it could indicates serious correlations between variable pairs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;car::vif(mutl.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      lwt      age     race      ftv 
## 1.063922 1.094536 1.053463 1.063170&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions&lt;/h2&gt;
&lt;p&gt;Here I demonstrate how interactions are included within the &lt;code&gt;lm&lt;/code&gt; framework. Let’s examine the possibility of interaction between ftv and age.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutl.lm.int &amp;lt;- lm(formula = bwt ~ lwt + age*ftv + race,  # also considers the variable by themselves alone
                  data = birthwt)

summary(mutl.lm.int)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ lwt + age * ftv + race, data = birthwt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2125.17  -488.93     7.98   499.74  1859.13 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2891.918    391.641   7.384 5.24e-12 ***
## lwt            3.635      1.749   2.078   0.0391 *  
## age           -8.439     13.282  -0.635   0.5260    
## ftv         -326.177    227.299  -1.435   0.1530    
## race        -128.740     57.942  -2.222   0.0275 *  
## age:ftv       13.923      9.185   1.516   0.1313    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 710.8 on 183 degrees of freedom
## Multiple R-squared:  0.0751, Adjusted R-squared:  0.04983 
## F-statistic: 2.972 on 5 and 183 DF,  p-value: 0.01324&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mutl.lm.int.2 &amp;lt;- lm(formula = bwt ~ lwt + age:ftv + race,  # only considers the interaction terms
                  data = birthwt)

summary(mutl.lm.int.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ lwt + age:ftv + race, data = birthwt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2082.65  -469.29    35.85   530.68  1927.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 2692.612    267.808  10.054   &amp;lt;2e-16 ***
## lwt            3.603      1.737   2.074   0.0395 *  
## race        -130.183     57.537  -2.263   0.0248 *  
## age:ftv        1.256      1.912   0.657   0.5119    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 711.1 on 185 degrees of freedom
## Multiple R-squared:  0.06435,    Adjusted R-squared:  0.04918 
## F-statistic: 4.241 on 3 and 185 DF,  p-value: 0.006311&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical Predictors&lt;/h2&gt;
&lt;p&gt;Sometimes the predictors are not continuous. They could be categorical or nominal variables. In R you would need to manually convert the class of the variables to &lt;code&gt;factor&lt;/code&gt; if they are already not.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Coerce the predictor into a vector of named (or unnamed factor)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make sure you convert it into a factor first
birthwt$smoke &amp;lt;- factor(birthwt$smoke)

mutl.lm.factor &amp;lt;- lm(formula = bwt ~ smoke,
                     data = birthwt)

summary(mutl.lm.factor)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ smoke, data = birthwt)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2062.9  -475.9    34.3   545.1  1934.3 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  3055.70      66.93  45.653  &amp;lt; 2e-16 ***
## smoke1       -283.78     106.97  -2.653  0.00867 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 717.8 on 187 degrees of freedom
## Multiple R-squared:  0.03627,    Adjusted R-squared:  0.03112 
## F-statistic: 7.038 on 1 and 187 DF,  p-value: 0.008667&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Care should be taken while interpreting a categorical variable. When they are binary for instance in the above example, when the value of &lt;code&gt;smoke&lt;/code&gt; is 1 the response will increase or decrease by an amount of (Intercept+Coefficient) &lt;em&gt;on average keeping everything else constant&lt;/em&gt;. On the other hand when &lt;code&gt;smoke&lt;/code&gt; is 0, the amount increase/decrease is recorded only by the (Intercept) term.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create dummy/indicator variables&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Lets create 3 columns: one an indicator for age less than 20, another for 21-29 age group and lastly another for greater than 30&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthwt &amp;lt;- mutate(birthwt, under.20 = ifelse(age &amp;lt; 21,1,0),
                           between.20.30 = ifelse(age &amp;lt; 30 &amp;amp; age &amp;gt; 20,1,0),
                           over.30 = ifelse(age &amp;gt; 29,1,0))

mutl.lm.indicator &amp;lt;- lm(formula = bwt ~ smoke + under.20 + between.20.30 + over.30,
                        data = birthwt)

summary(mutl.lm.indicator)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = bwt ~ smoke + under.20 + between.20.30 + over.30, 
##     data = birthwt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1981.23  -536.47    32.71   565.53  1701.46 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     3288.5      143.0  22.999  &amp;lt; 2e-16 ***
## smoke1          -288.2      106.8  -2.698  0.00762 ** 
## under.20        -215.1      162.3  -1.325  0.18672    
## between.20.30   -310.1      156.1  -1.986  0.04846 *  
## over.30             NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 714 on 185 degrees of freedom
## Multiple R-squared:  0.05666,    Adjusted R-squared:  0.04137 
## F-statistic: 3.704 on 3 and 185 DF,  p-value: 0.01274&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indicator variables have there own coefficients in the summary table.Clearly this is not a great example of indicator variables used in regression.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Logistic Regression&lt;/h1&gt;
&lt;p&gt;It is inappropriate to treat the categorical response (such as a binary response of yes/no) as continuous and perform multiple linear regression. Consider this plot where I treat &lt;code&gt;low&lt;/code&gt; as continuous and fit a best fit line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;birthwt %&amp;gt;%
  ggplot(aes(y = low,x = age))+
  geom_point(col = &amp;quot;#FC4E07&amp;quot;)+
  stat_smooth(method = &amp;quot;lm&amp;quot;, col = &amp;quot;black&amp;quot;,se=F)+
  theme_minimal()+
  labs(title = &amp;quot;Low Birthweight vs Age&amp;quot;, x = &amp;quot;Low&amp;quot;, y = &amp;quot;Age&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The best fit line would eventually fall below 0 as age would increase and would lead a to response (the value of &lt;code&gt;low&lt;/code&gt;) value lower than 0 which then becomes unrealistic. Logistic regression models is one class of models under ‘Generalized Linear Model’. Logistic models are called so because they transform the response through a &lt;strong&gt;logit&lt;/strong&gt; function. Logit belong to a vast array of &lt;em&gt;link&lt;/em&gt; functions that are different for different linear models.&lt;/p&gt;
&lt;p&gt;Logit is short for log of the odds. Mathematically this means:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  log\{\frac{\text{Pr} (y|x)}{1-\text{Pr}(y|x)}\} =  \beta_0 + \beta_1x \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Alternatively,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[    \frac{\text{Pr} (y|x)}{1-\text{Pr}(y|x)} = e^{\beta_0 + \beta_1x}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y = \frac{e^{\beta_0 + \beta_1x}}{1+e^{\beta_0 + \beta_1x}} \]&lt;/span&gt; This then becomes our response variable. The only drawback of using a logistic or other generalized linear models is when interpreting the coefficients of the predictors. We would have to adjust by transforming from the log scale. This sometimes makes interpretation complicated.&lt;/p&gt;
&lt;div id=&#34;fitting-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the model&lt;/h2&gt;
&lt;p&gt;We use the &lt;code&gt;glm&lt;/code&gt; function to fit a logistic regression model. Notice the family is specified as binomial. This reflects the binary nature of the response variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bwt &amp;lt;- birthwt[,-11:-13]
logistic.lm &amp;lt;- glm(low ~ ., family = binomial, data = bwt)
summary(logistic.lm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = low ~ ., family = binomial, data = bwt)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.890e-04  -2.100e-08  -2.100e-08   2.100e-08   1.593e-04  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)
## (Intercept)  1.161e+03  2.074e+05   0.006    0.996
## age          3.223e-01  1.787e+03   0.000    1.000
## lwt         -1.733e-01  3.202e+02  -0.001    1.000
## race         6.494e-01  3.165e+04   0.000    1.000
## smoke1      -1.746e+01  7.668e+04   0.000    1.000
## ptl          1.267e+02  3.406e+05   0.000    1.000
## ht           3.636e+01  1.237e+05   0.000    1.000
## ui          -6.183e+01  7.547e+04  -0.001    0.999
## ftv         -8.925e+00  1.624e+04  -0.001    1.000
## bwt         -4.466e-01  6.468e+01  -0.007    0.994
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2.3467e+02  on 188  degrees of freedom
## Residual deviance: 1.0537e-07  on 179  degrees of freedom
## AIC: 20
## 
## Number of Fisher Scoring iterations: 25&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-the-coefficients&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpreting the coefficients&lt;/h2&gt;
&lt;p&gt;The summary table is very similar to the multiple regression summary table. However you should be careful while interpreting the coefficients. You have to remember they are in logit scale. You can rescale them using a function as I did here and then interpret that value as a percent increase or decrease. For instance I will rescale the age coefficent:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logit.rescale &amp;lt;- function(x){
  (1 - (exp(x)/(1+exp(x))))*100
}

(logit.rescale(3.223e-01))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42.01153&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This means for every unit increase in age, the chances of low birthweight increase by 42%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variable-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variable Selection&lt;/h2&gt;
&lt;p&gt;Before moving onto how to predict using a real life data set, its worth understanding how to select variables when there are a large list of variables to be considered for a model fit. The concept of variable selection is connected to the &lt;em&gt;principle of parsimony&lt;/em&gt; in statistics. The principle advocates the use of a simpler model (with fewer predictor vars) when fitting or predicting. In practice this is sometimes useful because it reduces the burden of interpreting multiple variables. It is especially useful when a simpler model is more accurate in prediction than a complex model. There are number of strategies for selecting the ‘best’ model. The first one is using ANOVA after manually building the models. The next two strategies uses R to automate the iteration over multiple models and helps identify the best model.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;All possible regression model&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As the name suggests this technique considers all possible combination of predictors while building the model. Then we use ceratin statistics such as the Bayesian Info Criteria or Residual Sum of Squares to decide on which predictors to keep and which ones to reject.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.subset &amp;lt;- leaps::regsubsets(low ~.,
                                data = bwt,
                                nbest = 20) # number of subsets to consider

all.subset.summary &amp;lt;- summary(all.subset) # store the summary into an object

tmp &amp;lt;- data.frame(all.subset.summary$which,
                        all.subset.summary[c(&amp;quot;rss&amp;quot;,&amp;quot;cp&amp;quot;,&amp;quot;bic&amp;quot;)],
                                                    id = 1:nrow(all.subset.summary$which))
colnames(tmp)[1] &amp;lt;- &amp;quot;Intercept&amp;quot;

tmp %&amp;gt;%
    gather(variable, present, -rss, -cp, -bic, -id) %&amp;gt;% 
    gather(type, value, -id, -variable, -present)%&amp;gt;%
    ggplot(aes(variable, factor(round(value)))) +
      geom_tile(aes(fill = present),col = &amp;quot;black&amp;quot;) +
      facet_wrap(~ type, scales = &amp;quot;free&amp;quot;) +
      scale_fill_manual(&amp;quot;&amp;quot;, values = c(&amp;quot;TRUE&amp;quot; = &amp;quot;#FC4E07&amp;quot;, &amp;quot;FALSE&amp;quot; = &amp;quot;white&amp;quot;), guide = FALSE) +
      theme_minimal()+
      labs(x = &amp;quot;&amp;quot;, y = &amp;quot;&amp;quot;)+
      theme(axis.text.x = element_text(angle = 90))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can summarize the results of the procedure with the picture above. We could any one of the plots above to decide an appropriate model to use. For instance if we look at the RSS facet, we can see for a particular value of RSS which variables are included. The lowest RSS value of 33, has all variables except race and the first level of ftv.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Automated stepwise regression&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This school of variable selection is based on taking ‘steps’ towards a better model. We start with a basic model (one with just the intercept term) and slowly build to the full model with all covariates. At each step, a predictor is added and checked if it is significant in the model and if it betters a statistic (such as BIC/AIC). If it does, then the variable is introduced in the model else it is rejected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;null.model &amp;lt;- glm(low ~ 1, family = binomial, data = bwt)
full.model &amp;lt;- logistic.lm

# forward regression
step(null.model, scope=list(lower=null.model, upper=full.model), direction=&amp;quot;forward&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Start:  AIC=236.67
## low ~ 1
## 
##         Df Deviance    AIC
## + bwt    1     0.00   4.00
## + ptl    1   227.89 231.89
## + lwt    1   228.69 232.69
## + ui     1   229.60 233.60
## + smoke  1   229.81 233.81
## + ht     1   230.65 234.65
## + race   1   231.10 235.10
## + age    1   231.91 235.91
## &amp;lt;none&amp;gt;       234.67 236.67
## + ftv    1   233.90 237.90
## 
## Step:  AIC=4
## low ~ bwt
## 
##         Df   Deviance AIC
## &amp;lt;none&amp;gt;     4.9323e-07   4
## + ui     1 1.7864e-07   6
## + lwt    1 2.3991e-07   6
## + ptl    1 4.9184e-07   6
## + ht     1 4.9216e-07   6
## + smoke  1 4.9276e-07   6
## + race   1 4.9415e-07   6
## + ftv    1 4.9577e-07   6
## + age    1 4.9704e-07   6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:  glm(formula = low ~ bwt, family = binomial, data = bwt)
## 
## Coefficients:
## (Intercept)          bwt  
##    2975.975       -1.186  
## 
## Degrees of Freedom: 188 Total (i.e. Null);  187 Residual
## Null Deviance:       234.7 
## Residual Deviance: 4.932e-07     AIC: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The other option is to go &lt;code&gt;backward&lt;/code&gt; from the full model to the simple model and reverse the selection strategy we discussed before. We could also go in either direction, using the option &lt;code&gt;both&lt;/code&gt; in the &lt;code&gt;step&lt;/code&gt; function above which mixes up the forward and backward step. It slows down the process although sometimes comes up with better answers because it considers more permutations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Prediction&lt;/h1&gt;
&lt;p&gt;We are finally ready to apply the steps above in an actual example. We will use the very popular &lt;code&gt;Titanic&lt;/code&gt; data set from &lt;a href=&#34;https://www.kaggle.com/c/titanic&#34;&gt;Kaggle&lt;/a&gt;. I will just use the training data to further show the preprocessing steps before prediction.&lt;/p&gt;
&lt;p&gt;The sinking of the &lt;a href=&#34;https://en.wikipedia.org/wiki/RMS_Titanic&#34;&gt;RMS Titanic&lt;/a&gt; is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. In the iconic movie &lt;em&gt;Titanic,(1997)&lt;/em&gt;, the creaters depict how different factors (such the passenger class) decide who gets on the life boats. It painted a horrific and emotional picture of how the human lives depended on the wealth and social status of the passengers.&lt;/p&gt;
&lt;p&gt;One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.&lt;/p&gt;
&lt;p&gt;The variables in the data set are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survival:&lt;/strong&gt; Survival 0 = No, 1 = Yes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pclass:&lt;/strong&gt; Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sex:&lt;/strong&gt; Sex of passesnger&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Age:&lt;/strong&gt; Age in years&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sibsp:&lt;/strong&gt; number of siblings / spouses aboard the Titanic&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;parch:&lt;/strong&gt; number of parents / children aboard the Titanic&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ticket:&lt;/strong&gt; Ticket number&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;fare:&lt;/strong&gt; Passenger fare&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cabin:&lt;/strong&gt; Cabin number&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;embarked:&lt;/strong&gt; Port of Embarkation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The challenge is the build a logitic regression model to predict the survival status of individuals aboard Titanic that fateful night.&lt;/p&gt;
&lt;div id=&#34;preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocessing&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Load the data&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Regression/train.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Analyze the structure&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 891
## Variables: 12
## $ PassengerId &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,...
## $ Survived    &amp;lt;int&amp;gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,...
## $ Pclass      &amp;lt;int&amp;gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3,...
## $ Name        &amp;lt;chr&amp;gt; &amp;quot;Braund, Mr. Owen Harris&amp;quot;, &amp;quot;Cumings, Mrs. John Bra...
## $ Sex         &amp;lt;chr&amp;gt; &amp;quot;male&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;male&amp;quot;, &amp;quot;mal...
## $ Age         &amp;lt;dbl&amp;gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, ...
## $ SibSp       &amp;lt;int&amp;gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4,...
## $ Parch       &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1,...
## $ Ticket      &amp;lt;chr&amp;gt; &amp;quot;A/5 21171&amp;quot;, &amp;quot;PC 17599&amp;quot;, &amp;quot;STON/O2. 3101282&amp;quot;, &amp;quot;1138...
## $ Fare        &amp;lt;dbl&amp;gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, ...
## $ Cabin       &amp;lt;chr&amp;gt; NA, &amp;quot;C85&amp;quot;, NA, &amp;quot;C123&amp;quot;, NA, NA, &amp;quot;E46&amp;quot;, NA, NA, NA, ...
## $ Embarked    &amp;lt;chr&amp;gt; &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Q&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;C&amp;quot;, ...&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Look at the summary of the data set&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PassengerId       Survived          Pclass          Name          
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  
##  Median :446.0   Median :0.0000   Median :3.000   Mode  :character  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309                     
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000                     
##                                                                     
##      Sex                 Age            SibSp           Parch       
##  Length:891         Min.   : 0.42   Min.   :0.000   Min.   :0.0000  
##  Class :character   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  
##  Mode  :character   Median :28.00   Median :0.000   Median :0.0000  
##                     Mean   :29.70   Mean   :0.523   Mean   :0.3816  
##                     3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  
##                     Max.   :80.00   Max.   :8.000   Max.   :6.0000  
##                     NA&amp;#39;s   :177                                     
##     Ticket               Fare           Cabin             Embarked        
##  Length:891         Min.   :  0.00   Length:891         Length:891        
##  Class :character   1st Qu.:  7.91   Class :character   Class :character  
##  Mode  :character   Median : 14.45   Mode  :character   Mode  :character  
##                     Mean   : 32.20                                        
##                     3rd Qu.: 31.00                                        
##                     Max.   :512.33                                        
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would need to rectify the classes for a few variable to reflect their ordinal or nominal nature more appropriately.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Variable transformations and Imputations&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;First we transform some of the variable types from numeric to factor to reflect their binary/multinomial nature.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% dplyr::select(PassengerId,Survived,Pclass,
                                   Age,SibSp,Parch,Embarked,Fare,Sex)%&amp;gt;%
                      mutate(PassengerId = as.character(PassengerId),
                          Survived = factor(Survived),
                          Pclass = factor(Pclass),
                          Sex = factor(Sex),
                          Age = as.numeric(Age),
                          SibSp = factor(SibSp),
                          Parch = factor(Parch),
                          Embarked = factor(Embarked))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;We check for missing values&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colSums(is.na(data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PassengerId    Survived      Pclass         Age       SibSp       Parch 
##           0           0           0         177           0           0 
##    Embarked        Fare         Sex 
##           2           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Impute missing Age with median and then omit the 2 observations of embarked&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% 
     mutate_at(vars(Age), ~ifelse(is.na(.), median(., na.rm = TRUE), .)) %&amp;gt;%
     na.omit()

colSums(is.na(data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## PassengerId    Survived      Pclass         Age       SibSp       Parch 
##           0           0           0           0           0           0 
##    Embarked        Fare         Sex 
##           0           0           0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize distributions&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

g1 &amp;lt;- data %&amp;gt;%
      group_by(Survived)%&amp;gt;%
      summarise(count = n())%&amp;gt;%
      ggplot(aes(x = Survived, y = count, fill = Survived))+
      geom_bar(stat = &amp;quot;identity&amp;quot;,col = &amp;quot;black&amp;quot;)+
      scale_fill_manual(values = my2cols)+
      theme_minimal()+
      labs(title = &amp;quot;Survived Proportions&amp;quot;)

g2 &amp;lt;- data %&amp;gt;%
      group_by(Pclass)%&amp;gt;%
      summarise(count = n())%&amp;gt;%
      ggplot(aes(x = Pclass, y = count, fill = Pclass))+
      geom_bar(stat = &amp;quot;identity&amp;quot;,col = &amp;quot;black&amp;quot;)+
      scale_fill_manual(values = my3cols)+
      theme_minimal()+
      labs(title = &amp;quot;Class Distribution&amp;quot;)

g3 &amp;lt;- data %&amp;gt;%
      group_by(SibSp)%&amp;gt;%
      summarise(count = n())%&amp;gt;%
      ggplot(aes(x = SibSp, y = count, fill = SibSp))+
      geom_bar(stat = &amp;quot;identity&amp;quot;,col = &amp;quot;black&amp;quot;)+
      scale_fill_brewer(palette = &amp;quot;Set2&amp;quot;)+
      theme_minimal()+
      labs(title = &amp;quot;Siblings Distribution&amp;quot;)

g4 &amp;lt;- data %&amp;gt;%
      group_by(Sex)%&amp;gt;%
      summarise(count = n())%&amp;gt;%
      ggplot(aes(x = Sex, y = count, fill = Sex))+
      geom_bar(stat = &amp;quot;identity&amp;quot;,col = &amp;quot;black&amp;quot;)+
      scale_fill_manual(values = my3cols)+
      theme_minimal()+
      labs(title = &amp;quot;Sex Distribution&amp;quot;)

g5 &amp;lt;- data %&amp;gt;%
      ggplot(aes(x = Age))+
      geom_density(fill = &amp;quot;#FC4E07&amp;quot;, col = &amp;quot;black&amp;quot;)+
      scale_fill_manual(values = my3cols)+
      theme_minimal()+
      labs(title = &amp;quot;Age distribution&amp;quot;)

g6 &amp;lt;- data %&amp;gt;%
      ggplot(aes(Age,Fare,col = Pclass ,shape = Sex))+
      geom_point()+
      scale_color_manual(values = my3cols)+
      theme_minimal()+
      labs(title = &amp;quot;Fare vs Age&amp;quot;)+
      coord_cartesian(ylim = c(0,300))

gridExtra::grid.arrange(g1,g2,g3,g4,g5,g6, ncol = 3, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomize-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Randomize the dataset&lt;/h2&gt;
&lt;p&gt;Randomizing the data set is important to make sure its free from bias when building the model. This is achieved by randomizing the row numbers of the data set and then ordering the original data by the random index. Randomizing leads to bias free results and accurately analyze the effect of predictors on response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for reproducibility
set.seed(42)
# obtain a random sample of row ids
rows &amp;lt;- sample(nrow(data))
# order the data set with the vector above
data &amp;lt;- data[rows,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-into-train-and-test-lets-try-a-7030-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split into train and test: Lets try a 70:30 split&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a split point
split &amp;lt;- round(nrow(data)*0.7)
# train
train &amp;lt;- data[1:split,]
# test
test &amp;lt;- data[(split+1):nrow(data),]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-on-train&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit the model on train&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit &amp;lt;- glm(Survived ~.,
               data = train[,-1], family = &amp;quot;binomial&amp;quot;)

summary(fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Survived ~ ., family = &amp;quot;binomial&amp;quot;, data = train[, 
##     -1])
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.7358  -0.6141  -0.4212   0.5908   2.3965  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  3.876e+00  5.689e-01   6.813 9.54e-12 ***
## Pclass2     -1.131e+00  3.499e-01  -3.232 0.001229 ** 
## Pclass3     -2.136e+00  3.435e-01  -6.219 5.00e-10 ***
## Age         -3.350e-02  9.780e-03  -3.425 0.000614 ***
## SibSp1      -1.145e-01  2.748e-01  -0.417 0.676960    
## SibSp2      -4.409e-01  6.744e-01  -0.654 0.513272    
## SibSp3      -2.238e+00  8.095e-01  -2.765 0.005695 ** 
## SibSp4      -2.554e+00  1.164e+00  -2.193 0.028279 *  
## SibSp5      -1.523e+01  1.380e+03  -0.011 0.991197    
## SibSp8      -1.614e+01  8.924e+02  -0.018 0.985569    
## Parch1       6.015e-01  3.672e-01   1.638 0.101374    
## Parch2       2.214e-01  4.541e-01   0.488 0.625867    
## Parch3      -6.360e-01  1.461e+00  -0.435 0.663291    
## Parch4      -1.635e+01  1.300e+03  -0.013 0.989961    
## Parch5      -1.560e+01  1.408e+03  -0.011 0.991161    
## Parch6      -1.648e+01  2.400e+03  -0.007 0.994520    
## EmbarkedQ    3.681e-01  4.705e-01   0.782 0.433935    
## EmbarkedS   -3.035e-01  2.860e-01  -1.061 0.288706    
## Fare         6.962e-04  2.591e-03   0.269 0.788149    
## Sexmale     -2.748e+00  2.504e-01 -10.975  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 831.40  on 621  degrees of freedom
## Residual deviance: 528.36  on 602  degrees of freedom
## AIC: 568.36
## 
## Number of Fisher Scoring iterations: 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, sex is significant in predicting survival. Females we given priority during the rescue attempts. The class of passenger and the age were also significant. Older individuals and first class passengers were given priority over the other classes.&lt;/p&gt;
&lt;p&gt;One could also use the &lt;em&gt;stepwise&lt;/em&gt; regression procedure for variable selection before moving onto predictions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predict-with-the-model-above&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predict with the model above&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict.glm(fit, newdata = test, type = c(&amp;quot;response&amp;quot;))

# visualise the predictions

predict.df &amp;lt;- data.frame(
      PassengerId = test$PassengerId,
      predictions = predictions,
      predict.status = ifelse(predictions &amp;gt; 0.5,1,0),
      actual.status = test$Survived
)

predict.df %&amp;gt;% 
   left_join(test,&amp;quot;PassengerId&amp;quot;) %&amp;gt;%
   ggplot(aes(x = Age, y = predictions, col = Pclass))+
   geom_point()+
   facet_grid(.~Sex)+
   labs(y = &amp;quot;Predictions&amp;quot;, title = &amp;quot;Visualizing Predictions&amp;quot;)+
   scale_color_manual(values = my3cols)+
   theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Female predicted survival rates are higher than male. Within each sex, the elderly probably died more when Titanic sank. But in general as age increased the predicted survival probability fell. Finally class 1 have higher survival rates. They were probably first to be rescued followed by 2 and then 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-a-naive-classification-boundary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using a naive classification boundary&lt;/h2&gt;
&lt;p&gt;Next, we use a naive decision rule to classify the predicted probability. Lets use 0.5 as the cutoff value. The 2x2 confusion matrix and the accuracy value is computed below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- with(predict.df,table(predict.status,actual.status))
(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               actual.status
## predict.status   0   1
##              0 137  27
##              1  32  71&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;#39;Accuracy:&amp;#39;,round((t[1]+t[4])/(t[1]+t[2]+t[3]+t[4]),2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy: 0.78&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-charts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC charts&lt;/h2&gt;
&lt;p&gt;A ROC chart is useful when finding the balance between true positive and true negative is manually tricky. The ROC plots all the values for true positive and true negative for different cutoff points. A weak classifier will have the ROC line passing diagonally across the plot whereas a strong classifier will have a prominent ‘elbow’ with at least one point where the TPR is sufficiently high and the FPR will be sufficiently low.&lt;/p&gt;
&lt;p&gt;The following function (adapted from this &lt;a href=&#34;https://www.r-bloggers.com/simple-roc-plots-with-ggplot2-part-1/&#34;&gt;link&lt;/a&gt;) calculates the AUC (Area under curve) as well as the ROC data which can then be used inside the ggplot2 framework.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rocdata &amp;lt;- function(grp, pred){
  # Produces x and y co-ordinates for ROC curve plot
  # Arguments: grp - labels classifying subject status
  #            pred - values of each observation
  # Output: List with 2 components:
  #         roc = data.frame with x and y co-ordinates of plot
  #         stats = data.frame containing: area under ROC curve, p value, upper and lower 95% confidence interval
 
  grp &amp;lt;- as.factor(grp)
  if (length(pred) != length(grp)) {
    stop(&amp;quot;The number of classifiers must match the number of data points&amp;quot;)
  } 
 
  if (length(levels(grp)) != 2) {
    stop(&amp;quot;There must only be 2 values for the classifier&amp;quot;)
  }
 
  cut &amp;lt;- unique(pred)
  tp &amp;lt;- sapply(cut, function(x) length(which(pred &amp;gt; x &amp;amp; grp == levels(grp)[2])))
  fn &amp;lt;- sapply(cut, function(x) length(which(pred &amp;lt; x &amp;amp; grp == levels(grp)[2])))
  fp &amp;lt;- sapply(cut, function(x) length(which(pred &amp;gt; x &amp;amp; grp == levels(grp)[1])))
  tn &amp;lt;- sapply(cut, function(x) length(which(pred &amp;lt; x &amp;amp; grp == levels(grp)[1])))
  tpr &amp;lt;- tp / (tp + fn)
  fpr &amp;lt;- fp / (fp + tn)
  roc = data.frame(x = fpr, y = tpr)
  roc &amp;lt;- roc[order(roc$x, roc$y),]
 
  i &amp;lt;- 2:nrow(roc)
  auc &amp;lt;- (roc$x[i] - roc$x[i - 1]) %*% (roc$y[i] + roc$y[i - 1])/2
 
  pos &amp;lt;- pred[grp == levels(grp)[2]]
  neg &amp;lt;- pred[grp == levels(grp)[1]]
  q1 &amp;lt;- auc/(2-auc)
  q2 &amp;lt;- (2*auc^2)/(1+auc)
  se.auc &amp;lt;- sqrt(((auc * (1 - auc)) + ((length(pos) -1)*(q1 - auc^2)) + ((length(neg) -1)*(q2 - auc^2)))/(length(pos)*length(neg)))
  ci.upper &amp;lt;- auc + (se.auc * 0.96)
  ci.lower &amp;lt;- auc - (se.auc * 0.96)
 
  se.auc.null &amp;lt;- sqrt((1 + length(pos) + length(neg))/(12*length(pos)*length(neg)))
  z &amp;lt;- (auc - 0.5)/se.auc.null
  p &amp;lt;- 2*pnorm(-abs(z))
 
  stats &amp;lt;- data.frame (auc = auc,
                       p.value = p,
                       ci.upper = ci.upper,
                       ci.lower = ci.lower
                       )
 
  return (list(roc = roc, stats = stats))
}

# Get ROC data

obj.roc &amp;lt;- rocdata(grp = predict.df$actual.status, pred = predict.df$predictions)$roc
roc.data &amp;lt;- data.frame(false_positive = obj.roc$x, true_positive = obj.roc$y,cutoff = round(unique(predictions),4))

# create the ROC chart

ggplot(roc.data,aes(x = false_positive, y = true_positive))+
  geom_point()+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-jack-and-rose_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We notice in the ROC plot above that a cutoff point of 0.6 gives us a around 79% accuracy. This can be verified as below&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc.data[roc.data$cutoff == 0.5937,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     false_positive true_positive cutoff
## 128      0.2916667     0.8265306 0.5937&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict.df &amp;lt;- data.frame(
      predictions = predictions,
      predict.status = ifelse(predictions &amp;gt; 0.6,1,0),
      actual.status = test$Survived
)


t &amp;lt;- with(predict.df,table(predict.status,actual.status))
(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               actual.status
## predict.status   0   1
##              0 144  31
##              1  25  67&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;#39;Accuracy:&amp;#39;,round((t[1]+t[4])/(t[1]+t[2]+t[3]+t[4]),3)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy: 0.79&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is how a ROC chart helps us understand the trade off between true positive and true negative and come up with better cutoff points.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiperiod Optimization with Gurobi</title>
      <link>/post/multiperiod-optimization-with-gurobi/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/multiperiod-optimization-with-gurobi/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;This is a simple example of multiperiod programming. The question was also derived from &lt;strong&gt;Optimization in Operations Research&lt;/strong&gt; by &lt;em&gt;Rardin&lt;/em&gt;. The question reads:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ace Green Windows (AGW) manufactures environmentally efficient windows as replacements of those in existing homes. It has just received a contract for the next 6 months, requiring 100, 250, 190, 140, 220, and 110 units in months 1 through 6, respectively. Production costs for windows vary with time due to the specialized materials. AGW estimates production will cost&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\$250\)&lt;/span&gt;, $ $450,$350, $400, $520$, and &lt;span class=&#34;math inline&#34;&gt;\(\$500\)&lt;/span&gt; &lt;em&gt;per unit in periods 1 through 6, respectively. To take advantage of cost variations, AGW may produce more windows than the contract requires in some months and hold up to 375 of them in inventory for later months. But holding inventory costs $30 per window per month. Assume there is no beginning inventory.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For simplicity let’s assume that the months are from January to June. This creates the dimension of the problem. Let it be indexed by . Let the requirements for each month be denoted by &lt;span class=&#34;math inline&#34;&gt;\(d_t\)&lt;/span&gt;. Let the cost of production for each month be denoted by &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The time-phased linear optimization model can be written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{t = Jan}^{Jun} p_t x_t + 30 \sum_{t = Jan}^{Jun} h_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subject to:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ h_{t-1} + x_t = d_t + h_t \quad \forall \quad t = 1,..6;  ....(1) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ h_t \leq 375 \quad \forall \quad t = 1,..6; ....(2)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ h_t, x_t \geq 0 \quad ....(3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first constraint defines the relationship between closing inventory from last period, current production and the current inventory with the present demand. The second constraint follows from the instruction in the question allowing an upper bound on the holding inventory to go beyond the required for every month. Lastly, we assume both are non negative.&lt;/p&gt;
&lt;p&gt;We should take care while interpreting the first constraint in the opening and closing months. For January &lt;span class=&#34;math inline&#34;&gt;\(h_{t-1}\)&lt;/span&gt; is just 0 while for the closing month in June, the &lt;span class=&#34;math inline&#34;&gt;\(h_t\)&lt;/span&gt; is zero thereby reassuring the remaining inventory from May and current production from June equals current requirement for June and there are no left-over windows.&lt;/p&gt;
&lt;p&gt;Here’s the solution in Python:&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from gurobipy import *
m = Model(&amp;#39;Window Inventory&amp;#39;)
#create dimensions of the problem
calender = [&amp;#39;Jan&amp;#39;,&amp;#39;Feb&amp;#39;,&amp;#39;Mar&amp;#39;,&amp;#39;Apr&amp;#39;,&amp;#39;May&amp;#39;,&amp;#39;Jun&amp;#39;] #will be indexed by t
#monthly requirements
requirements = {&amp;#39;Jan&amp;#39;:100,
                &amp;#39;Feb&amp;#39;:250,
                &amp;#39;Mar&amp;#39;:190,
                &amp;#39;Apr&amp;#39;:140,
                &amp;#39;May&amp;#39;:220,
                &amp;#39;Jun&amp;#39;:110}
# production costs
prod_cost =    {&amp;#39;Jan&amp;#39;:250,
                &amp;#39;Feb&amp;#39;:450,
                &amp;#39;Mar&amp;#39;:350,
                &amp;#39;Apr&amp;#39;:400,
                &amp;#39;May&amp;#39;:520,
                &amp;#39;Jun&amp;#39;:500}
                
                
#create decision variables
#units produced in month t
produced = {}
for t in calender:
    produced[t] = m.addVar(lb = 0, vtype = GRB.CONTINUOUS, name = &amp;#39;produced_%s&amp;#39;%(t))
# units held at time t
held = {}
for t in calender:
    held[t] = m.addVar(lb = 0, ub = 375, vtype = GRB.CONTINUOUS, name = &amp;#39;held_%s&amp;#39;%(t))
    
m.update()
#set the objective
obj1 = quicksum( produced[t]*prod_cost[t] for t in calender )
obj2 = 30*quicksum( held[t] for t in calender )
obj = obj1 + obj2
m.setObjective(obj)
#define constraints
#beginning inventory + production amount - ending inventory = demand
#begining inventory + production amount = demand + ending inventory
#initial balance
m.addConstr(produced[&amp;#39;Jan&amp;#39;] == requirements[&amp;#39;Jan&amp;#39;]+held[&amp;#39;Jan&amp;#39;], &amp;#39;inital&amp;#39;)
#closing balance
m.addConstr(produced[&amp;#39;Jun&amp;#39;]+held[&amp;#39;May&amp;#39;] == requirements[&amp;#39;Jun&amp;#39;], &amp;#39;closing&amp;#39;)
# remaining balances
for t in calender:
    if t != calender[0] and t != calender[5]:
        m.addConstr(held[calender[calender.index(t) -1]]+produced[t] == requirements[t]+held[t], &amp;#39;remaining&amp;#39;)
        
m.optimize()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Optimize a model with 6 rows, 12 columns and 16 nonzeros
## Coefficient statistics:
##   Matrix range     [1e+00, 1e+00]
##   Objective range  [3e+01, 5e+02]
##   Bounds range     [4e+02, 4e+02]
##   RHS range        [1e+02, 3e+02]
## Presolve removed 2 rows and 5 columns
## Presolve time: 0.06s
## Presolved: 4 rows, 7 columns, 10 nonzeros
## 
## Iteration    Objective       Primal Inf.    Dual Inf.      Time
##        0    3.8200000e+04   1.137500e+02   0.000000e+00      0s
##        4    3.5020000e+05   0.000000e+00   0.000000e+00      0s
## 
## Solved in 4 iterations and 0.11 seconds
## Optimal objective  3.502000000e+05&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;if m.status == GRB.status.OPTIMAL:
    print(&amp;#39;\nMinimum objective is :&amp;#39;,m.objval)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Minimum objective is : 350200.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;if m.status == GRB.status.OPTIMAL:
    print (&amp;#39;\nOptimal inventory :&amp;#39;)
    for t in calender:
        print(&amp;#39;produced at &amp;#39;+ str(t) + &amp;#39;: &amp;#39; + str(produced[t].x))
        print(&amp;#39;held at &amp;#39; + str(t) + &amp;#39;: &amp;#39; + str(held[t].x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Optimal inventory :
## produced at Jan: 475.0
## held at Jan: 375.0
## produced at Feb: 0.0
## held at Feb: 125.0
## produced at Mar: 440.0
## held at Mar: 375.0
## produced at Apr: 95.0
## held at Apr: 330.0
## produced at May: 0.0
## held at May: 110.0
## produced at Jun: 0.0
## held at Jun: 0.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output gives the optimal or minimal cost of inventory and production at &lt;span class=&#34;math inline&#34;&gt;\(\$ 350200\)&lt;/span&gt;. The company does make use of the variable production cost specifically in the months of January and March when they also hold the maximum specified inventory. This allows the company to store more and produce less (almost 0) during the ending months when the production costs are higher.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Of Fortune Tellers and Crystal Balls</title>
      <link>/post/of-fortune-tellers-and-crystal-balls/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/of-fortune-tellers-and-crystal-balls/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Fortune telling is an ancient form of magic (not dark) and is art of predicting a person’s future. Fortune tellers have crystal balls that help them delve into the future. (Un)Fortunately, the plethora of skeptics don’t believe the practice simply because it isn’t based on scientific facts. But what if I told you that fortune telling is all math? Don’t believe me? If you read this chapter you’ll realize why the practice of foreseeing the future isn’t exactly a fairy tale.&lt;/p&gt;
&lt;div id=&#34;load-the-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Load the packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr) # to read in data
library(readxl) # to read in the data
library(dplyr) # for data manipulation
library(tidyr) # for cleaning up data
library(ggplot2) # for data viz
library(fpp2) # time series 
library(forecast) # time series&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;time-series-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time series Data&lt;/h1&gt;
&lt;p&gt;Time series data are a sequence of correlated points (denoted by &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;) indexed by uniformly separated elements of time. Therefore, time series data represents a &lt;a href=&#34;https://en.wikipedia.org/wiki/Stochastic_process&#34;&gt;stochastic&lt;/a&gt; process. Another interesting fact is time series data is a special case of panel data. Panel/Longitudinal data is multidimensional data that involves measurement over time. Cross-sectional data is another instance of panel data which frequently occurs in economics. It’s concerns measuring and comparing multiple subjects at the same point in time.&lt;/p&gt;
&lt;p&gt;An example of time series data is the weekly closing price for AT&amp;amp;T stocks obtained from the &lt;a href=&#34;https://datamarket.com/data/set/22s6/weekly-closing-price-of-att-common-shares-1979#!ds=22s6&amp;amp;display=line&#34;&gt;datamarket&lt;/a&gt; website. The example below shows how the AT&amp;amp;T stock prices behave as time series elements.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;att &amp;lt;- read_excel(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Time series/att.xlsx&amp;quot;)

ggplot(att,aes(Week,Price))+
  geom_line()+
  ggtitle(&amp;quot;Time series plot of weekly closing prices of AT&amp;amp;T&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;components-of-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Components of Time Series&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Trend:&lt;/strong&gt; Trend is the overall tendency of the time series data to move in a general upward or downward direction. The term trend is vague and has no mathematical basis. It is only used to give the data an overall direction.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Seasonality:&lt;/strong&gt; Seasonality are &lt;em&gt;short&lt;/em&gt;, &lt;em&gt;consistent&lt;/em&gt; periodic pattern in the time series data that is &lt;em&gt;repeated&lt;/em&gt; every time period.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can visualise the trend and seasonality using the &lt;code&gt;stl&lt;/code&gt; function. For instance lets look at the time series decomposition of monthly sales volumes for anti-diabetic drugs in Australia:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;season.decomp.ts &amp;lt;- (stl(a10, s.window=&amp;quot;periodic&amp;quot;)$time.series)
season.decomp.df &amp;lt;- timetk::tk_tbl(season.decomp.ts)

#convert to long form
season.decomp.df %&amp;gt;%
     gather(key, value, -index) %&amp;gt;%
     ggplot(aes(x = index, y = value))+
     geom_line()+
     facet_grid(key~., scales = &amp;quot;free&amp;quot;)+
     theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see and upward trend and small peaks along the way. The rise and fall around the peaks represent seasons that is repeated every calender year. Seasonality may be the result of external factors that influence the time series data - such as the AT&amp;amp;T stock prices; the rise and fall of which might be affected by sentiments of customers. Another cool way to visualize the seasons within each year:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggseasonplot(a10, polar = TRUE) + theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Cyclicity:&lt;/strong&gt; This too has a rise and fall pattern like seasonality except cycles are &lt;em&gt;longer&lt;/em&gt; and &lt;em&gt;inconsistant&lt;/em&gt;. They are inconsistent in their magnitude as well as their repeat patterns. More often cycles are a part of a longer time series extending over decades.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;type-of-time-series-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Type of Time series data&lt;/h2&gt;
&lt;p&gt;Based on behavior, time series can be categorized in two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Frequency domain:&lt;/strong&gt; This approach assumes that the characteristics of a time series are explained by systematic or periodic movements that are observed in nature.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Time domain:&lt;/strong&gt; It is primary based on the assumption that adjacent time series objects are correlated.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This brings us to the world of ARIMA models. The &lt;em&gt;AutoRegressiveMovingAverage&lt;/em&gt; models are the outcome of landmark work by Box and Jenkins. This is why a univariate ARIMA model is also referred to as a UBJ (univariate box-jenkins) model. ARIMA models fall under the time domain. UBJ models are broadly classified into two categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stationary:&lt;/strong&gt; Stationarity can of two forms - mean stationary and variance stationary. As the name suggests, a time series that has a constant mean is mean stationary and it’s variance stationary when the variance of &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;’s are independent of time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Non Stationary:&lt;/strong&gt; Any time series that is either mean or variance non stationary is non stationary in nature.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For instance, the AT&amp;amp;T stock prices is a non stationary process over the 52 week period. However, the first difference of the above series becomes a stationary (approximately). The &lt;code&gt;diff&lt;/code&gt; fucntion is used to compute the difference of a speacific order.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;diff_df &amp;lt;- data.frame(difference = diff(att$Price,1),time.index = 1:51)

ggplot(diff_df,aes(time.index,difference))+
   geom_line()+
   geom_point(col = &amp;quot;#FC4E07&amp;quot;)+
   labs(title = &amp;quot;Stationarized AT&amp;amp;T Time series&amp;quot;,y = &amp;quot;First order difference&amp;quot;,x = &amp;quot;Time&amp;quot;)+
   theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;white-noise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;White Noise&lt;/h2&gt;
&lt;p&gt;Almost all statistical concepts are not complete without defining a term for randomness that real life data exhibits. In time series the term ‘white noise’ refers to a set of uncorrelated random variables. It is also not unusual for most statistical ideas to be optimistic and consider the distribution of random variables as normal. Mathematically, they are represented as: &lt;span class=&#34;math display&#34;&gt;\[Z_t \sim iid(0,\sigma^2)\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This just means that they are independently and identically distributed with mean 0 and has a inherent variation. The source of this variation can be factors that affect the data points externally and internally. White noise is also not observed(as with any random component).Lets look at a simulated white noise:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To observe the effect of white noise take a look at these images. The top image shows a simple cosine curve. The next image shows what happens when we introduce white noise in the curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- 1:500;w &amp;lt;- rnorm(500,0,1)
c &amp;lt;- 2*cos(2*pi*t/50)
c_new &amp;lt;- c+w
tmp &amp;lt;- data.frame(t = 1:500,c,c_new)

my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

tmp %&amp;gt;%
  gather(key,value,-t)%&amp;gt;%
  ggplot(aes(x = t, y = value, col = key))+
  geom_line()+
  facet_grid(key~.)+
  scale_color_manual(values = my2cols)+
  theme_minimal()+
  labs(title = &amp;quot;Effect of WN on a time series&amp;quot;, x = &amp;quot;time&amp;quot;)+
  theme(legend.position = &amp;quot;none&amp;quot;, strip.text = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;characteristics-of-stationary-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Characteristics of Stationary time series&lt;/h2&gt;
&lt;p&gt;Stationarity ensures that our time series is at a state of equilibrium and the basic behavior does not alter with time in terms of mean or variance. Therefore, the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt; could just be represented by &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. This simplifies our calculations and gives us a good starting point to describing the characteristics of time series. It’s also the same reason why if a series is non-stationary, we try to stationarize it first (We shall discuss how very soon).&lt;/p&gt;
&lt;p&gt;Stationary time series can further be classified into two types based on variance and autocovariance of the data points:&lt;br /&gt;
- A time series is &lt;em&gt;weakly&lt;/em&gt; stationary if the variance does not change with time&lt;br /&gt;
- A time series is &lt;em&gt;strictly&lt;/em&gt; stationary if the joint distribution of the set &lt;span class=&#34;math inline&#34;&gt;\({X_{t1},X_{t2}...}\)&lt;/span&gt; is equal to the time shifted set &lt;span class=&#34;math inline&#34;&gt;\(X_{t1+h},X_{t2+h}...\)&lt;/span&gt;. Also, the mean of the two sets along with the variance is stationary.&lt;/p&gt;
&lt;p&gt;In statistics, a collection of random variables always has two aspects- their distribution and their moments. Think of these as IDs but they’re not always unique. It’s common for two different random variables to have the same distribution and the same moments. The time series random variables also have distribution and moments. It is also possible to define a joint distribution for all these variables. Generally, one would define a joint distribution function as: &lt;span class=&#34;math display&#34;&gt;\[F(c_1,c_2,c_3,...,c_n) = P(x_{t1}&amp;lt;c_1,x_{t2}&amp;lt;c_2,....,x_{tn}&amp;lt;c_n)\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Defining such a ‘combined’ distribution becomes a problem when the random variables follow different distributions. So, we make the assumption that they follow the same (normal) distribution (ie, normal iid or multinomial Gaussian). Now we can define our CDF as:&lt;span class=&#34;math display&#34;&gt;\[F(c_1,c_2,c_3,...,c_n) = \prod_{t=1}^n 1/\sqrt(2 \pi)\int_\infty^x exp(-z^2/2) dz\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can now define the PDF as:&lt;span class=&#34;math display&#34;&gt;\[f_{t_1,t_2..}(x)=\frac{dF_{t_1,t_2..}(x)}{dx}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re now at a stage to define the different attributes of a stationary time series process but before that we need to define what a &lt;strong&gt;realization&lt;/strong&gt; is. A &lt;strong&gt;realization&lt;/strong&gt; is one subset of observations coming from the underlying process. Simply put it represents a sample. And normal statistical practice dictates that we estimate parameters of the process from this realization (or sample). From now on when we are referring to a time series we refer to realization of the actual underlying process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mean&lt;/strong&gt;: As with any statistical distribution, the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; of a time series is simply: &lt;span class=&#34;math display&#34;&gt;\[E(X_t)=\int_{- \infty}^{\infty} X f_t(X) dx\]&lt;/span&gt;. It is no surprise that the sample (or realization) mean estimates the actual mean. &lt;span class=&#34;math display&#34;&gt;\[\bar{X_t} = \hat{\mu_t}= \frac{\sum_{t=1}^n X_t}{N}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; : Following the usual definition of variance in statistical literature, the variance of a time series is given by &lt;span class=&#34;math display&#34;&gt;\[Var(X_t)=E(X_t - \mu_t)^2\]&lt;/span&gt;. While the estimated variance &lt;span class=&#34;math display&#34;&gt;\[Var(\bar{X_t})= \frac{\sigma^2}{N} \sum_{h=-(n-1)}^{n-1}(1- \frac{|k|}{n})\rho_h\]&lt;/span&gt;. where n = realization of length n.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Autocovariance function&lt;/strong&gt;: It is the second moment function. It is given by:&lt;span class=&#34;math display&#34;&gt;\[Cov(X_t,X_{t+h}) = E[(X_t-\mu_t)(X_{t+h}-\mu{t+h})]\]&lt;/span&gt; Here h is called &lt;em&gt;lag&lt;/em&gt; time which simply means that the observation &lt;span class=&#34;math inline&#34;&gt;\(X_{t+h}\)&lt;/span&gt; is ‘h’ lags ahead of &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;. This is often represented by &lt;span class=&#34;math inline&#34;&gt;\(\gamma(h)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(h=h+t-t\)&lt;/span&gt; for a stationary process and does not depend on t.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is important to realize that &lt;span class=&#34;math inline&#34;&gt;\(Cov(X_t,X_t)=Var(X_t)\)&lt;/span&gt;. Notice, there is no lag here or &lt;span class=&#34;math inline&#34;&gt;\((t-t)=0\)&lt;/span&gt;. Hence, variance is also represented as &lt;span class=&#34;math inline&#34;&gt;\(\gamma(0)\)&lt;/span&gt;. The estimated ACVF is given by: &lt;span class=&#34;math display&#34;&gt;\[\hat{\gamma_h}=\frac{1}{n} \sum_{t=1}^{n-|h|} (X_t - \bar{X})(X_{t+|h|}-\bar{X_t})\]&lt;/span&gt; where h=0,&lt;span class=&#34;math inline&#34;&gt;\(\pm 1,\pm2,..,\pm(n-1)\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;autocorrelation-function-acf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Autocorrelation function (ACF)&lt;/h2&gt;
&lt;p&gt;The autocorrelation function, mathematically is similar to the usual correlation function that we encounter in statistics. The intuition behind ACF is that there are correlations between &lt;em&gt;ordered pairs&lt;/em&gt; (I will explain ordered pairs in details when we discuss difference but for now think of ordered pairs as a pairs of observations that are separated by &lt;em&gt;h&lt;/em&gt; lags) of data. This is the essence of UBJ ARIMA modeling.&lt;/p&gt;
&lt;p&gt;The ACF is a very important tool during the initial model identification as well as final model selection stage. It is given by the formula:&lt;span class=&#34;math display&#34;&gt;\[Corr(X_t,X_{t+h})=\frac{Cov(X_t,X_{t+h})}{\sqrt{Var(X_t)}\sqrt{Var(X_{t+h})}}\]&lt;/span&gt; The difference in lag is &lt;span class=&#34;math inline&#34;&gt;\(t+h-t=h\)&lt;/span&gt; and therefore the above is popularly represented by &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)\)&lt;/span&gt; for a stationary process.&lt;/p&gt;
&lt;p&gt;It can be easily shown that:&lt;br /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)=\frac{\gamma(h)}{\gamma(0)}\)&lt;/span&gt;&lt;br /&gt;
- &lt;span class=&#34;math inline&#34;&gt;\(\rho(h)=\rho(-h)\)&lt;/span&gt;. This means that ACF of a stationary time series is symmetric about the origin.&lt;/p&gt;
&lt;p&gt;Therefore, the estimated ACF is just ratio of estimated ACVFs and is given by &lt;span class=&#34;math display&#34;&gt;\[\hat{\rho_h}=\frac{\hat{\gamma_h}}{\hat{\gamma_0}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is what the ACF (using the &lt;code&gt;ggAcf&lt;/code&gt; fucntion) of the AT&amp;amp;T stock prices looks like when plotted against lag:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(att$Price)+
  labs(title = &amp;quot;ACF PLot&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The first set of numbers are the actual values of ACF and the plot below that displays the values. The first value for ACF is always one.(Think why?). The blue dashed line is cutoff that indicates that only the ACF values at the respective lags are significant. Finally, the estimated ACF is also referred to as the sample ACF (SACF).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partial-autocorrelation-function-pacf&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partial Autocorrelation Function (PACF)&lt;/h2&gt;
&lt;p&gt;Recall, when I mentioned that the ACF measures the correlation between ordered pairs drawn from a time series data we chose to ignore the effects of the &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt;’s that lie between the pair. PACF does exactly that. It measures the correlation between &lt;span class=&#34;math inline&#34;&gt;\(X_t,X_{t+h}\)&lt;/span&gt; but takes into consideration all the random variable interactions between them. So, if we wanted to measure the PACF between &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{t+2}\)&lt;/span&gt;, this would also include the effects of &lt;span class=&#34;math inline&#34;&gt;\(X_{t+1}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(X_{t+2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The PACF is designated by the symbol &lt;span class=&#34;math inline&#34;&gt;\(\phi_{hh}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\phi_{hh}}\)&lt;/span&gt;. I will not go into the details of PACF calculation (it involves using the AR, MA and ARMA equations and developing recursive equations that give us fairly good estimates of PACF) since it is complicated and in reality these calculations will be done by R.&lt;br /&gt;
Then why do we study the ACF and PACF? The answer to this question will be revealed very soon when I explain how to do ARIMA modeling.&lt;/p&gt;
&lt;p&gt;Note: Just like theoretical and estimated ACF we also have the theoretical and estimated PACF. And just like it’s cousin the estimated PACF is referred to as SPACF or Sample PACF.&lt;/p&gt;
&lt;p&gt;Let’s look at the PACF (using the &lt;code&gt;ggPacf&lt;/code&gt;) for AT&amp;amp;T stock prices&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(att$Price)+
  labs(title = &amp;quot;PACF PLot&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, the first set of numbers are the actual values of PACF for the time series and the plot above is a graphical interpretation of those numbers. The blue dashed line once again indicates that only the lags for which the PACF peaks crosses the blue region are significant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-models-for-time-series&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistical Models for Time series&lt;/h1&gt;
&lt;p&gt;Recall that time series data are a collection of random variables indexed according to the sequence they are obtained in time. So, &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(t \in {0,\pm1, \pm2,..}\)&lt;/span&gt; is such a random variable and the subscript &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; represents the order or sequence of discrete points in time.&lt;/p&gt;
&lt;p&gt;The UBJ ARIMA family has 4 statistical models. They are nothing but an algebraic expression describing how these points are related.&lt;/p&gt;
&lt;div id=&#34;the-moving-average-ma-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The moving average (MA) model&lt;/h2&gt;
&lt;p&gt;The algebraic equation for the MA model is:&lt;span class=&#34;math display&#34;&gt;\[X_t = c + Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(theta_t\)&lt;/span&gt; are the parameters of the model and &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; are the white noise/error terms. &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the lag of the model and is referred to as the ‘order’ of the process. The choice of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is a personal one and it depends on the person modeling and the nature of the problem at hand.&lt;/p&gt;
&lt;p&gt;The intuition behind the above equation is that the current value &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; is an average of the past &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; random shocks (which are obviously unobserved) so instead of measuring &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; as the mean and the current random shock, say that they are an average of &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; random shocks.&lt;br /&gt;
There is another intuition behind the moving average equation and perhaps it’s a more mathematically accurate intuition. If you were to look at the MA equation, it appears as if it is linear regression of the current value of the series &lt;span class=&#34;math inline&#34;&gt;\(X_t\)&lt;/span&gt; against the current and past values of white noise.&lt;/p&gt;
&lt;p&gt;A key question from the MA equation is that even though I have stressed that a univariate Box-Jenkins model essentially studies the relationships between present and past random variables, the above process does not include past random variables. The answer is that the random shock terms above can be replaced by past values of the time series through manipulation. In other words, you can think of &lt;span class=&#34;math inline&#34;&gt;\(Z_t&amp;#39;s\)&lt;/span&gt; as being part of &lt;span class=&#34;math inline&#34;&gt;\(X_t&amp;#39;s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is an example of a simulated MA process with &lt;span class=&#34;math inline&#34;&gt;\(theta_1=0.5\)&lt;/span&gt; with 100 data points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ma_sim &amp;lt;- arima.sim(model = list(ma=0.5),n=50)
ma_sim &amp;lt;- as.data.frame(ma_sim)
ma_sim &amp;lt;- ma_sim %&amp;gt;% mutate(t = 1:50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;bindrcpp&amp;#39; was built under R version 3.4.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ma_sim,aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs(&amp;quot;Simulated MA Series&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + Z_t-\theta_1Z_{t-1}\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + Z_t-\theta_1Z_{t-1}+\theta_2Z_{t-2}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Backshift Operator:&lt;/em&gt; There exists an operator called the backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Z_t = B^0Z_t,Z_{t-1}=B^1Z_t,Z_{t-2}=B^2Z_t\)&lt;/span&gt;. This gets rid of all the confusing subscripts. It is also convention to write &lt;span class=&#34;math inline&#34;&gt;\(X_t - c\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t}\)&lt;/span&gt;. Using these conventions we can write the above examples as:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t} = (1 - \theta_1B)Z_t\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t} = (1 - \theta_1B - \theta_2B^2)Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence a general MA(q) process can be represented as: &lt;span class=&#34;math display&#34;&gt;\[\tilde{X_t} = (1 - \theta_1B - \theta_2B^2 - ..... - \theta_qB^q)Z_t = \Theta_q(B)Z_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: It can be shown that variance of an MA process is independent of &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. This effect trickles down to their ACF function.&lt;/p&gt;
&lt;p&gt;When I discussed ACF and PACF, I had mentioned that the importance of estimated ACF and PACF becomes paramount in the model selection steps. Here’s why:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACF of an MA process:&lt;br /&gt;
For an MA process the ACF shows spikes at lags where the autocorrelation are significant. Specifically, the MA will have spikes at k &lt;span class=&#34;math inline&#34;&gt;\(\le\)&lt;/span&gt; q and then &lt;em&gt;sharply&lt;/em&gt; drops to zero. Let’s look at the ACF from our simulated MA model:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ma_sim &amp;lt;- arima.sim(model = list(ma=0.5),n=100)
ggAcf(ma_sim)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The MA(1) process has a &lt;em&gt;spike&lt;/em&gt; at lag one.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PACF of an MA process: The PACF of an MA process dampens to zero &lt;em&gt;quickly&lt;/em&gt; but not &lt;em&gt;sharply&lt;/em&gt;. The fall is gradual but relatively smooth and quick yet not a sharp drop. Lets look at the PACF of the simulated MA model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At first glance it might be difficult to see the gradual drop as it looks like a spike but if you carefully inspect the &lt;em&gt;spikes&lt;/em&gt; on both sides of the axis, you will realise that the spikes actually change slowly from being significant to insignificant. The ability to recognise this drop versus a spike takes time and experience.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-autoregressive-ar-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The autoregressive (AR) model&lt;/h2&gt;
&lt;p&gt;The algebraic equation for the MA model is:&lt;span class=&#34;math display&#34;&gt;\[X_t = c +\phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p} + Z_t\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(phi_t&amp;#39;s\)&lt;/span&gt; are the parameters of the model and &lt;span class=&#34;math inline&#34;&gt;\(X_t&amp;#39;s\)&lt;/span&gt; are the previous lagged terms. &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the lag of the model and is referred to as the ‘order’ of the process. Once again, the choice of &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is a personal one and it depends on the person modeling and the nature of the problem at hand.&lt;/p&gt;
&lt;p&gt;The AR process is more intuitive than the MA. We can see how the present value is related to the previous value through a regression equation. Also, just like a regression equation, the &lt;span class=&#34;math inline&#34;&gt;\(Z_t\)&lt;/span&gt; represents the probabilistic aspect of a linear model. Let’s look at a simulated AR time series:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(42)
ar_sim &amp;lt;- data.frame(x = arima.sim(model = list(ar=0.5),n=50))
ar_sim %&amp;gt;% 
 mutate(t = 1:50)%&amp;gt;%
 ggplot(aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs(&amp;quot;Simulated AR Series&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;br /&gt;
- AR(1): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c + \phi_1X_{t-1} + Z_t\)&lt;/span&gt;&lt;br /&gt;
- AR(2): &lt;span class=&#34;math inline&#34;&gt;\(X_t = c +\phi_1X_{t-1}+\phi_2X_{t-2}+Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Backshift Operator:&lt;/em&gt; For AR process also there exists a backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(X_t = B^0X_t,X_{t-1}=B^1X_t,X_{t-2}=B^2X_t\)&lt;/span&gt;. Like the MA process, it is also convention to write &lt;span class=&#34;math inline&#34;&gt;\(X_t - \mu\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_t}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{c}{1-\phi_1}\)&lt;/span&gt;. Using these conventions we can write the above examples as:&lt;br /&gt;
- MA(1): &lt;span class=&#34;math inline&#34;&gt;\((1 - \theta_1B)\tilde{X_t} = Z_t\)&lt;/span&gt;&lt;br /&gt;
- MA(2): &lt;span class=&#34;math inline&#34;&gt;\((1 - \theta_1B - \theta_2B^2)\tilde{X_t} = Z_t\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Hence a general MA(q) process can be represented as: &lt;span class=&#34;math display&#34;&gt;\[(1 - \theta_1B - \theta_2B^2 - ..... - \theta_pB^p)\tilde{X_t} = Z_t = \Phi_p(B)X_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note: For an AR process the variance depends on time. This effect is visible when we discuss the ACF and PACF of an AR process next.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ACF of AR process:&lt;/strong&gt;The ACF for the AR process is exactly opposite to the ACF of the MA. The AR ACF shows a gradual decline to zero&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PACF of AR process:&lt;/strong&gt; The PACF is exactly opposite to the PACF of the MA. The AR PACF shows spikes for lags. Let’s look at the ACF and PACF plots for AR and MA process.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(ar_sim)+
  labs(title = &amp;quot;ACF&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(ar_sim)+
  labs(title = &amp;quot;PACF&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-14-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is how ACF and PACF act as distinguishing factors in the identification of the appropriate statistical model that best explains the given realisation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arma-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ARMA process&lt;/h2&gt;
&lt;p&gt;We have learnt that sometimes the present value depends on the past values (AR process) of the time series process and sometimes the present value depends on the past random shocks (MA process). There’s another model that uses both these features. This is called the ARMA model. In time series this process is fully represented by ARMA(p,q) where p, q carry the same meaning as before. Perhaps the mathematical representation will make the intuition more clear,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X_t = c + \phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p}+Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using the backshift operator this can be represented as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Phi_p(B)\tilde{X_t} = \Theta_q(B)Z_t\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here the symbols have their usual meaning except &lt;span class=&#34;math inline&#34;&gt;\(\tilde{X_T}\)&lt;/span&gt;. This is &lt;span class=&#34;math inline&#34;&gt;\(X_t - \mu\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\mu = \frac{c}{1-\phi_1-\phi_2...-\phi_p}\)&lt;/span&gt;. For instance an ARMA(1,1) process can be written as &lt;span class=&#34;math inline&#34;&gt;\((1-\phi_1B)X_t=(1-\theta_1B)Z_t\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arima-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ARIMA Model&lt;/h2&gt;
&lt;p&gt;This is a upgrade from the ARMA model. The ‘I’ in ARIMA stands for Integrated which indicates the time series values have been differenced. The ARIMA components can be represented in two different ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the model has a &lt;em&gt;non seasonal&lt;/em&gt; component only the model is represented as &lt;strong&gt;ARIMA(p,d,q)&lt;/strong&gt; where p,d,q stands for the AR, MA and difference order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If the has both &lt;em&gt;seasonal and non seasonal&lt;/em&gt; components, the model is represented by &lt;strong&gt;ARIMA(p,d,q)(P,D,Q)&lt;/strong&gt; where P,D,Q stands for the seasonal AR, MA and difference order.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Identification:&lt;/strong&gt; Identifying the order of an ARIMA can be tricky. From personal experience, when there are indications of both AR and MA process in the ACF and PACF, it could indicate the combination of both process. Signs of seasonal components is displayed by spikes or decays at the seasonal lag. Rob Hyndman’s &lt;a href=&#34;https://www.otexts.org/fpp/8/9&#34;&gt;page&lt;/a&gt; on seasonal ARIMA has nice examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-makes-a-good-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What makes a good model&lt;/h2&gt;
&lt;p&gt;When we select a time series model, there are signs that indicate how good the model is. Here is a list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stationarity &amp;amp; Invertiblity:&lt;/strong&gt; We can use the coefficients of the model to check for stationarity and invertibility by using certain algebraic inequalities. This &lt;a href=&#34;http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/TimeSeries_Stationarity.pdf&#34;&gt;resource&lt;/a&gt; outlines the rules.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Correlation:&lt;/strong&gt; The coefficients in the model should not be highly correlated. If they are, they could represent redundent terms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Statistical Significance of parameters:&lt;/strong&gt; The parameters estimated should be statistically significant. We can check to see of the p-value is lower than the significance value in the output.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Diagnostic Checking:&lt;/strong&gt; Although in practice we cannot observe the random shocks, it is important that they are statistically independent. If the residuals are autocorrelated they are not white noise. We perform the Ljung-Box (Box-Pierce) test which essentially tests the autocorrelations among random shocks. The random noise should be normally distributed and the residual-ACF should resemble white noise ACF (no significant lags).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;steps-in-arima-modelling-and-forecasting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Steps in ARIMA modelling and forecasting&lt;/h1&gt;
&lt;p&gt;Now we are ready to layout the steps we use to first identify a time series model parameters and then forecast future values. The data used contains 1810 observations in Bowling Green, Ohio of the maximum daily temperature in degrees Celsius, one observation every day from January 1, 2011 - December 31, 2015 (so 5 years of complete data).&lt;/p&gt;
&lt;p&gt;The Bowling Green station has a latitude and longitude of 41.3831, -83.6111. The data goes into the GHCN (Global Historical Climatology Network)-Daily database, maintained by the National Climactic Data Center (NCDC), part of the U.S. Department of Commerce.The data was aggregated over every week and the mean maximum weekly temperature was obtained.The final 51 weeks were used as test set, and the rest was used to train the ARIMA model.&lt;/p&gt;
&lt;div id=&#34;step-1-visualize-the-time-series&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Visualize the time series&lt;/h2&gt;
&lt;p&gt;The first step is to visualize the time series to gain insights into the behavior (such as seasonality) of the time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;%
  ggplot(aes(x = week, y = weekly_mean))+
  geom_line(col = my2cols[1])+
  geom_point(col = my2cols[2])+
  labs(title = &amp;quot;Bowling Green Weekly Average Maximum&amp;quot;,y = &amp;quot;Temperture&amp;quot;,x = &amp;quot;Week ID&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The time series plot for the first four years of data is clearly seasonal with a period of length s = 52 (weeks).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-identify-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Identify the model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ACF Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggAcf(train$weekly_mean, lag.max = 200)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PACF Plot&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggPacf(train$weekly_mean, lag.max = 200)+
  labs(title = &amp;quot;&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The inferences can be summarized:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looking at the ACF, the early lags’ decay is possibly quick enough that a differencing of the original series will not be necessary (d = 0), but we may consider it as well (d = 1) since it is a rather grey area. (&lt;em&gt;NOT SHOWN: A first order difference ACF plot showed the possibility of an MA component&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;The seasonal spikes at 52, 104, and 156 are decreasing rather slowly but are within the 5% significance limits, so on the seasonal side there is also a grey area as to whether D = 0 or D = 1 needs to be used.&lt;/li&gt;
&lt;li&gt;Looking at the PACF, we have spikes for lags 1 and 2, especially for 1 (meaning that, if we use d = 0, may start with an AR1 model)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Based on the above information lets fit two different models - &lt;em&gt;ARIMA(0,1,1)(0,1,1)&lt;/em&gt; and &lt;em&gt;ARIMA(1,0,1)(1,0,1)&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-estimation-of-model-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Estimation of model parameters&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;Arima&lt;/code&gt; function from &lt;code&gt;forecast&lt;/code&gt; package is used to fit the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a time series object
y &amp;lt;- ts(train$weekly_mean, frequency = 52, start = c(2011, 1))

first.model &amp;lt;- forecast::Arima(y = y,
                               order = c(0,1,1),
                               seasonal = list(order = c(0,1,1), period = 52))
  

summary(first.model) # summary of the model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: y 
## ARIMA(0,1,1)(0,1,1)[52]                    
## 
## Coefficients:
##           ma1     sma1
##       -0.8196  -0.9772
## s.e.   0.0608   1.1281
## 
## sigma^2 estimated as 12.43:  log likelihood=-448.97
## AIC=903.94   AICc=904.1   BIC=913.07
## 
## Training set error measures:
##                      ME     RMSE      MAE       MPE    MAPE      MASE
## Training set -0.1625666 3.024313 2.109829 -106.9443 152.265 0.5313132
##                    ACF1
## Training set 0.01856747&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second.model &amp;lt;- forecast::Arima(y = y,
                               order = c(1,0,1),
                               seasonal = list(order = c(1,0,1), period = 52))
  

summary(second.model) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Series: y 
## ARIMA(1,0,1)(1,0,1)[52] with non-zero mean 
## 
## Coefficients:
##          ar1      ma1    sar1     sma1     mean
##       0.9527  -0.4237  0.9915  -0.9215  14.6272
## s.e.  0.0220   0.0638  0.0750   0.3446   4.7782
## 
## sigma^2 estimated as 14.57:  log likelihood=-589.01
## AIC=1190.03   AICc=1190.45   BIC=1210.05
## 
## Training set error measures:
##                     ME     RMSE      MAE       MPE     MAPE      MASE
## Training set 0.1306786 3.770671 3.051861 -35.31861 103.6716 0.7685429
##                     ACF1
## Training set -0.06111836&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-other-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4: Other tests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Significance test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(first.model) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##       Estimate Std. Error  z value Pr(&amp;gt;|z|)    
## ma1  -0.819648   0.060812 -13.4785   &amp;lt;2e-16 ***
## sma1 -0.977199   1.128073  -0.8663   0.3864    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmtest::coeftest(second.model) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##            Estimate Std. Error z value  Pr(&amp;gt;|z|)    
## ar1        0.952665   0.022011 43.2818 &amp;lt; 2.2e-16 ***
## ma1       -0.423745   0.063845 -6.6371   3.2e-11 ***
## sar1       0.991536   0.075026 13.2159 &amp;lt; 2.2e-16 ***
## sma1      -0.921538   0.344585 -2.6743  0.007488 ** 
## intercept 14.627228   4.778240  3.0612  0.002204 ** 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that one of the parameters in the first model is insignificant.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Correlation test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(first.model$var.coef) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              ma1        sma1
## ma1   1.00000000 -0.02333284
## sma1 -0.02333284  1.00000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov2cor(second.model$var.coef) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   ar1          ma1        sar1        sma1    intercept
## ar1        1.00000000 -0.250681177 -0.02601634  0.01704827 -0.118705291
## ma1       -0.25068118  1.000000000 -0.04030825  0.01638146  0.003743894
## sar1      -0.02601634 -0.040308250  1.00000000 -0.99878199  0.039394253
## sma1       0.01704827  0.016381464 -0.99878199  1.00000000 -0.036329713
## intercept -0.11870529  0.003743894  0.03939425 -0.03632971  1.000000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extremely high correlation between the SAR and SMA term in the second model.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Error measures&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;first.model$sigma2 # first model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12.43442&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;second.model$sigma2 # second model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 14.56816&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first model has better (lower) error measure&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Assumption Checking&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;checkresiduals(first.model) # first model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(0,1,1)(0,1,1)[52]
## Q* = 126.13, df = 102, p-value = 0.05286
## 
## Model df: 2.   Total lags used: 104&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;checkresiduals(second.model) # second model&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-23-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Ljung-Box test
## 
## data:  Residuals from ARIMA(1,0,1)(1,0,1)[52] with non-zero mean
## Q* = 262.02, df = 99, p-value &amp;lt; 2.2e-16
## 
## Model df: 5.   Total lags used: 104&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The assumptions for both models seem to be satisfied.&lt;/p&gt;
&lt;p&gt;At this point we make a decision to move forward with one of the model for forecasting. We could go forward with both models and further check prediction errors on both models. But this process would be unrealistic if we had plenty of models. I will pick the first model because it is simpler.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-forecasting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5: Forecasting&lt;/h2&gt;
&lt;p&gt;We can forecast using the &lt;code&gt;forecast&lt;/code&gt; function using an appropriate forecast window.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forecasts.object &amp;lt;- forecast(first.model, h = 51)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets visualize the forecasts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# obtain the forecast means and confidence intervals

forecast.df &amp;lt;- data.frame(
                 week = test$week,
                 forecasts = as.matrix(forecasts.object$mean),
                 upper = as.matrix(forecasts.object$upper),
                 lower = as.matrix(forecasts.object$lower),
                 actual = test$weekly_mean
)


forecast.df %&amp;gt;%
  ggplot()+
    geom_line(mapping = aes(x = week, y = forecasts), col = my2cols[1], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = actual), col = my2cols[2], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = lower.95.), lty = 2)+
    geom_line(mapping = aes(x = week, y = upper.95.), lty = 2)+
    labs(y = &amp;quot;Temperature&amp;quot;,x = &amp;quot;Week&amp;quot;,title = &amp;quot;Actual vs Forecasted Value&amp;quot;)+
    theme(plot.title = element_text(lineheight=.8, face=&amp;quot;bold&amp;quot;),legend.position=&amp;quot;top&amp;quot;)+
    theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-of-fortune-tellers-and-crystal-balls_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line represent the forecasted values while the red one represents the actual ones. The dotted lines on the other hand represent the upper and lower confidence intervals. We can see the model does a pretty good job at forecasting the values for the test data set.&lt;/p&gt;
&lt;p&gt;Finally, let’s calculate the accuracy of the forecasted values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error = function(actual, pred){
  mse = sqrt(mean((actual - pred)^2))
  mad =  mean(abs(actual - pred))
  mape = mean(abs((actual - pred)/actual))
  list(mse = mse, mape = mape, mad = mad)
}

error(forecast.df$actual, forecast.df$forecasts)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $mse
## [1] 3.533256
## 
## $mape
## [1] 0.3449951
## 
## $mad
## [1] 2.630848&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The error function computes the standard error measures (Mean Square Error, Mean Absolute Deviation, Mean Absolute Percentage Error). The error values seem to be sufficiently small.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Optimization with Gurobi</title>
      <link>/post/optimization-with-gurobi/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/optimization-with-gurobi/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;The problem we are solving is the &lt;strong&gt;Brisbane Earth Moving Problem&lt;/strong&gt; from the book &lt;strong&gt;Optimization in Operations Research (2nd Edition)&lt;/strong&gt; by &lt;em&gt;Rardin&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A major expansion of the Brisbane airport will require moving substantial quantities of earth from 4 sites where it is surplus to 7 locations where it is needed. The following table shows the haul distances (hundreds of meters) between points, as well as the quantity available at each surplus site. Quantities needed are 247 cubic meters at the extension, 394 at the dry pond, 265 along roads, 105 in the parking area, 90 at the fire station, 85 in the industrial park, and 145 along the perimeter road. The site engineer wishes to compute a minimum total distance times volume plan for accomplishing the required earth moving.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The parameters of the problem are (1) the volume of earth avaialable at the supply site, (2) the volume of earth required at the need sites and (3) the distance between the supply and the need sites&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt; represent the supply sites where &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,4\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; represent the need/demand sites where &lt;span class=&#34;math inline&#34;&gt;\(j = 1,....,7\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(d_{i,j}\)&lt;/span&gt; represent the distance between a particular &lt;span class=&#34;math inline&#34;&gt;\(s_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given the above parameters the &lt;strong&gt;objective function&lt;/strong&gt; can be represented as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{min} \quad \sum_{i=1}^{4} \sum_{j=1}^4 d_{i,j}∗v_{i,j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Subject to:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assuring full amount being moved from every surplus site can be collectively represented by the following indexed equality:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \sum^7_{j = 1} v_{i,j} = s_i \quad \forall \quad i = 1,..,4\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assuring the required amount being moved to each needed location can be represented by the following indexed equality:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \sum^4_{i = 1} v_{i,j} = n_j \quad \forall \quad j = 1,..,7\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since the volume of earth moved has to zero or positive, we can write the following variable-type constraint:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ v_{i,j} \geq 0 \quad \forall \quad i=1,..,4 \quad \textrm{and} \quad j = 1,..,7 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the solution in Python using Gurobi&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# import from gurobi library
from gurobipy import *
## create dimensions of the problem
#create list of surplus sites
surplus = [&amp;#39;Apron&amp;#39;,&amp;#39;Term&amp;#39;,&amp;#39;Cargo&amp;#39;,&amp;#39;Access&amp;#39;] #will be indexed by i
#create list of need sites
need = [&amp;#39;Extension&amp;#39;,&amp;#39;Dry Pond&amp;#39;,&amp;#39;Roads&amp;#39;,&amp;#39;Parking&amp;#39;,&amp;#39;Fire Station&amp;#39;,
                                     &amp;#39;Industrial Park&amp;#39;,&amp;#39;Perimeter Road&amp;#39;] #will be indexed by j
                                     
## create indexed data
#the distance between the surplus and need 
distance_matrix = {
    (&amp;#39;Apron&amp;#39;,&amp;#39;Extension&amp;#39;):26,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Dry Pond&amp;#39;):12,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Roads&amp;#39;):10,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Parking&amp;#39;):18,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Fire Station&amp;#39;):11,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Industrial Park&amp;#39;):8,
    (&amp;#39;Apron&amp;#39;,&amp;#39;Perimeter Road&amp;#39;):20,
    (&amp;#39;Term&amp;#39;,&amp;#39;Extension&amp;#39;):28,
    (&amp;#39;Term&amp;#39;,&amp;#39;Dry Pond&amp;#39;):14,
    (&amp;#39;Term&amp;#39;,&amp;#39;Roads&amp;#39;):12,
    (&amp;#39;Term&amp;#39;,&amp;#39;Parking&amp;#39;):20,
    (&amp;#39;Term&amp;#39;,&amp;#39;Fire Station&amp;#39;):13,
    (&amp;#39;Term&amp;#39;,&amp;#39;Industrial Park&amp;#39;):10,
    (&amp;#39;Term&amp;#39;,&amp;#39;Perimeter Road&amp;#39;):22,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Extension&amp;#39;):20,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Dry Pond&amp;#39;):26,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Roads&amp;#39;):20,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Parking&amp;#39;):2,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Fire Station&amp;#39;):6,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Industrial Park&amp;#39;):22,
    (&amp;#39;Cargo&amp;#39;,&amp;#39;Perimeter Road&amp;#39;):18,
    (&amp;#39;Access&amp;#39;,&amp;#39;Extension&amp;#39;):26,
    (&amp;#39;Access&amp;#39;,&amp;#39;Dry Pond&amp;#39;):10,
    (&amp;#39;Access&amp;#39;,&amp;#39;Roads&amp;#39;):4,
    (&amp;#39;Access&amp;#39;,&amp;#39;Parking&amp;#39;):16,
    (&amp;#39;Access&amp;#39;,&amp;#39;Fire Station&amp;#39;):24,
    (&amp;#39;Access&amp;#39;,&amp;#39;Industrial Park&amp;#39;):14,
    (&amp;#39;Access&amp;#39;,&amp;#39;Perimeter Road&amp;#39;):21,
}
#available volume at the surplus sites
quantity_available = {&amp;#39;Apron&amp;#39;:660,&amp;#39;Term&amp;#39;:301,&amp;#39;Cargo&amp;#39;:271,&amp;#39;Access&amp;#39;:99}
#the requirements at the need sites
requirements = {&amp;#39;Extension&amp;#39;:247,&amp;#39;Dry Pond&amp;#39;:394,&amp;#39;Roads&amp;#39;:265,&amp;#39;Parking&amp;#39;:105,&amp;#39;Fire Station&amp;#39;:90,
                                                        &amp;#39;Industrial Park&amp;#39;:85,&amp;#39;Perimeter Road&amp;#39;:145}
                                                        
## create model
m = Model(&amp;#39;Brisbane Airport&amp;#39;)
## create decision variables
#let flow represent the cubic meters moved from surplus i to site j
flow = {}
for i in surplus:
    for j in need:
        flow[i,j] = m.addVar(lb = 0, vtype = GRB.CONTINUOUS, name=&amp;#39;flow_%s_%s&amp;#39; % (i, j))
        
m.update()
#set the objective
obj = quicksum ( flow[i,j]*distance_matrix[i,j] for i in surplus for j in need )
m.setObjective(obj)
# Add supply constraints
for i in surplus:
    m.addConstr(quicksum(flow[i,j] for j in need) == quantity_available[i], &amp;#39;supply_%s&amp;#39; % (i))
    
# Add demand constraints
for j in need:
    m.addConstr(quicksum(flow[i,j] for i in surplus) == requirements[j], &amp;#39;demand_%s&amp;#39; % (j))
  
# optimize  
m.optimize()
## Print solution
# The objective value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Optimize a model with 11 rows, 28 columns and 56 nonzeros
## Coefficient statistics:
##   Matrix range     [1e+00, 1e+00]
##   Objective range  [2e+00, 3e+01]
##   Bounds range     [0e+00, 0e+00]
##   RHS range        [9e+01, 7e+02]
## Presolve time: 0.06s
## Presolved: 11 rows, 28 columns, 56 nonzeros
## 
## Iteration    Objective       Primal Inf.    Dual Inf.      Time
##        0    1.3980000e+04   1.752000e+03   0.000000e+00      0s
##        6    1.7592000e+04   0.000000e+00   0.000000e+00      0s
## 
## Solved in 6 iterations and 0.16 seconds
## Optimal objective  1.759200000e+04&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;if m.status == GRB.status.OPTIMAL:
    print(&amp;#39;\nMinimum objective is :&amp;#39;,m.objval)
# The optimal flows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Minimum objective is : 17592.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;if m.status == GRB.status.OPTIMAL:
    print (&amp;#39;\nOptimal flows :&amp;#39;)
    for i in surplus:
        for j in need:
            if flow[i,j].x &amp;gt; 0:
                print (i, &amp;#39;-&amp;gt;&amp;#39;, j, &amp;#39;:&amp;#39;, flow[i,j].x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Optimal flows :
## Apron -&amp;gt; Extension : 81.0
## Apron -&amp;gt; Dry Pond : 93.0
## Apron -&amp;gt; Roads : 166.0
## Apron -&amp;gt; Fire Station : 90.0
## Apron -&amp;gt; Industrial Park : 85.0
## Apron -&amp;gt; Perimeter Road : 145.0
## Term -&amp;gt; Dry Pond : 301.0
## Cargo -&amp;gt; Extension : 166.0
## Cargo -&amp;gt; Parking : 105.0
## Access -&amp;gt; Roads : 99.0&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;s In A Basket</title>
      <link>/post/what-s-in-a-basket/</link>
      <pubDate>Sun, 07 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/what-s-in-a-basket/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Ever wondered how online retail sellers come up with &lt;em&gt;‘recommendations’&lt;/em&gt;? How they already know what to refer after you’ve made a purchase? The powerful &lt;em&gt;Apriori&lt;/em&gt; algorithm that is based on simple high school probability concepts is working behind the scenes to discover these connections. Association mining is based on the Apriori Algorithm. Association mining helps analyse the habits of buyers to find the relationship between different items in a ‘market basket’. The discovery of these relationships can help to develop a sales strategy by considering the items frequently purchased in sequence by customers. .This tutorial demonstrates the concepts in Market Basket Analysis and how to perform the analysis in R.&lt;/p&gt;
&lt;div id=&#34;get-the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Get the libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl) # reading in file 
library(readr)
library(arulesViz) #association viz
library(ggplot2) # data viz
library(dplyr) # data manipulation
library(tidyr) # data manipulation&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;market-basket-terminology&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Market Basket Terminology&lt;/h1&gt;
&lt;p&gt;Before we begin association mining we need to understand some relevant terminology. A &lt;strong&gt;rule&lt;/strong&gt; in association mining refers to sequence in which items are bought. So, the rule A =&amp;gt; B means B was bought after A.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Support:&lt;/strong&gt; It is simply the probability of buying A and B together. Statistically this refers to the intersection of A and B.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \text{Support} = \frac{\text{Total number of A and B}}{\text{Total number of transactions}} \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Confidence:&lt;/strong&gt; This is the conditional probability of B given A. A high value of confidence therefore means there is a high chance of B being bought after A.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \text{Confidence} = \frac{\text{Number of Tr. with both A and B}}{\text{Total number of Tr.}} \]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lift:&lt;/strong&gt; It is the confidence divided by the expected confidence. Therefore, a high value of lift mean that there is a higher chance of A and B occurring together.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  \text{Lift} = \frac{\text{Confidence}}{\text{Expected Conf.}} = \frac{\text{Pr(A union B)}}{\text{Pr(A).Pr(B)}}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where expected confidence is &lt;span class=&#34;math display&#34;&gt;\[ \text{Pr(B)} = \frac{\text{Number of Tr. of B}}{\text{Total Number of Tr.}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Its worth noting that using Lift to create recommendations is a bad idea. If you consider the transactions A =&amp;gt; B and B =&amp;gt; A, and if you work out the math with the formula above (which is pretty straight-forward), you’d notice that lift and support turns out to be same!. Therefore, if one is interested in making a &lt;em&gt;sequence&lt;/em&gt; recommendation you should use confidence instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-transaction-object&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create a transaction object&lt;/h1&gt;
&lt;div id=&#34;get-the-data-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the data set&lt;/h2&gt;
&lt;p&gt;We will use a online retail transaction database from a grocery shop in the UK. The data set is available freely &lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/online+retail&#34;&gt;UCI&lt;/a&gt; depository. Here are the variables in the data set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;InvoiceNo:&lt;/strong&gt; Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter ‘c’, it indicates a cancellation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StockCode:&lt;/strong&gt; Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt; Product (item) name. Nominal.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Quantity:&lt;/strong&gt; The quantities of each product (item) per transaction. Numeric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;InvoiceDate:&lt;/strong&gt; Invoice Date and time. Numeric, the day and time when each transaction was generated.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UnitPrice:&lt;/strong&gt; Unit price. Numeric, Product price per unit in sterling.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CustomerID:&lt;/strong&gt; Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Country:&lt;/strong&gt; Country name. Nominal, the name of the country where each customer resides.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocess-the-data-set&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocess the data set&lt;/h2&gt;
&lt;p&gt;Its helpful if the item desciption and the Invoice date-time is converted in factor and a date-time object respectively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; They have been commented out to make compiling easier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Online_Retail &amp;lt;- Online_Retail %&amp;gt;% mutate(Description = as.factor(Description))
# Online_Retail$Date &amp;lt;- as.Date(Online_Retail$InvoiceDate)
# Online_Retail$Time &amp;lt;- format(Online_Retail$InvoiceDate,&amp;quot;%H:%M:%S&amp;quot;)
# Online_Retail$InvoiceNo &amp;lt;- as.numeric(as.character(Online_Retail$InvoiceNo))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;transaction-format&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transaction format&lt;/h2&gt;
&lt;p&gt;Before we can apply the apriori algorithm, we need to transform the data to create &lt;em&gt;“baskets”&lt;/em&gt;. After arranging the data by ID you would need to group the items by the ID and by date and then record all the transactions in another column. Each row in this column is now a basket. You could also form smaller baskets by grouping by hour. After you create the baskets you would need export the file in &lt;em&gt;‘.csv’&lt;/em&gt; format to the local drive and then import it by coercing it as a &lt;em&gt;transaction class&lt;/em&gt; data. The import step is done using the &lt;code&gt;read.transactions&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# itemList &amp;lt;- Online_Retail %&amp;gt;%
               #  arrange(CustomerID) %&amp;gt;%
               #  group_by(CustomerID,Date)%&amp;gt;%
               #  summarise(List = paste0(Description, collapse = &amp;quot;,&amp;quot;))%&amp;gt;%
               #  ungroup()%&amp;gt;%
               #  select(List) # we just need the column containing the baskets


# write.csv(itemList,&amp;quot;itemList.csv&amp;quot;, quote = FALSE, row.names = F)
 
transactions &amp;lt;- read.transactions(file = &amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Market Basket/itemList.csv&amp;quot;,
                                  format = c(&amp;quot;basket&amp;quot;),  # each row is a basket
                                  sep = &amp;quot;,&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;apriori&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Apriori&lt;/h1&gt;
&lt;p&gt;The apriori is an association algo that was proposed in the early 1900s. It works by discovering certain rules (using metrics such as support and confidence) using certain thresholds. A nice overview of the steps in the algorithm is outlined in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Apriori_algorithm&#34;&gt;wikipedia&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;In R you would use the &lt;code&gt;apriori&lt;/code&gt; function in the &lt;code&gt;arules&lt;/code&gt; or &lt;code&gt;arulesViz&lt;/code&gt; package to create a set of rules. It is also worth noting that all objects created using the package are &lt;code&gt;S4&lt;/code&gt; class objects. This means you can extract the elements using the &lt;code&gt;@&lt;/code&gt; symbol.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rules &amp;lt;- apriori(transactions, parameter=list(support=0.003, confidence=0.85))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##        0.85    0.1    1 none FALSE            TRUE       5   0.003      1
##  maxlen target   ext
##      10  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 58 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[8719 item(s), 19574 transaction(s)] done [0.06s].
## sorting and recoding items ... [1737 item(s)] done [0.01s].
## creating transaction tree ... done [0.01s].
## checking subsets of size 1 2 3 4 5 6 7 done [0.18s].
## writing ... [2812 rule(s)] done [0.01s].
## creating S4 object  ... done [0.01s].&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(rules)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## set of 2812 rules
## 
## rule length distribution (lhs + rhs):sizes
##   2   3   4   5   6   7 
##  65 339 989 944 431  44 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   2.000   4.000   5.000   4.522   5.000   7.000 
## 
## summary of quality measures:
##     support           confidence          lift        
##  Min.   :0.003014   Min.   :0.8500   Min.   :  9.027  
##  1st Qu.:0.003167   1st Qu.:0.8714   1st Qu.: 21.061  
##  Median :0.003423   Median :0.8986   Median : 24.704  
##  Mean   :0.003919   Mean   :0.9070   Mean   : 40.604  
##  3rd Qu.:0.004036   3rd Qu.:0.9365   3rd Qu.: 59.330  
##  Max.   :0.020691   Max.   :1.0000   Max.   :326.233  
## 
## mining info:
##          data ntransactions support confidence
##  transactions         19574   0.003       0.85&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;inspect-the-rules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Inspect the rules&lt;/h1&gt;
&lt;p&gt;The rules are extracted using the &lt;code&gt;inspect&lt;/code&gt; function. I sorted them by lift. I like the style of output because it is very intuitive and straight-forward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(head(sort(rules, by =&amp;quot;lift&amp;quot;),10),linebreak = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     lhs                             rhs                   support   
## [1] {SET 3 RETROSPOT TEA}        =&amp;gt; {SUGAR}               0.01512210
## [2] {SUGAR}                      =&amp;gt; {SET 3 RETROSPOT TEA} 0.01512210
## [3] {COFFEE,SET 3 RETROSPOT TEA} =&amp;gt; {SUGAR}               0.01512210
## [4] {COFFEE,SUGAR}               =&amp;gt; {SET 3 RETROSPOT TEA} 0.01512210
## [5] {BACK DOOR}                  =&amp;gt; {KEY FOB}             0.01185246
## [6] {SHED}                       =&amp;gt; {KEY FOB}             0.01312966
## [7] {SET 3 RETROSPOT TEA}        =&amp;gt; {COFFEE}              0.01512210
## [8] {SUGAR}                      =&amp;gt; {COFFEE}              0.01512210
## [9] {SET 3 RETROSPOT TEA,SUGAR}  =&amp;gt; {COFFEE}              0.01512210
##     confidence lift    
## [1] 1          66.12838
## [2] 1          66.12838
## [3] 1          66.12838
## [4] 1          66.12838
## [5] 1          52.61828
## [6] 1          52.61828
## [7] 1          51.10705
## [8] 1          51.10705
## [9] 1          51.10705&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;removing-subsets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Removing subsets&lt;/h2&gt;
&lt;p&gt;If we look at the first two rules above we see what subsets mean. These rules are identical with only the LHS and RHS reversed and are therefore redundant. We might want to get rid of these rules. I implemented the codes from this &lt;a href=&#34;http://r-statistics.co/Association-Mining-With-R.html&#34;&gt;source&lt;/a&gt; to remove the subsets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#smaller rule set for the purpose of demonstration
rules.small &amp;lt;- apriori(transactions, parameter=list(support=0.001, confidence=0.95,maxlen = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport maxtime support minlen
##        0.95    0.1    1 none FALSE            TRUE       5   0.001      1
##  maxlen target   ext
##       2  rules FALSE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 19 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[8719 item(s), 19574 transaction(s)] done [0.08s].
## sorting and recoding items ... [2622 item(s)] done [0.01s].
## creating transaction tree ... done [0.01s].
## checking subsets of size 1 2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in apriori(transactions, parameter = list(support = 0.001,
## confidence = 0.95, : Mining stopped (maxlen reached). Only patterns up to a
## length of 2 returned!&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  done [0.13s].
## writing ... [52 rule(s)] done [0.02s].
## creating S4 object  ... done [0.01s].&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(rules.small[1:20]) # rules 14,15,18 and 19 are redundant&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      lhs                         rhs                 support    
## [1]  {BILLBOARD FONTS DESIGN} =&amp;gt; {WRAP}              0.001481557
## [2]  {BLACK TEA}              =&amp;gt; {SUGAR JARS}        0.002350056
## [3]  {BLACK TEA}              =&amp;gt; {COFFEE}            0.002350056
## [4]  {WHITE TEA}              =&amp;gt; {SUGAR JARS}        0.003269643
## [5]  {WHITE TEA}              =&amp;gt; {COFFEE}            0.003269643
## [6]  {WOBBLY CHICKEN}         =&amp;gt; {DECORATION}        0.001685910
## [7]  {WOBBLY CHICKEN}         =&amp;gt; {METAL}             0.001685910
## [8]  {DECOUPAGE}              =&amp;gt; {GREETING CARD}     0.001379381
## [9]  {SUGAR JARS}             =&amp;gt; {COFFEE}            0.004700112
## [10] {WOBBLY RABBIT}          =&amp;gt; {DECORATION}        0.001992439
## [11] {WOBBLY RABBIT}          =&amp;gt; {METAL}             0.001992439
## [12] {FUNK MONKEY}            =&amp;gt; {ART LIGHTS}        0.002196792
## [13] {ART LIGHTS}             =&amp;gt; {FUNK MONKEY}       0.002196792
## [14] {TRAY}                   =&amp;gt; {BREAKFAST IN BED}  0.003422908
## [15] {BREAKFAST IN BED}       =&amp;gt; {TRAY}              0.003422908
## [16] {NURSERY A}              =&amp;gt; {C PAINTED LETTERS} 0.003065291
## [17] {C PAINTED LETTERS}      =&amp;gt; {NURSERY A}         0.003065291
## [18] {NURSERY A}              =&amp;gt; {B}                 0.003065291
## [19] {B}                      =&amp;gt; {NURSERY A}         0.003065291
## [20] {C PAINTED LETTERS}      =&amp;gt; {B}                 0.003065291
##      confidence lift     
## [1]  1          631.41935
## [2]  1          212.76087
## [3]  1           51.10705
## [4]  1          212.76087
## [5]  1           51.10705
## [6]  1          343.40351
## [7]  1          343.40351
## [8]  1          230.28235
## [9]  1           51.10705
## [10] 1          343.40351
## [11] 1          343.40351
## [12] 1          455.20930
## [13] 1          455.20930
## [14] 1          292.14925
## [15] 1          292.14925
## [16] 1          326.23333
## [17] 1          326.23333
## [18] 1          326.23333
## [19] 1          326.23333
## [20] 1          326.23333&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subsetRules &amp;lt;- which(colSums(is.subset(rules.small, rules.small)) &amp;gt; 1) # get subset rules in vector
length(subsetRules)  # 24 redundant pairs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rules.new &amp;lt;- rules.small[-subsetRules] # remove subset rules.
inspect(rules.new[1:20])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      lhs                         rhs                support     confidence
## [1]  {BILLBOARD FONTS DESIGN} =&amp;gt; {WRAP}             0.001481557 1         
## [2]  {BLACK TEA}              =&amp;gt; {SUGAR JARS}       0.002350056 1         
## [3]  {BLACK TEA}              =&amp;gt; {COFFEE}           0.002350056 1         
## [4]  {WHITE TEA}              =&amp;gt; {SUGAR JARS}       0.003269643 1         
## [5]  {WHITE TEA}              =&amp;gt; {COFFEE}           0.003269643 1         
## [6]  {WOBBLY CHICKEN}         =&amp;gt; {DECORATION}       0.001685910 1         
## [7]  {WOBBLY CHICKEN}         =&amp;gt; {METAL}            0.001685910 1         
## [8]  {DECOUPAGE}              =&amp;gt; {GREETING CARD}    0.001379381 1         
## [9]  {SUGAR JARS}             =&amp;gt; {COFFEE}           0.004700112 1         
## [10] {WOBBLY RABBIT}          =&amp;gt; {DECORATION}       0.001992439 1         
## [11] {WOBBLY RABBIT}          =&amp;gt; {METAL}            0.001992439 1         
## [12] {PINK  SPOTS}            =&amp;gt; {SWISS ROLL TOWEL} 0.001379381 1         
## [13] {STICKY GORDON}          =&amp;gt; {GREETING CARD}    0.001328293 1         
## [14] {OVERCROWDED POOL.}      =&amp;gt; {GREETING CARD}    0.001328293 1         
## [15] {CHOCOLATE  SPOTS}       =&amp;gt; {SWISS ROLL TOWEL} 0.003167467 1         
## [16] {GARAGE DESIGN}          =&amp;gt; {KEY FOB}          0.005670788 1         
## [17] {LIGHT PINK}             =&amp;gt; {FEATHER PEN}      0.005006641 1         
## [18] {ELEPHANT}               =&amp;gt; {BIRTHDAY CARD}    0.006692551 1         
## [19] {CUPCAKE SINGLE HOOK}    =&amp;gt; {METAL SIGN}       0.004342495 1         
## [20] {FRONT  DOOR}            =&amp;gt; {KEY FOB}          0.007561050 1         
##      lift     
## [1]  631.41935
## [2]  212.76087
## [3]   51.10705
## [4]  212.76087
## [5]   51.10705
## [6]  343.40351
## [7]  343.40351
## [8]  230.28235
## [9]   51.10705
## [10] 343.40351
## [11] 343.40351
## [12] 283.68116
## [13] 230.28235
## [14] 230.28235
## [15] 283.68116
## [16]  52.61828
## [17]  82.24370
## [18]  71.17818
## [19]  84.37069
## [20]  52.61828&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the rules no longer appear!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tuning-the-apriori&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tuning the apriori&lt;/h2&gt;
&lt;p&gt;One can further tune the parameters inside the &lt;code&gt;apriori&lt;/code&gt; function. Here are 2 important adjustments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;maxlen&lt;/code&gt;: The max len is specified within the &lt;code&gt;parameter&lt;/code&gt; list. This allows you to specify the maximum length of items to be considered within each basket. It is a good idea to specify max len because it is not very intuitive to see the relationship between too many items within each basket.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;minlen&lt;/code&gt;: Just as you wouldn’t want too many items, you wouldn’t want too few either. One item in a basket does not make sense. The minlen should be set to 2.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;appearence&lt;/code&gt;: This useful option allows you to specify a list where you can filter the rules created by a specific item. This could be useful in creating network plots that will then show these associations.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tuned.aprior &amp;lt;- apriori(transactions, 
                        # list of parameters that includes max and min length      
                        parameter=list(support=0.001, confidence=0.1, maxlen = 4, minlen = 2),
                        # control the appearence
                        appearance = list(default=&amp;quot;rhs&amp;quot;,lhs=&amp;quot;PARTY BUNTING&amp;quot;),
                        control = list(verbose=F))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;rhs&lt;/code&gt; within the&lt;code&gt;appearence&lt;/code&gt; option controls the fact that Party Bunting appears only on the left hand side of the rules. These rules show which items were bought after someone bought Party Bunting. And obviously you can reverse this procedure to inspect what items are bought &lt;em&gt;before&lt;/em&gt; Party Bunting.Inspect the elements:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inspect(tuned.aprior[1:10])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      lhs                rhs                                     support confidence     lift
## [1]  {PARTY BUNTING} =&amp;gt; {TEA TIME PARTY BUNTING}            0.009246960  0.1324067 7.690591
## [2]  {PARTY BUNTING} =&amp;gt; {PAPER BUNTING RETROSPOT}           0.008429549  0.1207023 4.130465
## [3]  {PARTY BUNTING} =&amp;gt; {LUNCH BAG APPLE DESIGN}            0.008123020  0.1163131 2.776479
## [4]  {PARTY BUNTING} =&amp;gt; {ASSORTED COLOUR BIRD ORNAMENT}     0.009860018  0.1411851 2.179461
## [5]  {PARTY BUNTING} =&amp;gt; {ALARM CLOCK BAKELIKE RED}          0.007254521  0.1038771 2.449747
## [6]  {PARTY BUNTING} =&amp;gt; {SPOTTY BUNTING}                    0.019873301  0.2845647 5.814270
## [7]  {PARTY BUNTING} =&amp;gt; {LUNCH BAG SUKI DESIGN}             0.008429549  0.1207023 2.819363
## [8]  {PARTY BUNTING} =&amp;gt; {HEART OF WICKER SMALL}             0.007305609  0.1046086 2.237824
## [9]  {PARTY BUNTING} =&amp;gt; {SET OF 4 PANTRY JELLY MOULDS}      0.007816491  0.1119239 2.649092
## [10] {PARTY BUNTING} =&amp;gt; {WOODEN PICTURE FRAME WHITE FINISH} 0.008174109  0.1170446 2.679569&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-rules&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing rules&lt;/h1&gt;
&lt;p&gt;One may be interested in inspecting the relationship between the computed lift, confidence and support. Personally, these graphs are not very informative especially when there are thousands of rules. One use of these plots though are that they help coming up with appropriate values of support or confidence while applying the apriori algorithms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

quality &amp;lt;- rules@quality  # extract the information

ggplot(quality, aes(support,confidence))+
  geom_point(col = my2cols[2],alpha = 0.5)+
  labs(title = &amp;quot;Support Vs Confidence&amp;quot;)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-what-s-in-a-basket_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(quality, aes(support,lift, col = confidence))+
  geom_point(alpha = 0.1)+
  scale_color_gradient(low = &amp;quot;#00AFBB&amp;quot;, high = &amp;quot;#FC3E07&amp;quot;)+
  labs(title = &amp;quot;Lift vs Support&amp;quot;)+
  coord_cartesian(ylim = c(0,150),xlim = c(0.003,0.01))+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-what-s-in-a-basket_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The package &lt;code&gt;ArulesViz&lt;/code&gt; provides a host of other visualizations plots which might be worth exploring. Using them is pretty straightforward and you can find them in the official &lt;a href=&#34;https://cran.r-project.org/web/packages/arulesViz/arulesViz.pdf&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-assocations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualizing Assocations&lt;/h1&gt;
&lt;p&gt;One can also visually inspect the association for instance in the party bunting case.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(tuned.aprior,method=&amp;quot;graph&amp;quot;,shading=NA)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-07-what-s-in-a-basket_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Carbon Dioxide Emissions</title>
      <link>/post/carbon-dioxide-emmissions/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/carbon-dioxide-emmissions/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Global warming is real. 100%. NASA came out with a &lt;a href=&#34;https://www.nasa.gov/press/goddard/2014/november/nasa-computer-model-provides-a-new-portrait-of-carbon-dioxide/&#34;&gt;beautiful simulation&lt;/a&gt; of carbon dioxide emissions for countries all around the world. Ever since then, I was eager to replicate this in R. Of course I couldn’t make it as beautiful as theirs but nevertheless, the final picture I made came out great by R standards. I found some good data on carbon dioxide emissions from the WRI &lt;a href=&#34;http://datasets.wri.org/dataset/cait-country&#34;&gt;website&lt;/a&gt;. I was able to use the data to create an animation with &lt;code&gt;ggplot2&lt;/code&gt;. A great way to perform animations with &lt;code&gt;ggplot2&lt;/code&gt; is to use the &lt;code&gt;gganimate&lt;/code&gt; package. But the package is filled with bugs and fails to run on many occasions. The &lt;code&gt;gganimate&lt;/code&gt; function is just a wrapper around imagemagick. You can easily create the frames, save them in a drive and just run imagemagick from your console. But if you do manage to make &lt;code&gt;gganimate&lt;/code&gt; run on your R-studio then go ahead use that option instead.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;div id=&#34;carbon&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Carbon&lt;/h2&gt;
&lt;p&gt;This is the carbon dioxide emission data by country and Year.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon &amp;lt;- read.csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/Projects/Carbon Emmissions/carbon.csv&amp;quot;)
colnames(carbon)[3] &amp;lt;- &amp;quot;emission&amp;quot;

knitr::kable(tail(carbon,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Year&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;emission&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30828&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Vietnam&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;160.0705&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;30829&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;World&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34389.5959&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30830&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Yemen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25.9220&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;30831&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Zambia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.4472&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;30832&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Zimbabwe&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2013&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.3572&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rename USA and RUSSIA
carbon$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, carbon$Country)
carbon$Country &amp;lt;- gsub(&amp;quot;Russian Federation&amp;quot;, &amp;quot;Russia&amp;quot;, carbon$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;world-map&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;World Map&lt;/h2&gt;
&lt;p&gt;World data with latitude and longitude values is easily available using the &lt;code&gt;map_data&lt;/code&gt; function. You need to convert it into a data frame. I removed Antarctica because data for carbon dioxide emissions was not available for Antarctica.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;world &amp;lt;- map_data(&amp;#39;world&amp;#39;) %&amp;gt;% 
                   data.frame() %&amp;gt;%
                   select(1:3,5) %&amp;gt;%
                   filter(region != &amp;quot;Antarctica&amp;quot;) %&amp;gt;%
                   rename(Country = region)

knitr::kable(head(world,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;long&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;group&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.89912&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.45200&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.89571&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.42300&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-69.94219&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.43853&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-70.00415&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.50049&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;-70.06612&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.54697&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Aruba&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Transformations&lt;/h1&gt;
&lt;p&gt;I noticed there was a lot of missing values for the years before 1950. So I filtered those years out. I also converted &lt;code&gt;Country&lt;/code&gt; into factors. &lt;em&gt;Please don’t convert the years into factors&lt;/em&gt;. We would take advantage of the numeric nature to loop through the years and create separate images.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carbon &amp;lt;- carbon %&amp;gt;%
              filter(Year &amp;gt; 1949) %&amp;gt;%
              mutate(Country = as.factor(Country))
              

carbon[is.na(carbon)] &amp;lt;- 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualization&lt;/h1&gt;
&lt;p&gt;The following codes would create 64 frame (or plots), one for each Year and save it to the current working directory. It’s pretty straight forward. The first few lines is just a fancy way to create the names for the &lt;code&gt;jpeg&lt;/code&gt;s as they are created. Then I extract the &lt;code&gt;Year&lt;/code&gt; one by one and combine it with the world data. The penultimate step is to create the plot in &lt;code&gt;ggplot2&lt;/code&gt;. The final trick is to use the &lt;code&gt;ggsave&lt;/code&gt; function to save the &lt;code&gt;jpegs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After you have saved the plots in the current working directory, you need to download &lt;a href=&#34;https://www.imagemagick.org/script/download.php&#34;&gt;imagemagick&lt;/a&gt; on your device. Then you need to go your command line (cmd for windows users) and after that your need to navigate to the working directory using &lt;code&gt;cd&lt;/code&gt;. After you change the directory, you need to type this:&lt;br /&gt;
&lt;code&gt;magick convert *.jpeg -delay 10 -loop 0 carbon.gif&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;frames &amp;lt;- length(unique(carbon$Year))

windowsFonts(Times=windowsFont(&amp;quot;Times New Roman&amp;quot;))

for(i in 1:frames){
  # creating a name for each plot file with leading zeros
  if (i &amp;lt; 10) {name = paste(&amp;#39;000&amp;#39;,i,&amp;#39;.jpeg&amp;#39;,sep=&amp;#39;&amp;#39;)}
  if (i &amp;lt; 100 &amp;amp;&amp;amp; i &amp;gt;= 10) {name = paste(&amp;#39;00&amp;#39;,i,&amp;#39;.jpeg&amp;#39;, sep=&amp;#39;&amp;#39;)}
  if (i &amp;gt;= 100) {name = paste(&amp;#39;0&amp;#39;, i,&amp;#39;.jpeg&amp;#39;, sep=&amp;#39;&amp;#39;)}
  
  data &amp;lt;- carbon %&amp;gt;%
               filter(Year == (1949+i))
  
  combine &amp;lt;- left_join(world,data,&amp;quot;Country&amp;quot;)
  
  # create plot
  g &amp;lt;- ggplot(combine, aes(x = long,y = lat,group = group))+
           geom_polygon(aes(fill = emission))+
           geom_path()+ 
           scale_fill_gradientn( name = &amp;quot;Carbon Emmissions Level&amp;quot;,
                                 colours = rev(heat.colors(10)),
                                 na.value = &amp;quot;grey90&amp;quot;,
                                 limits = c(0, 7e3),
                                 guide = guide_legend( keyheight = unit(3, units = &amp;quot;mm&amp;quot;), 
                                                       keywidth = unit(12, units = &amp;quot;mm&amp;quot;),
                                                       label.position = &amp;quot;bottom&amp;quot;, 
                                                       title.position = &amp;#39;top&amp;#39;, 
                                                       nrow = 1))+
           labs(title = paste(&amp;#39;Emissions in the year&amp;#39;,1949+i),
                subtitle = &amp;quot;Carbon Dioxide Emmission since 1950&amp;quot;, 
                caption = &amp;quot;World Resources Institute&amp;quot;,
                x = &amp;quot;Longitude&amp;quot;,y = &amp;quot;Latitude&amp;quot;)+
           theme(text = element_text(color = &amp;quot;#22211d&amp;quot;), 
                 plot.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA), 
                 panel.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA), 
                 legend.background = element_rect(fill = &amp;quot;#f5f5f2&amp;quot;, color = NA),
                 plot.title = element_text(family = &amp;quot;Times&amp;quot;,
                                           size = 18, 
                                           hjust = 0.01, 
                                           color = &amp;quot;#4e4d47&amp;quot;, 
                                           margin = margin(b = -0.1, t = 0.4, l = 2, unit = &amp;quot;cm&amp;quot;)),
                plot.subtitle = element_text(size= 15, 
                                             hjust = 0.01, 
                                             color = &amp;quot;#4e4d47&amp;quot;, 
                                             margin = margin(b = -0.1, t = 0.43, l = 2, unit = &amp;quot;cm&amp;quot;)),
                plot.caption = element_text( size = 12, 
                                             color = &amp;quot;#4e4d47&amp;quot;, 
                                             margin = margin(b = 0.3, r=-99, unit = &amp;quot;cm&amp;quot;) ),
                legend.position = c(0.7, 0.09))
  
  # print and save
  # ggsave(name,width = 40, height = 20, units = &amp;quot;cm&amp;quot;) You would need to uncomment this
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find the gif &lt;a href=&#34;https://gfycat.com/gifs/detail/ImaginaryDistortedAlaskajingle&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Beautiful!&lt;/p&gt;
&lt;p&gt;Its not as fancy as the NASA plot but nonetheless, it’s pretty accurate. Notice the colors for USA and China. Right towards the end USA manages to reduce the emission levels but China’s emission levels keep on rising!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nobel Laureates</title>
      <link>/post/nobel-laureates/</link>
      <pubDate>Sat, 06 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nobel-laureates/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;I was always curious to know if there is any relationship between the Intelligent Quotient (IQ) levels and the number of nobel laureates a country has produced. After I found the &lt;a href=&#34;https://www.kaggle.com/nobelfoundation/nobel-laureates&#34;&gt;Nobel Prize&lt;/a&gt; data set on Kaggle, I was eager to check how the numbers matched up against the IQ levels of a country. I found some average IQ data (from 1990s to 2010) for several country from this &lt;a href=&#34;https://www.worlddata.info/iq-by-country.php&#34;&gt;website&lt;/a&gt; and I was able to scrap it out to match it up with the nobel prize data. The results were quite surprising (especially for me and I will explain why). I decided to extend the plot to include more features. Surely the number of nobel laureates a country produces has to depend on how advanced the country is. A few months ago when I was working to deliver a lecture on &lt;code&gt;ggplot2&lt;/code&gt; at my school I had stumbled upon a &lt;strong&gt;Human Development Index&lt;/strong&gt; data set from this website. HDI is a good indicator of how advanced the country is. When I added this aesthetic to the plot the results finally made more sense. More after the plot below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-libraries&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The libraries&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
library(readr)
library(ggplot2)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The data&lt;/h1&gt;
&lt;div id=&#34;iq-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IQ data&lt;/h2&gt;
&lt;p&gt;Here’s a look at the IQ data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iq &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Visuals/Nobel Prize/iq.csv&amp;quot;, col_types = cols(`Daily max temperature` = col_skip(), `Education expenditures per capita` = col_number(), Income = col_number()))

# rename the 5th column
colnames(iq)[5] &amp;lt;- &amp;quot;exp_per_capita&amp;quot;

knitr::kable(head(iq,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Rank&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;IQ&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Income&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;exp_per_capita&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Singapore&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;935&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hong Kong&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;108&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26057&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;South Korea&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14077&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;538&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Taiwan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;106&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Japan&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37244&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1263&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;nobel-prize-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nobel Prize Data&lt;/h2&gt;
&lt;p&gt;The Nobel Prize Data has several variables. I was interested only in the &lt;code&gt;bornCountry&lt;/code&gt; variable. I made the assumption that for a nobel prize winner who was born in a country, the credits of the prize went to that country even if he/she had moved out later on in life. Also if the name of the country changed over time (some cities belong to different countries now), I preserved the older membership since the person was originally born in that country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nobel &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Visuals/Nobel Prize/nobel_prize_by_winner.csv&amp;quot;)
glimpse(nobel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 972
## Variables: 20
## $ id                &amp;lt;int&amp;gt; 846, 846, 783, 230, 918, 428, 773, 597, 615,...
## $ firstname         &amp;lt;chr&amp;gt; &amp;quot;Elinor&amp;quot;, &amp;quot;Elinor&amp;quot;, &amp;quot;Wangari Muta&amp;quot;, &amp;quot;Dorothy...
## $ surname           &amp;lt;chr&amp;gt; &amp;quot;Ostrom&amp;quot;, &amp;quot;Ostrom&amp;quot;, &amp;quot;Maathai&amp;quot;, &amp;quot;Hodgkin&amp;quot;, &amp;quot;T...
## $ born              &amp;lt;chr&amp;gt; &amp;quot;8/7/1933&amp;quot;, &amp;quot;8/7/1933&amp;quot;, &amp;quot;4/1/1940&amp;quot;, &amp;quot;5/12/19...
## $ died              &amp;lt;chr&amp;gt; &amp;quot;6/12/2012&amp;quot;, &amp;quot;6/12/2012&amp;quot;, &amp;quot;9/25/2011&amp;quot;, &amp;quot;7/29...
## $ bornCountry       &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;Kenya&amp;quot;, &amp;quot;Egypt&amp;quot;, &amp;quot;China&amp;quot;, &amp;quot;US...
## $ bornCountryCode   &amp;lt;chr&amp;gt; &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;KE&amp;quot;, &amp;quot;EG&amp;quot;, &amp;quot;CN&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;IR&amp;quot;, &amp;quot;I...
## $ bornCity          &amp;lt;chr&amp;gt; &amp;quot;Los Angeles, CA&amp;quot;, &amp;quot;Los Angeles, CA&amp;quot;, &amp;quot;Nyeri...
## $ diedCountry       &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;Kenya&amp;quot;, &amp;quot;United Kingdom&amp;quot;, NA,...
## $ diedCountryCode   &amp;lt;chr&amp;gt; &amp;quot;US&amp;quot;, &amp;quot;US&amp;quot;, &amp;quot;KE&amp;quot;, &amp;quot;GB&amp;quot;, NA, &amp;quot;US&amp;quot;, NA, &amp;quot;IT&amp;quot;, ...
## $ diedCity          &amp;lt;chr&amp;gt; &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Nairo...
## $ gender            &amp;lt;chr&amp;gt; &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;female&amp;quot;, &amp;quot;fem...
## $ year              &amp;lt;int&amp;gt; 2009, 2009, 2004, 1964, 2015, 1983, 2003, 19...
## $ category          &amp;lt;chr&amp;gt; &amp;quot;economics&amp;quot;, &amp;quot;economics&amp;quot;, &amp;quot;peace&amp;quot;, &amp;quot;chemistr...
## $ overallMotivation &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ share             &amp;lt;int&amp;gt; 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,...
## $ motivation        &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;for her analysis of economic governance, ...
## $ name              &amp;lt;chr&amp;gt; &amp;quot;Indiana University&amp;quot;, &amp;quot;Arizona State Univers...
## $ city              &amp;lt;chr&amp;gt; &amp;quot;Bloomington, IN&amp;quot;, &amp;quot;Tempe, AZ&amp;quot;, NA, &amp;quot;Oxford&amp;quot;...
## $ country           &amp;lt;chr&amp;gt; &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, NA, &amp;quot;United Kingdom&amp;quot;, &amp;quot;China&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;human-development-index&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Human Development Index&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HDI &amp;lt;- read_excel(&amp;quot;C:/Users/routh/Desktop/Study Materials/Projects/Basic Workshop/HDI.xlsx&amp;quot;, col_types = c(&amp;quot;text&amp;quot;, &amp;quot;numeric&amp;quot;, &amp;quot;blank&amp;quot;))

knitr::kable(head(HDI,5))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Country&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;HDI&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Norway&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9494228&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Australia&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9386795&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Switzerland&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9391309&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Germany&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9256689&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Denmark&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9246494&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;No explanation needed here!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-munging&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Munging&lt;/h1&gt;
&lt;p&gt;I need to take exactly what I need from each of the data set and merge them to make the final plot. From the &lt;code&gt;nobel&lt;/code&gt; data set, I extracted the &lt;code&gt;bornCountry&lt;/code&gt; column to and found the total count by country.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nobel.prize &amp;lt;- nobel %&amp;gt;%
                 mutate(bornCountry = gsub(&amp;quot;\\s*\\([^\\)]+\\)&amp;quot;,&amp;quot;&amp;quot;,as.character(bornCountry)))%&amp;gt;%
                 group_by(bornCountry) %&amp;gt;%
                 summarise(count = n()) %&amp;gt;%
                 na.omit() %&amp;gt;%
                 rename(Country = bornCountry)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I extracted all the columns from the IQ data set and changed United States to USA to match it up with the other data sets. The &lt;code&gt;exp_per_capita&lt;/code&gt; stands for education expenditure per capita for all the countries in dollars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iq.country &amp;lt;- iq %&amp;gt;%
                 select(Country,IQ,exp_per_capita)
iq.country$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, iq.country$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, all the columns were used from the HDI data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HDI$Country &amp;lt;- gsub(&amp;quot;United States&amp;quot;, &amp;quot;USA&amp;quot;, HDI$Country)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualization&lt;/h1&gt;
&lt;p&gt;Finally we’re ready to merge the frames. A few things to note before you move on to the plots is that I rescaled the x and y variable on to a logarithmic scale to make the points closer and easier to interpret.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;combine &amp;lt;- list(nobel.prize,iq.country,HDI)%&amp;gt;%
                      purrr::reduce(left_join,&amp;quot;Country&amp;quot;) %&amp;gt;%
                      na.omit() %&amp;gt;%
                      mutate(log.count = log(count+1),log.iq = log(IQ+1),
                             rank = cut(HDI,c(0.61,0.84,0.89,0.95),c(&amp;quot;Low&amp;quot;,&amp;quot;Medium&amp;quot;,&amp;quot;High&amp;quot;))) %&amp;gt;%
                      filter(log.iq &amp;gt; 4.35 &amp;amp; log.count &amp;gt; 1)



my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
windowsFonts(Times=windowsFont(&amp;quot;Times New Roman&amp;quot;))

ggplot(combine, aes(x = log.count, y = log.iq, size = exp_per_capita, fill = rank))+
  geom_point(pch = 21)+
  ggrepel::geom_text_repel(aes(label = Country),
                           size = 3,
                           segment.color = &amp;quot;black&amp;quot;,
                           segment.size = 1,
                           force = 2,
                           arrow = arrow(angle = 4, length = unit(0.09,&amp;quot;lines&amp;quot;)))+
  theme_minimal(base_size = 10)+
  scale_fill_manual(&amp;quot;HDI\nRank&amp;quot;,values = my3cols)+
  guides(fill = guide_legend(override.aes = list(size=7)))+
  scale_size_continuous(name = &amp;quot;Education Expenditure\nPer Capita&amp;quot; ,range = c(3,6))+
  scale_x_continuous(name = &amp;quot;Logarithm Nobel Prize Count&amp;quot;,breaks = seq(0,6,0.5))+
  scale_y_continuous(name = &amp;quot;Logarithm Country IQ&amp;quot;,breaks = seq(4.2,5,0.05))+
  theme(legend.position = &amp;quot;top&amp;quot;,
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(family = &amp;quot;Times&amp;quot;,size = 18))+
  labs(title = &amp;quot;Intelligent Quotient vs Nobel Prize Count&amp;quot;, 
       subtitle = &amp;quot;Relationship between IQ levels,Nobel Prize count and the Human Development Index by country&amp;quot;,
       caption = &amp;quot;Source : Multiple&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-06-nobel-laureates_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plot above shows IQ of a country versus the number of nobel prize won. The points are sized according to the expenditure per capita and the countries are colored by how high or low the Human Development Index is.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I had initially thought India would be ahead in terms of IQ but the lower position is justified by the low HDI level.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Turns out there is a relationship between the number of nobel prizes and the IQ level.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some countries however don’t exhibit this relationship for instance Luxembourg and New Zealand.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a strong relationship however between HDI and IQ levels. We can see 3 clear stratas. This is also true for expenditure per capita. Countries higher up in the IQ levels have higher expenditures for expenditure.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Wise Old Tree</title>
      <link>/post/the-wise-old-tree/</link>
      <pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-wise-old-tree/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Gandalf is one of the primary fictional character in J.R.R Tolkien &lt;em&gt;Lord of the Ring&lt;/em&gt;. He is a wise old wizard who people turn to for council on matters that are tough to decide. A decision tree is a lot like Gandalf. The greatest gift of decision trees is that it makes extremely hard decisions easy to interpret. It’s what statisticians call &lt;em&gt;white box&lt;/em&gt;. We know exactly whats going on when a tree makes a decision. In this tutorial I will explain the mathematics behind the working of a decision tree and how to build a functioning decision tree in R that can predict.&lt;/p&gt;
&lt;div id=&#34;the-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The packages&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr) # read in data
library(dplyr) # data manipulation
library(tidyr) # data manipulation
library(ggplot2) # for data viz
library(rpart) # decision tree
library(party) # decision tree&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The data&lt;/h1&gt;
&lt;p&gt;I will be using the &lt;em&gt;Pokemon&lt;/em&gt; data set from &lt;a href=&#34;https://www.kaggle.com/abcsds/pokemon&#34;&gt;Kaggle&lt;/a&gt;. Pokemon is a media franchise owned by Nintendo. It became famous when the game was turned into a animation series where an ambitious young boy tries to capture creatures/Pokemon who help him win Pokemon-battles. In his quest to become the best Pokemon trainer, he comes across a variety of Pokemon but it was always his dream to capture the legendary Pokemon &lt;em&gt;MewTwo&lt;/em&gt;. A Pokemon is legendary when it’s extremely powerful (measured by its attacking or defense style) and is extremely rare.&lt;/p&gt;
&lt;p&gt;The Pokemon data set comes with all the Pokemon and their type and stats. We will use a decision tree to decide if the Pokemon is legendary or not. This would also help us realize what factors contribute to deciding if a Pokemon is legendary or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokemon &amp;lt;- read_csv(&amp;quot;C:/Users/routh/Desktop/Study Materials/My website/Trees/Pokemon.csv&amp;quot;, col_types = cols(`#` = col_skip()))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-inspection&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Inspection&lt;/h1&gt;
&lt;p&gt;Here are the columns in the data with their descriptions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Name:&lt;/strong&gt; Name of each Pokemon&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type 1:&lt;/strong&gt; Each Pokemon has a type, this determines weakness/resistance to attacks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type 2:&lt;/strong&gt; Some Pokemon are dual type and have 2&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Total:&lt;/strong&gt; sum of all stats that come after this, a general guide to how strong a Pokemon is&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HP:&lt;/strong&gt; hit points, or health, defines how much damage a Pokemon can withstand before fainting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Attack:&lt;/strong&gt; the base modifier for normal attacks (eg. Scratch, Punch)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Defense:&lt;/strong&gt; the base damage resistance against normal attacks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SP Atk:&lt;/strong&gt; special attack, the base modifier for special attacks (e.g. fire blast, bubble beam)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SP Def:&lt;/strong&gt; the base damage resistance against special attacks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; determines which Pokemon attacks first each round&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generation:&lt;/strong&gt; the generation the pokemon belongs to.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(pokemon)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Name              Type 1             Type 2              Total      
##  Length:800         Length:800         Length:800         Min.   :180.0  
##  Class :character   Class :character   Class :character   1st Qu.:330.0  
##  Mode  :character   Mode  :character   Mode  :character   Median :450.0  
##                                                           Mean   :435.1  
##                                                           3rd Qu.:515.0  
##                                                           Max.   :780.0  
##        HP             Attack       Defense          Sp. Atk      
##  Min.   :  1.00   Min.   :  5   Min.   :  5.00   Min.   : 10.00  
##  1st Qu.: 50.00   1st Qu.: 55   1st Qu.: 50.00   1st Qu.: 49.75  
##  Median : 65.00   Median : 75   Median : 70.00   Median : 65.00  
##  Mean   : 69.26   Mean   : 79   Mean   : 73.84   Mean   : 72.82  
##  3rd Qu.: 80.00   3rd Qu.:100   3rd Qu.: 90.00   3rd Qu.: 95.00  
##  Max.   :255.00   Max.   :190   Max.   :230.00   Max.   :194.00  
##     Sp. Def          Speed          Generation     Legendary        
##  Min.   : 20.0   Min.   :  5.00   Min.   :1.000   Length:800        
##  1st Qu.: 50.0   1st Qu.: 45.00   1st Qu.:2.000   Class :character  
##  Median : 70.0   Median : 65.00   Median :3.000   Mode  :character  
##  Mean   : 71.9   Mean   : 68.28   Mean   :3.324                     
##  3rd Qu.: 90.0   3rd Qu.: 90.00   3rd Qu.:5.000                     
##  Max.   :230.0   Max.   :180.00   Max.   :6.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(pokemon)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 800
## Variables: 12
## $ Name       &amp;lt;chr&amp;gt; &amp;quot;Bulbasaur&amp;quot;, &amp;quot;Ivysaur&amp;quot;, &amp;quot;Venusaur&amp;quot;, &amp;quot;VenusaurMega V...
## $ `Type 1`   &amp;lt;chr&amp;gt; &amp;quot;Grass&amp;quot;, &amp;quot;Grass&amp;quot;, &amp;quot;Grass&amp;quot;, &amp;quot;Grass&amp;quot;, &amp;quot;Fire&amp;quot;, &amp;quot;Fire&amp;quot;,...
## $ `Type 2`   &amp;lt;chr&amp;gt; &amp;quot;Poison&amp;quot;, &amp;quot;Poison&amp;quot;, &amp;quot;Poison&amp;quot;, &amp;quot;Poison&amp;quot;, NA, NA, &amp;quot;Fl...
## $ Total      &amp;lt;int&amp;gt; 318, 405, 525, 625, 309, 405, 534, 634, 634, 314, 4...
## $ HP         &amp;lt;int&amp;gt; 45, 60, 80, 80, 39, 58, 78, 78, 78, 44, 59, 79, 79,...
## $ Attack     &amp;lt;int&amp;gt; 49, 62, 82, 100, 52, 64, 84, 130, 104, 48, 63, 83, ...
## $ Defense    &amp;lt;int&amp;gt; 49, 63, 83, 123, 43, 58, 78, 111, 78, 65, 80, 100, ...
## $ `Sp. Atk`  &amp;lt;int&amp;gt; 65, 80, 100, 122, 60, 80, 109, 130, 159, 50, 65, 85...
## $ `Sp. Def`  &amp;lt;int&amp;gt; 65, 80, 100, 120, 50, 65, 85, 85, 115, 64, 80, 105,...
## $ Speed      &amp;lt;int&amp;gt; 45, 60, 80, 80, 65, 80, 100, 100, 100, 43, 58, 78, ...
## $ Generation &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
## $ Legendary  &amp;lt;chr&amp;gt; &amp;quot;False&amp;quot;, &amp;quot;False&amp;quot;, &amp;quot;False&amp;quot;, &amp;quot;False&amp;quot;, &amp;quot;False&amp;quot;, &amp;quot;False...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-transformations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data transformations&lt;/h1&gt;
&lt;p&gt;In a classification task, we need to convert response &lt;code&gt;Legendary&lt;/code&gt; to a factor along with &lt;code&gt;Generation&lt;/code&gt;. I will also create a variable &lt;code&gt;Dual&lt;/code&gt; to indicate if a Pokemon has dual nature. After that we can get rid of the irrelevant columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pokemon &amp;lt;- pokemon %&amp;gt;%
              mutate(Legendary = as.factor(Legendary),
                     Generation = as.factor(Generation),
                     Dual = factor(ifelse(is.na(`Type 1`) | is.na(`Type 2`),0,1))) %&amp;gt;%
              rename(sp.attack = `Sp. Atk`, sp.defence = `Sp. Def`) %&amp;gt;%
              select(4:13)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mechanics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Mechanics&lt;/h1&gt;
&lt;p&gt;Before moving on to how to build and predict a decision tree in R, we need to understand how a decision tree operates, the important terminologies, how to make it work better and most importantly how it decides what to chose.&lt;/p&gt;
&lt;div id=&#34;terminology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Terminology:&lt;/h2&gt;
&lt;p&gt;Decision Trees or &lt;strong&gt;CART&lt;/strong&gt; (Classification and Regression Trees) have the following elements:&lt;/p&gt;
&lt;p&gt;Take a look at this &lt;a href=&#34;https://clearpredictions.com/Images/Decision_Tree_2.png&#34;&gt;picture&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;strong&gt;root node&lt;/strong&gt; is where the first split in decision occurs. A &lt;strong&gt;decision or internal&lt;/strong&gt; node is the rest of the nodes following the root node where a decision is made. Finally the &lt;strong&gt;terminal or leaf&lt;/strong&gt; node is where the outcomes (based on the previous decisions) are finalized.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The initial splitting node is the &lt;strong&gt;parent&lt;/strong&gt; node to it’s &lt;strong&gt;children&lt;/strong&gt; nodes. The children form the branches of the whole tree&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-criterion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting Criterion&lt;/h2&gt;
&lt;p&gt;We know that splitting occurs at every node. But what splits? Or more important how does it split? The split occurs on the values of the predictor variables. A node &lt;em&gt;splits into&lt;/em&gt; two values (which is why it’s also called a binary tree). Technically though, the node can split into more than two values but most trees have binary splits.That brings us to the first kind of tree called the &lt;strong&gt;Recursive Partitioning Tree (RP)&lt;/strong&gt;. It does exactly what the name suggests. Refer to this nice &lt;a href=&#34;http://cdn.iopscience.com/images/1749-4699/5/1/015004/Full/csd422281fig1.jpg&#34;&gt;picture&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you would think about a decision tree with 2 variables, what a RP does is it partitions the space (containing values of the response) again and again until each small little partition has one kind or class of values of the response. That’s it! That’s all that a decision tree does. Brilliant yet simple! But there is one final hurdle - how does it decide what variable to pick first or at what value of a predictor variable it should ideally split on?&lt;/p&gt;
&lt;p&gt;To answer the first question, in an RP tree, there are a number of &lt;em&gt;criterions&lt;/em&gt; that can be used to split a node. &lt;em&gt;Information Gain&lt;/em&gt; is a popular splitting criterion that uses &lt;em&gt;entropy&lt;/em&gt;. Entropy is a fundamental concept of &lt;em&gt;Information Theory&lt;/em&gt;. Tbh, &lt;a href=&#34;https://en.wikipedia.org/wiki/Information_theory&#34;&gt;Info Theory&lt;/a&gt; is a huge topic by itself and (much) beyond the scope of this tutorial (and myself) for sure. Intuitively, entropy is &lt;em&gt;how much information is missing&lt;/em&gt;. If you asked me where I am from and I answer, “India”; there is lots of “entropy” in this answer. Translate this to DTs and splits, an entropy for a variable is how much information you are loosing if you pick a variable to split on out of many other variables. Mathematically, this is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[    E = \sum_{i=1}^c - p_i log_2(p_i)     \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Lets understand with an example. Lets say the repose variable is if I go out today or not. Arbitrarily, out of 14 instances (Lets say), I assign 9 to yes (I go out) and 5 to no. So what is the entropy for these decisions?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entropy &amp;lt;- function(vector){
  p = vector/sum(vector) # gives you the p_i&amp;#39;s
  sum(-(p*log2(p))) # the sum
}

entropy(c(9,5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.940286&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets ramp it up a little. Now I decide to go out depending on (a) If it is sunny or not or (b) if I’m feeling lazy or not. Here the two 2x2 table to help you visualize:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(4)
fake.data.sunny &amp;lt;- data.frame(sunny = sample(c(1,0),14,replace = T),
                              go.out = sample(c(1,0),14,replace = T))

with(fake.data.sunny,table(sunny,go.out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      go.out
## sunny 0 1
##     0 6 1
##     1 4 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fake.data.lazy &amp;lt;- data.frame(lazy = sample(c(1,0),14,replace = T),
                             go.out = sample(c(1,0),14,replace = T))

with(fake.data.lazy,table(lazy,go.out))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     go.out
## lazy 0 1
##    0 7 1
##    1 3 3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Entropy for two attributes (go out and sunny or go out and lazy) is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Entropy(Target|Variable) = \sum Entropy(Var)*Probability(Var)   \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This helpful function computes the entropy for a variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;entropy.var &amp;lt;- function(...){  # enter a list of vectors
  list.vector = list(...)
  p = purrr::map_dbl(list.vector,~sum(.x))
  prop = p/sum(p)
  sum(prop*(purrr::map_dbl(list.vector,entropy)),na.rm = T)
}

# for sunny
entropy.var(c(6,1),c(4,3))  # 6+1 &amp;amp; 4+3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7884505&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# for lazy
entropy.var(c(7,1),c(3,3)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.7391797&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Therefore &lt;strong&gt;Information Gain&lt;/strong&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Gain(Target|Variable) = Entrpoy(Target) - Entropy(Target|Variable)  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For sunny it would be &lt;span class=&#34;math inline&#34;&gt;\(0.94 - 0.788 = 0.15\)&lt;/span&gt; and for lazy it would be &lt;span class=&#34;math inline&#34;&gt;\(0.94 - 0.739 = 0.2\)&lt;/span&gt;. The decision tree now &lt;em&gt;selects the variable with the largest information gain&lt;/em&gt;. In this case, me going out or not would depend on if I was feeling lazy first (so true) and then if it was sunny.&lt;/p&gt;
&lt;p&gt;To answer the second question of what value it chose to split on? We can use Information Gain here too. For every value of a variable, the Information Gain is calculated and then the maximum IG is chosen as the best possible IG for that variable. This best possible IG is calculated for all variables. After that we would choose the variable with the &lt;em&gt;highest maximum&lt;/em&gt; IG and then the best split point (the one with the highest IG) for that variable is chosen as the point of split.&lt;/p&gt;
&lt;p&gt;Now, CARTs can also be used to build &lt;strong&gt;regression trees&lt;/strong&gt;, where the outcomes are not categorical. The splitting criterion for such trees is the &lt;strong&gt;MSE&lt;/strong&gt; for a particular predictor (p) on the response variable instead, where MSE is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[  MSE_p = 1/n\sum_{i=1}^n (\hat{Y_{i,p}} - Y_{i,p})    \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}\)&lt;/span&gt; is the mean response value for that region or partition. Obviously here, the variable with the &lt;em&gt;lowest&lt;/em&gt; MSE is then chosen as the splitting variable and the value of predictor with minimum possible MSE is chosen as the split point.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The second kind of tree is called &lt;strong&gt;Conditional Partitioning Tree&lt;/strong&gt;. Instead of using IG as splitting criterion, this tree is grown using hypothesis tests at every node and the variable that is most significant is chosen as the candidate variable to split on.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prunning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prunning&lt;/h2&gt;
&lt;p&gt;A DT grown using the process above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. A way to overcome the variance (overfitting) is to grow a smaller tree at the cost of lower bias.&lt;/p&gt;
&lt;p&gt;One possible way to tackle this is to build the tree only as long as the decrease in the RSS or increase in IG due to each split exceeds some (high) threshold. This strategy will result in smaller trees, but is too “risky” since a seemingly worthless split early on in the tree might be followed by a very good split-that later on.&lt;/p&gt;
&lt;p&gt;Another way is to grow the tree to full depth and the &lt;em&gt;prune&lt;/em&gt; it back to create a &lt;em&gt;subtree&lt;/em&gt; in a way that gives the lowest possible test error rate. Later on we will see how to calculate the CV error for a tree. If we record the CV error for all possible subtrees we can select the subtree with lowest error. However this might be a inefficient procedure especially for a larger tree with alot of internal nodes. With the &lt;code&gt;prune&lt;/code&gt; function in R we essentially tell &lt;code&gt;rpart&lt;/code&gt; to see if a split is improving the fit by an amount (specified by the &lt;strong&gt;CP or cost complexity&lt;/strong&gt; criterion). If it doesn’t the algorithm will not pursue further splitting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Prediction&lt;/h1&gt;
&lt;p&gt;Finally we are ready to implement this procedure in R. We will use the Pokemon data to predict if a Pokemon with specific features is Legendary or not.&lt;/p&gt;
&lt;div id=&#34;split-into-train-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split into train-test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_test_split &amp;lt;- function(data,percent,seed){
  set.seed(seed)
  rows = sample(nrow(data))
  data &amp;lt;- data[rows,]
  split = round(nrow(data)*percent)
  list(train = data[1:split,], test =  data[(split+1):nrow(data),])
}


list &amp;lt;- train_test_split(pokemon,0.7,123)
train &amp;lt;- list$train; test &amp;lt;- list$test&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recursive-partitioning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recursive partitioning&lt;/h2&gt;
&lt;p&gt;The recursive partitioning is implemented using the &lt;code&gt;rpart&lt;/code&gt; function in the &lt;code&gt;party&lt;/code&gt; package. You can also specify the categorical nature of response in &lt;code&gt;method&lt;/code&gt; argument. The &lt;code&gt;fancyRpartPlot&lt;/code&gt; in the rattle package helps visualize the tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rtree.fit &amp;lt;- rpart(formula = Legendary ~.,
                   method = &amp;quot;class&amp;quot;,
                   data = train) 

rattle::fancyRpartPlot(rtree.fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-02-the-wise-old-tree_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is R’s representation of the decision tree. We can see that not all variables were used to grow the tree. The most important variable was total points. Let’s take the first node. 92% of observations are not Legendary with the total points less than 580. The label (True/False) above these proportions indicates the way the nodes are voting while the numbers below indicates the composition of the node.&lt;/p&gt;
&lt;p&gt;Let’s move on to the next node. If the total is less than 580, you move &lt;em&gt;left&lt;/em&gt; and you reach the second node. Here 86% (of the total) of Pokemon are not Legendary. The tree stops because this node correctly votes all these Pokemon as non legendary (indicated by the 1 against 0). If we move to node 3 however, where the total is greater than 580, out of the 14% (do the math 86+14 = 100% of the data, which shows the binary partition) of the passengers, about 42% was correctly voted and the 58% was incorrectly voted as non legendary. Therefore, this node splits again. This process continues until all nodes (see the nodes below) are as pure as possible. Theoretically one could keep on going until all nodes are 100% pure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;controlled-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Controlled split&lt;/h2&gt;
&lt;p&gt;You can further control the splitting process using the &lt;code&gt;rpart.control&lt;/code&gt; option and &lt;code&gt;parms&lt;/code&gt; option. Here is a demonstration:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;control &amp;lt;- rpart.control(minsplit = 5, # min #obs to attempt splitting
                         minbucket = 15, 
                         cp = 0.07,
                         maxcompete = 4,
                         xval = 5, # number of cross validations
                         maxdepth = 15) # how deep should the tree grow on any node

control.tree &amp;lt;-  rpart(formula = Legendary ~.,
                   method = &amp;quot;class&amp;quot;,
                   data = train,
                   parms = list(prior = c(0.2,0.8), split = &amp;quot;information&amp;quot;),  # default is gini
                   control = control) 

rattle::fancyRpartPlot(control.tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-02-the-wise-old-tree_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-partitioning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional partitioning&lt;/h2&gt;
&lt;p&gt;Conditional partitioning are implemented using the &lt;code&gt;ctree&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctree.fit &amp;lt;- ctree(formula = Legendary ~.,
                   data = train)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;legendary-or-not&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Legendary or not?&lt;/h2&gt;
&lt;p&gt;Predictions on the test set are done using the generic &lt;code&gt;predict&lt;/code&gt; function.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Recursive Tree Fit&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recursive.prediction &amp;lt;- predict(rtree.fit, newdata = test, type = &amp;quot;class&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Conditional Tree Fit&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conditional.prediction &amp;lt;- predict(ctree.fit, newdata = test)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction-accuracy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction accuracy&lt;/h2&gt;
&lt;p&gt;Its pretty straightforward to compute the prediction accuracy. In this case, the accuracy in prediction is very high (0.94).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict.df &amp;lt;- data.frame(
  predictions = recursive.prediction,
  actual = test$Legendary
)

(t &amp;lt;- table(predict.df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            actual
## predictions False True
##       False   214    8
##       True      6   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;#39;Accuracy:&amp;#39;,round((t[1]+t[4])/(t[1]+t[2]+t[3]+t[4]),2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy: 0.94&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Using ROC plot&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If the outcome is probabilistic we could build a ROC curve to measure the accuracy. This version makes probabilistic predictions and the ROC is plotted to measure Area under the curve. The &lt;code&gt;prediction&lt;/code&gt; and &lt;code&gt;performance&lt;/code&gt; function from the &lt;code&gt;ROCR&lt;/code&gt; package is used to measure the TPR and FPR. In this case we see how good the classification was by the strong ROC curve.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rtree.fit.prob &amp;lt;- rpart(formula = Legendary ~.,
                   method = &amp;quot;anova&amp;quot;,
                   data = train)
recursive.prediction.prob &amp;lt;- predict(rtree.fit, newdata = test)


rtree_roc &amp;lt;- rtree.fit.prob %&amp;gt;%
      predict(newdata = test) %&amp;gt;%
      ROCR::prediction(test$Legendary) %&amp;gt;%
      ROCR::performance(&amp;quot;tpr&amp;quot;, &amp;quot;fpr&amp;quot;)

roc_df &amp;lt;- data.frame(
  FPR = rtree_roc@x.values[[1]],
  TPR = rtree_roc@y.values[[1]],
  cutoff = rtree_roc@alpha.values[[1]]
)

ggplot(roc_df,aes(x = FPR, y = TPR))+
  geom_point()+
  geom_line()+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-02-the-wise-old-tree_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;pruning&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pruning&lt;/h3&gt;
&lt;p&gt;Finally one can prune a tree using the &lt;code&gt;prune&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;my3cols &amp;lt;- c(&amp;quot;#E7B800&amp;quot;, &amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)
my2cols &amp;lt;- c(&amp;quot;#2E9FDF&amp;quot;, &amp;quot;#FC4E07&amp;quot;)

cp &amp;lt;- data.frame(rtree.fit$cptable)
best.cp &amp;lt;- cp[which.min(cp$xerror),&amp;quot;CP&amp;quot;]

cp %&amp;gt;%
  select(CP,nsplit,xerror,rel.error)%&amp;gt;%
  gather(key,value,-nsplit)%&amp;gt;%
  ggplot(aes(x = nsplit, y = value, col = key))+
  geom_point(size = 2)+
  geom_line(size = 1.1)+
  scale_color_manual(values = my3cols)+
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-02-the-wise-old-tree_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results show that 3 splits gives us the minimum xerror (related to the &lt;a href=&#34;https://en.wikipedia.org/wiki/PRESS_statistic&#34;&gt;PRESS&lt;/a&gt; error) at a CP value of 0.06 approximately. We can prune the tree using this CP value. If we choose to use relative error instead, we would choose 6 splits and a CP of 0.01 instead. Different error measures gives us different CP values and different number of splits. We will create the pruned tree by supplying the minimum CP value based on xerror and get the prediction accuracy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prune.tree &amp;lt;- prune(rtree.fit, cp = best.cp)

rattle::fancyRpartPlot(prune.tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-02-the-wise-old-tree_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions.prune &amp;lt;- predict(prune.tree, newdata = test, type = &amp;quot;class&amp;quot;)

predict.df.prune &amp;lt;- data.frame(
  predictions = predictions.prune,
  actual = test$Legendary
)

(t &amp;lt;- table(predict.df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            actual
## predictions False True
##       False   214    8
##       True      6   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(paste(&amp;#39;Accuracy:&amp;#39;,round((t[1]+t[4])/(t[1]+t[2]+t[3]+t[4]),2)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Accuracy: 0.94&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, prunning doesn’t really improve the accuracy in prediction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using Cross-Validation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Resampling methods such as Cross-Validation can also be used on the training data to pick out a CP value to prune and fit the best tree. The most convenient way to perform cross-validation is to use the &lt;code&gt;train&lt;/code&gt; function from the &lt;code&gt;caret&lt;/code&gt; package. The call to the object outputs the best CP (which in this case is picked according to the accuracy of 1-missclassification rate) value that can then be used to prune the tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tc &amp;lt;- caret::trainControl(&amp;quot;cv&amp;quot;,50,classProbs = T)

train.rpart &amp;lt;- caret::train(Legendary ~., 
                             data = train, 
                             method=&amp;quot;rpart&amp;quot;,
                             trControl=tc, 
                             tuneLength = 10, 
                             parms=list(split=&amp;#39;information&amp;#39;))

train.rpart&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## CART 
## 
## 560 samples
##   9 predictor
##   2 classes: &amp;#39;False&amp;#39;, &amp;#39;True&amp;#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (50 fold) 
## Summary of sample sizes: 549, 549, 549, 550, 549, 549, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa    
##   0.00000000  0.9359091  0.5002696
##   0.02962963  0.9322424  0.4419367
##   0.05925926  0.9392121  0.5448884
##   0.08888889  0.9410303  0.5786528
##   0.11851852  0.9448182  0.6275213
##   0.14814815  0.9448182  0.6404524
##   0.17777778  0.9464848  0.6612857
##   0.20740741  0.9358788  0.7028026
##   0.23703704  0.9393636  0.7327317
##   0.26666667  0.8984545  0.2406340
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was cp = 0.1777778.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GGPLOT2 Lecture</title>
      <link>/project/ggplot2-lecture/</link>
      <pubDate>Sun, 31 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/ggplot2-lecture/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;

&lt;p&gt;Never thought I&amp;rsquo;d have the opportunity to deliver a lecture on ggplot2 but thanks to &lt;a href=&#34;https://www.bgsu.edu/business/students/student-organizations/basic--business-analytics-student-informs-chapter-.html&#34; target=&#34;_blank&#34;&gt;BASIC&lt;/a&gt; I got the opportunity. There was healthy crowd which initially did make me nervous but I was able to focus on the task at hand and teach the basics of the wonderful package.&lt;/p&gt;

&lt;p&gt;It was an hour long lecture where I covered the basics of ggplot2: the &amp;lsquo;layer&amp;rsquo; composition of a ggplot, the hist, bar and point geom along with an advanced topic of &amp;lsquo;tidying&amp;rsquo; the data. Overall it was a fun experience and I was able to learn a lot from it myself. I hope the lecture was able to motivate fellow falcons to move away from base plotting system in R and explore the powerful ggplot2. Here is the &lt;a href=&#34;https://github.com/pallavr/ggplot2&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to the presentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Owens Illinois Student Case Competition</title>
      <link>/project/owens-illinoise-student-case-competition/</link>
      <pubDate>Thu, 28 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/owens-illinoise-student-case-competition/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;

&lt;p&gt;Our team (Me, Alex Frank and Yanan Tang) were declared runners up at the Owen Illinois Student Case competition at the &lt;a href=&#34;https://www.bgsu.edu/business/centers-and-institutes/center-for-business-analytics/business-analytics-symposium.html&#34; target=&#34;_blank&#34;&gt;Business Analytics Symposium&lt;/a&gt; at BGSU. Although we are sad we couldn&amp;rsquo;t win (even though we probably deserved it) we are delighted that our hard work paid off.&lt;/p&gt;

&lt;p&gt;This is a &lt;a href=&#34;https://github.com/pallavr/Owen-Illinois-Competition&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; to the github page where you can access the final results. We used state of the art univariate forecasting techniques such as ETS, traditional forecasting techniques such as ARIMA and Time Series Regression and also simple forecasting techniques such as seasonal naive model to forecast the future sales volume of the product line of Owens Illinois.&lt;/p&gt;

&lt;p&gt;For those who don&amp;rsquo;t know, &lt;a href=&#34;http://www.o-i.com/&#34; target=&#34;_blank&#34;&gt;Owens Illinois&lt;/a&gt; is one of the largest glass manufacturing firms in the United States. We are extremely lucky to have had the opportunity to analyze a large data set in R. The dimension of the data set made the project more challenging. We had to forecast for several products (about five) for each country using five different models. The experience was fascinating because personally, I was able to get first hand insight into what a large data set looks like and then execute a plan to finish the project. It was a huge learning curve for me.&lt;/p&gt;

&lt;p&gt;I certainly look forward to many more challenges in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
