---
title: Of Fortune Tellers and Crystal Balls
author: ''
date: '2018-01-07'
slug: of-fortune-tellers-and-crystal-balls
categories:
  - predictive analytics
tags:
  - Time Series
  - Forecasting
  - ggplot2
header:
  image: "headers/crystal.jpg"
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

Fortune telling is an ancient form of magic (not dark) and is art of predicting a person's life. They have crystal balls that help them delve into the future. (Un)Fortunately, the plethora of skeptics don't believe the practice simply because it isn't based on scientific facts. But what if I told you that fortune telling is all math? Don't believe me? If you read this chapter you'll realize why the practice of fore-seeing the future isn't exactly a fairy tale. 

# Load the packages

```{r,warning=FALSE,message=FALSE}
library(readr) # to read in data
library(readxl) # to read in the data
library(dplyr) # for data manipulation
library(tidyr) # for cleaning up data
library(ggplot2) # for data viz
library(fpp2) # time series 
library(forecast) # time series
```


# Time series Data

Time series data are a sequence of correlated points (denoted by $X_t$) indexed by uniformly separated elements of time. Therefore, time series data represents a  [stochastic](https://en.wikipedia.org/wiki/Stochastic_process) process. Another interesting fact is time series data is a special case of panel data. Panel/Longitudinal data is multidimensional data that involves measurement over time. Cross-sectional data is another instance of panel data which frequently occurs in economics. It's concerns measuring and comparing multiple subjects at the same point in time. 

An example of time series data is the weekly closing price for AT&T stocks obtained from the [datamarket](https://datamarket.com/data/set/22s6/weekly-closing-price-of-att-common-shares-1979#!ds=22s6&display=line) website. The example below shows how the AT&T stock prices behave as time series elements. 

```{r}
att <- read_excel("C:/Users/routh/Desktop/Study Materials/My website/Time series/att.xlsx")

ggplot(att,aes(Week,Price))+
  geom_line()+
  ggtitle("Time series plot of weekly closing prices of AT&T")+
  theme_minimal()
```


## Components of Time Series

1. **Trend:** Trend is the overall tendency of the time series data to move in a general upward or downward direction. The term trend is vague and has no mathematical basis. It is only used to give the data an overall direction.


2. **Seasonality:** Seasonality are *short*, *consistent* periodic pattern in the time series data that is *repeated* every time period. 

We can visualise the trend and seasonality using the `stl` function. For instance lets look at the time series decomposition of monthly sales volumes for anti-diabetic drugs in Australia:

```{r,warning=FALSE,message=FALSE}
season.decomp.ts <- (stl(a10, s.window="periodic")$time.series)
season.decomp.df <- timetk::tk_tbl(season.decomp.ts)

#convert to long form
season.decomp.df %>%
     gather(key, value, -index) %>%
     ggplot(aes(x = index, y = value))+
     geom_line()+
     facet_grid(key~., scales = "free")+
     theme_minimal()
     
```


We see and upward trend and small peaks along the way. The rise and fall around the peaks represent seasons that is repeated every calender year. Seasonality may be the result of external factors that influence the time series data - such as the AT&T stock prices; the rise and fall of which might be affected by sentiments of customers. Another cool way to visualize the seasons within each year:

```{r}
ggseasonplot(a10, polar = TRUE) + theme_minimal()
```


3. **Cyclicity:** This too has a rise and fall pattern like seasonality except cycles are *longer* and *inconsistant*. They are inconsistent in their magnitude as well as their repeat patterns. More often cycles are a part of a longer time series extending over decades.


## Type of Time series data

Based on behavior, time series can be categorized in two categories:

- **Frequency domain:** This approach assumes that the characteristics of a time series are explained by systematic or periodic movements that are observed in nature.

- **Time domain:** It is primary based on the assumption that adjacent time series objects are correlated.

This brings us to the world of ARIMA models. The *AutoRegressiveMovingAverage* models are the outcome of landmark work by Box and Jenkins. This is why a univariate ARIMA model is also referred to as a UBJ (univariate box-jenkins) model. ARIMA models fall under the time domain. UBJ models are broadly classified into two categories:

- **Stationary:** Stationarity can of two forms - mean stationary and variance stationary. As the name suggests, a time series that has a constant mean is mean stationary and it's variance stationary when the variance of $X_t$'s are independent of time.

- **Non Stationary:** Any time series that is either mean or variance non stationary is non stationary in nature.

For instance, the AT&T stock prices is a non stationary process over the 52 week period. However, the first difference of the above series becomes a stationary (approximately). The `diff` fucntion is used to compute the difference of a speacific order.

```{r}
diff_df <- data.frame(difference = diff(att$Price,1),time.index = 1:51)

ggplot(diff_df,aes(time.index,difference))+
   geom_line()+
   geom_point(col = "#FC4E07")+
   labs(title = "Stationarized AT&T Time series",y = "First order difference",x = "Time")+
   theme_minimal()
```

## White Noise

Almost all statistical concepts are not complete without defining a term for randomness that real life data exhibits. In time series the term 'white noise' refers to a set of uncorrelated random variables. It is also not unusual for most statistical ideas to be optimistic and consider the distribution of random variables as normal. Mathematically, they are represented as: $$Z_t \sim iid(0,\sigma^2)$$. 

This just means that they are independently and identically distributed with mean 0 and has a inherent variation. The source of this variation can be factors that affect the data points externally and internally. White noise is also not observed(as with any random component).Lets look at a simulated white noise:

```{r,echo=FALSE}
set.seed(3)
white_noise <- ts(rnorm(36))
autoplot(white_noise)+
   theme_minimal()+
   labs(title = "White Noise Series",y = "Value", x = "Time")
```

To observe the effect of white noise take a look at these images. The top image shows a simple cosine curve. The next image shows what happens when we introduce white noise in the curve.

```{r}
t <- 1:500;w <- rnorm(500,0,1)
c <- 2*cos(2*pi*t/50)
c_new <- c+w
tmp <- data.frame(t = 1:500,c,c_new)

my3cols <- c("#E7B800", "#2E9FDF", "#FC4E07")
my2cols <- c("#2E9FDF", "#FC4E07")

tmp %>%
  gather(key,value,-t)%>%
  ggplot(aes(x = t, y = value, col = key))+
  geom_line()+
  facet_grid(key~.)+
  scale_color_manual(values = my2cols)+
  theme_minimal()+
  labs(title = "Effect of WN on a time series", x = "time")+
  theme(legend.position = "none", strip.text = element_blank())
```

## Characteristics of Stationary time series

Stationarity ensures that our time series is at a state of equilibrium and the basic behavior does not alter with time in terms of mean or variance. Therefore, the mean $\mu(t)$ could just be represented by $\mu$. This simplifies our calculations and gives us a good starting point to describing the characteristics of time series. It's also the same reason why if a series is non-stationary, we try to stationarize it first (We shall discuss how very soon).

Stationary time series can further be classified into two types based on variance and autocovariance of the data points:  
- A time series is *weakly* stationary if the variance does not change with time  
- A time series is *strictly* stationary if the joint distribution of the set ${X_{t1},X_{t2}...}$ is equal to the time shifted set $X_{t1+h},X_{t2+h}...$. Also, the mean of the two sets along with the variance is stationary.

In statistics, a collection of random variables always has two aspects- their distribution and their moments. Think of these as IDs but they're not always unique. It's common for two different random variables to have the same distribution and the same moments. The time series random variables also have distribution and moments. It is also possible to define a joint distribution for all these variables. Generally, one would define a joint distribution function as: $$F(c_1,c_2,c_3,...,c_n) = P(x_{t1}<c_1,x_{t2}<c_2,....,x_{tn}<c_n)$$.

Defining such a 'combined' distribution becomes a problem when the random variables follow different distributions. So, we make the assumption that they follow the same (normal) distribution (ie, normal iid or multinomial Gaussian). Now we can define our CDF as:$$F(c_1,c_2,c_3,...,c_n) = \prod_{t=1}^n 1/\sqrt(2 \pi)\int_\infty^x exp(-z^2/2) dz$$

We can now define the PDF as:$$f_{t_1,t_2..}(x)=\frac{dF_{t_1,t_2..}(x)}{dx}$$

We're now at a stage to define the different attributes of a stationary time series process but before that we need to define what a **realization** is. A **realization** is one subset of observations coming from the underlying process. Simply put it represents a sample. And normal statistical practice dictates that we estimate parameters of the process from this realization (or sample). From now on when we are referring to a time series we refer to realization of the actual underlying process.

- **Mean**: As with any statistical distribution, the mean $\mu_t$ of a time series is simply: $$E(X_t)=\int_{- \infty}^{\infty} X f_t(X) dx$$. It is no surprise that the sample (or realization) mean estimates the actual mean. $$\bar{X_t} = \hat{\mu_t}= \frac{\sum_{t=1}^n X_t}{N}$$

- **Variance** : Following the usual definition of variance in statistical literature, the variance of a time series is given by $$Var(X_t)=E(X_t - \mu_t)^2$$. While the estimated variance $$Var(\bar{X_t})= \frac{\sigma^2}{N} \sum_{h=-(n-1)}^{n-1}(1- \frac{|k|}{n})\rho_h$$. where n = realization of length n.

- **Autocovariance function**: It is the second moment function. It is given by:$$Cov(X_t,X_{t+h}) = E[(X_t-\mu_t)(X_{t+h}-\mu{t+h})]$$
Here h is called *lag* time which simply means that the observation $X_{t+h}$ is 'h' lags ahead of $X_t$. This is often represented by $\gamma(h)$ where $h=h+t-t$ for a stationary process and does not depend on t.

It is important to realize that $Cov(X_t,X_t)=Var(X_t)$. Notice, there is no lag here or $(t-t)=0$. Hence, variance is also represented as $\gamma(0)$. The estimated ACVF is given by: $$\hat{\gamma_h}=\frac{1}{n} \sum_{t=1}^{n-|h|} (X_t - \bar{X})(X_{t+|h|}-\bar{X_t})$$ where h=0,$\pm 1,\pm2,..,\pm(n-1)$

## Autocorrelation function (ACF) 

The autocorrelation function, mathematically is similar to the usual correlation function that we encounter in statistics. The intuition behind ACF is that there are correlations between *ordered pairs* (I will explain ordered pairs in details when we discuss difference but for now think of ordered pairs as a pairs of observations that are separated by *h* lags) of data. This is the essence of UBJ ARIMA modeling.

The ACF is a very important tool during the initial model identification as well as final model selection stage. It is given by the formula:$$Corr(X_t,X_{t+h})=\frac{Cov(X_t,X_{t+h})}{\sqrt{Var(X_t)}\sqrt{Var(X_{t+h})}}$$
The difference in lag is $t+h-t=h$ and therefore the above is popularly represented by $\rho(h)$ for a stationary process. 

It can be easily shown that:  
- $\rho(h)=\frac{\gamma(h)}{\gamma(0)}$  
- $\rho(h)=\rho(-h)$. This means that ACF of a stationary time series is symmetric about the origin. 

Therefore, the estimated ACF is just ratio of estimated ACVFs and is given by $$\hat{\rho_h}=\frac{\hat{\gamma_h}}{\hat{\gamma_0}}$$

This is what the ACF (using the `ggAcf` fucntion) of the AT&T stock prices looks like when plotted against lag:

```{r,warning=FALSE,message=FALSE}
ggAcf(att$Price)+
  labs(title = "ACF PLot")+
  theme_minimal()
```

The first set of numbers are the actual values of ACF and the plot below that displays the values. The first value for ACF is always one.(Think why?). The blue dashed line is cutoff that indicates that only the ACF values at the respective lags are significant. Finally, the estimated ACF is also referred to as the sample ACF (SACF).

## Partial Autocorrelation Function (PACF) 

Recall, when I mentioned that the ACF measures the correlation between ordered pairs drawn from a time series data we chose to ignore the effects of the $X_t$'s that lie between the pair. PACF does exactly that. It measures the correlation between $X_t,X_{t+h}$ but takes into consideration all the random variable interactions between them. So, if we wanted to measure the PACF between $X_t$ and $X_{t+2}$, this would also include the effects of $X_{t+1}$ on $X_{t+2}$.

The PACF is designated by the symbol $\phi_{hh}$ and $\hat{\phi_{hh}}$. I will not go into the details of PACF calculation (it involves using the AR, MA and ARMA equations and developing recursive equations that give us fairly good estimates of PACF) since it is complicated and in reality these calculations will be done by R.  
Then why do we study the ACF and PACF? The answer to this question will be revealed very soon when I explain how to do ARIMA modeling. 

Note: Just like theoretical and estimated ACF we also have the theoretical and estimated PACF. And just like it's cousin the estimated PACF is referred to as SPACF or Sample PACF.

Let's look at the PACF (using the `ggPacf`) for AT&T stock prices

```{r,message=FALSE,warning=FALSE}
ggPacf(att$Price)+
  labs(title = "PACF PLot")+
  theme_minimal()
```

Again, the first set of numbers are the actual values of PACF for the time series and the plot above is a graphical interpretation of those numbers. The blue dashed line once again indicates that only the lags for which the PACF peaks crosses the blue region are significant.


# Statistical Models for Time series

Recall that time series data are a collection of random variables indexed according to the sequence they are obtained in time. So, $X_t$ where $t \in {0,\pm1, \pm2,..}$ is such a random variable and the subscript $t$ represents the order or sequence of discrete points in time.

The UBJ ARIMA family has 4 statistical models. They are nothing but an algebraic expression describing how these points are related. 

## The moving average (MA) model

The algebraic equation for the MA model is:$$X_t = c + Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}$$
where $theta_t$ are the parameters of the model and $Z_t$ are the white noise/error terms. $q$ is the lag of the model and is referred to as the 'order' of the process. The choice of $q$ is a personal one and it depends on the person modeling and the nature of the problem at hand.

The intuition behind the above equation is that the current value $X_t$ is an average of the past $q$ random shocks (which are obviously unobserved) so instead of measuring $X_t$ as the mean and the current random shock, say that they are an average of $q$ random shocks.   
There is another intuition behind the moving average equation and perhaps it's a more mathematically accurate intuition. If you were to look at the MA equation, it appears as if it is linear regression of the current value of the series $X_t$ against the current and past values of white noise.

A key question from the MA equation is that even though I have stressed that a univariate Box-Jenkins model essentially studies the relationships between present and past random variables, the above process does not include past random variables. The answer is that the random shock terms above can be replaced by past values of the time series through manipulation. In other words, you can think of $Z_t's$ as being part of $X_t's$.

This is an example of a simulated MA process with $theta_1=0.5$ with 100 data points:

```{r,message=FALSE}
set.seed(42)
ma_sim <- arima.sim(model = list(ma=0.5),n=50)
ma_sim <- as.data.frame(ma_sim)
ma_sim <- ma_sim %>% mutate(t = 1:50)
ggplot(ma_sim,aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs("Simulated MA Series")
```


Examples:  
- MA(1): $X_t = c + Z_t-\theta_1Z_{t-1}$  
- MA(2): $X_t = c + Z_t-\theta_1Z_{t-1}+\theta_2Z_{t-2}$

*Backshift Operator:* There exists an operator called the backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:  
$Z_t = B^0Z_t,Z_{t-1}=B^1Z_t,Z_{t-2}=B^2Z_t$. This gets rid of all the confusing subscripts. It is also convention to write $X_t - c$ as $\tilde{X_t}$. Using these conventions we can write the above examples as:  
- MA(1): $\tilde{X_t} = (1 - \theta_1B)Z_t$  
- MA(2): $\tilde{X_t} = (1 - \theta_1B - \theta_2B^2)Z_t$

Hence a general MA(q) process can be represented as:
$$\tilde{X_t} = (1 - \theta_1B - \theta_2B^2 - ..... - \theta_qB^q)Z_t = \Theta_q(B)Z_t$$


Note: It can be shown that variance of an MA process is independent of $t$. This effect trickles down to their ACF function.

When I discussed ACF and PACF, I had mentioned that the importance of estimated ACF and PACF becomes paramount in the model selection steps. Here's why:

- ACF of an MA process:  
For an MA process the ACF shows spikes at lags where the autocorrelation are significant. Specifically, the MA will have spikes at k $\le$ q and then *sharply* drops to zero. Let's look at the ACF from our simulated MA model:  

```{r, warning=FALSE, message=FALSE}
ma_sim <- arima.sim(model = list(ma=0.5),n=100)
ggAcf(ma_sim)+
  labs(title = "")+
  theme_minimal()
```

The MA(1) process has a *spike* at lag one.

- PACF of an MA process: The PACF of an MA process dampens to zero *quickly* but not *sharply*. The fall is gradual but relatively smooth and quick yet not a sharp drop. Lets look at the PACF of the simulated MA model.

```{r,echo=FALSE}
ma_sim <- arima.sim(model = list(ma=0.5),n=100)
ggPacf(ma_sim)+
  labs(title = "")+
  theme_minimal()
```


At first glance it might be difficult to see the gradual drop as it looks like a spike but if you carefully inspect the *spikes* on both sides of the axis, you will realise that the spikes actually change slowly from being significant to insignificant. The ability to recognise this drop versus a spike takes time and experience. 

## The autoregressive (AR) model

The algebraic equation for the MA model is:$$X_t = c +\phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p} + Z_t$$
where $phi_t's$ are the parameters of the model and $X_t's$ are the previous lagged terms. $p$ is the lag of the model and is referred to as the 'order' of the process. Once again, the choice of $p$ is a personal one and it depends on the person modeling and the nature of the problem at hand.

The AR process is more intuitive than the MA. We can see how the present value is related to the previous value through a regression equation. Also, just like a regression equation, the $Z_t$ represents the probabilistic aspect of a linear model. Let's look at a simulated AR time series:

```{r, message=FALSE, warning=FALSE}
set.seed(42)
ar_sim <- data.frame(x = arima.sim(model = list(ar=0.5),n=50))
ar_sim %>% 
 mutate(t = 1:50)%>%
 ggplot(aes(t,x))+
  geom_line(col = my2cols[1])+
  theme_minimal()+
  labs("Simulated AR Series")
```

Examples:  
- AR(1): $X_t = c + \phi_1X_{t-1} + Z_t$  
- AR(2): $X_t = c +\phi_1X_{t-1}+\phi_2X_{t-2}+Z_t$

*Backshift Operator:* For AR process also there exists a backshift operator that allows us to write the above equation through a simpler syntax. It works in the following way:  
$X_t = B^0X_t,X_{t-1}=B^1X_t,X_{t-2}=B^2X_t$. Like the MA process, it is also convention to write $X_t - \mu$ as $\tilde{X_t}$ where $\mu = \frac{c}{1-\phi_1}$. Using these conventions we can write the above examples as:  
- MA(1): $(1 - \theta_1B)\tilde{X_t} = Z_t$  
- MA(2): $(1 - \theta_1B - \theta_2B^2)\tilde{X_t} = Z_t$

Hence a general MA(q) process can be represented as:
$$(1 - \theta_1B - \theta_2B^2 - ..... - \theta_pB^p)\tilde{X_t} = Z_t = \Phi_p(B)X_t$$

Note: For an AR process the variance depends on time. This effect is visible when we discuss the ACF and PACF of an AR process next.

- **ACF of AR process:**The ACF for the AR process is exactly opposite to the ACF of the MA. The AR ACF shows a gradual decline to zero

- **PACF of AR process:** The PACF is exactly opposite to the PACF of the MA. The AR PACF shows spikes for lags. Let's look at the ACF and PACF plots for AR and MA process.

```{r}

ggAcf(ar_sim)+
  labs(title = "ACF")+
  theme_minimal()

ggPacf(ar_sim)+
  labs(title = "PACF")+
  theme_minimal()


```

This is how ACF and PACF act as distinguishing factors in the identification of the appropriate statistical model that best explains the given realisation. 

## The ARMA process

We have learnt that sometimes the present value depends on the past values (AR process) of the time series process and sometimes the present value depends on the past random shocks (MA process). There's another model that uses both these features. This is called the ARMA model. In time series this process is fully represented by ARMA(p,q) where p, q carry the same meaning as before. Perhaps the mathematical representation will make the intuition more clear,

$$X_t = c + \phi_1X_{t-1} + \phi_2X_{t-2}...+\phi_pX_{t-p}+Z_t + \theta_1Z_{t-1} + \theta_2Z_{t-2}...+\theta_qZ_{t-q}$$

Using the backshift operator this can be represented as:

$$\Phi_p(B)\tilde{X_t} = \Theta_q(B)Z_t$$

Here the symbols have their usual meaning except $\tilde{X_T}$. This is $X_t - \mu$ where $\mu = \frac{c}{1-\phi_1-\phi_2...-\phi_p}$. For instance an ARMA(1,1) process can be written as $(1-\phi_1B)X_t=(1-\theta_1B)Z_t$.

## The ARIMA Model

This is a upgrade from the ARMA model. The 'I' in ARIMA stands for Integrated which indicates the time series values have been differenced. The ARIMA components can be represented in two different ways:

- If the model has a *non seasonal* component only the model is represented as **ARIMA(p,d,q)** where p,d,q stands for the AR, MA and difference order.

- If the has both *seasonal and non seasonal* components, the model is represented by **ARIMA(p,d,q)(P,D,Q)** where P,D,Q stands for the seasonal AR, MA and difference order.


**Identification:** Identifying the order of an ARIMA can be tricky. From personal experience, when there are indications of both AR and MA process in the ACF and PACF, it could indicate the combination of both process. Signs of seasonal components is displayed by spikes or decays at the seasonal lag. Rob Hyndman's [page](https://www.otexts.org/fpp/8/9) on seasonal ARIMA has nice examples.


## What makes a good model

When we select a time series model, there are signs that indicate how good the model is. Here is a list:

- **Stationarity & Invertiblity:** We can use the coefficients of the model to check for stationarity and invertibility by using certain algebraic inequalities. This [resource](http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/TimeSeries_Stationarity.pdf) outlines the rules.

- **Correlation:** The coefficients in the model should not be highly correlated. If they are, they could represent redundent terms.

- **Statistical Significance of parameters:** The parameters estimated should be statistically significant. We can check to see of the p-value is lower than the significance value in the output.
   
- **Diagnostic Checking:** Although in practice we cannot observe the random shocks, it is important that they are statistically independent. If the residuals are autocorrelated they are not white noise. We perform the Ljung-Box (Box-Pierce) test which essentially tests the autocorrelations among random shocks. The random noise should be normally distributed and the residual-ACF should resemble white noise ACF (no significant lags).

# Steps in ARIMA modelling and forecasting

Now we are ready to layout the steps we use to first identify a time series model parameters and then forecast future values. The data used contains 1810 observations in Bowling Green, Ohio of the maximum daily temperature in degrees Celsius, one observation every day from January 1, 2011 - December 31, 2015 (so 5 years of complete data). 

The Bowling Green station has a latitude and longitude of 41.3831, -83.6111. The data goes into the GHCN (Global Historical Climatology Network)-Daily database, maintained by the National Climactic Data Center (NCDC), part of the U.S. Department of Commerce.The data was aggregated over every week and the mean maximum weekly temperature was obtained.The final 51 weeks were used as test set, and the rest was used to train the ARIMA model.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
bg <- read_csv("C:/Users/routh/Desktop/Study Materials/My website/Time series/BG_weekly.csv")

train <- bg[1:(259-51),]
test <- bg[(259-51+1):259,]
```


## Step 1: Visualize the time series

The first step is to visualize the time series to gain insights into the behavior (such as seasonality) of the time series.

```{r}
train %>%
  ggplot(aes(x = week, y = weekly_mean))+
  geom_line(col = my2cols[1])+
  geom_point(col = my2cols[2])+
  labs(title = "Bowling Green Weekly Average Maximum",y = "Temperture",x = "Week ID")+
  theme_minimal()
```


The time series plot for the first four years of data is clearly seasonal with a period of length s = 52 (weeks).

## Step 2: Identify the model

- **ACF Plot**

```{r}
ggAcf(train$weekly_mean, lag.max = 200)+
  labs(title = "")+
  theme_minimal()
```

- **PACF Plot**

```{r}
ggPacf(train$weekly_mean, lag.max = 200)+
  labs(title = "")+
  theme_minimal()
```

The inferences can be summarized:

  - Looking at the ACF, the early lags' decay is possibly quick enough that a differencing of the original series will not be necessary (d = 0), but we may consider it as well (d = 1) since it is a rather grey area. (*NOT SHOWN: A first order difference ACF plot showed the possibility of an MA component*).
  - The seasonal spikes at 52, 104, and 156 are decreasing rather slowly but are within the 5% significance limits, so on the seasonal side there is also a grey area as to whether D = 0 or D = 1 needs to be used.
  - Looking at the PACF, we have spikes for lags 1 and 2, especially for 1 (meaning that, if we use d = 0, may start with an AR1 model)
  
Based on the above information lets fit two different models - *ARIMA(0,1,1)(0,1,1)* and *ARIMA(1,0,1)(1,0,1)*.

## Step 3: Estimation of model parameters

The `Arima` function from `forecast` package is used to fit the model.

```{r}

# create a time series object
y <- ts(train$weekly_mean, frequency = 52, start = c(2011, 1))

first.model <- forecast::Arima(y = y,
                               order = c(0,1,1),
                               seasonal = list(order = c(0,1,1), period = 52))
  

summary(first.model) # summary of the model


second.model <- forecast::Arima(y = y,
                               order = c(1,0,1),
                               seasonal = list(order = c(1,0,1), period = 52))
  

summary(second.model) 
```

## Step 4: Other tests

- **Significance test**

```{r}
lmtest::coeftest(first.model) # first model

lmtest::coeftest(second.model) # second model
```

We see that one of the parameters in the first model is insignificant. 


- **Correlation test**

```{r}
cov2cor(first.model$var.coef) # first model

cov2cor(second.model$var.coef) # second model
```

Extremely high correlation between the SAR and SMA term in the second model.

- **Error measures**

```{r}
first.model$sigma2 # first model
second.model$sigma2 # second model
```

The first model has better (lower) error measure

- **Assumption Checking**

```{r}
checkresiduals(first.model) # first model
checkresiduals(second.model) # second model
```

The assumptions for both models seem to be satisfied.

At this point we make a decision to move forward with one of the model for forecasting. We could go forward with both models and further check prediction errors on both models. But this process would be unrealistic if we had plenty of models. I will pick the first model because it is simpler.

## Step 5: Forecasting

We can forecast using the `forecast` function using an appropriate forecast window.

```{r}
forecasts.object <- forecast(first.model, h = 51)
```

Lets visualize the forecasts.

```{r}
# obtain the forecast means and confidence intervals

forecast.df <- data.frame(
                 week = test$week,
                 forecasts = as.matrix(forecasts.object$mean),
                 upper = as.matrix(forecasts.object$upper),
                 lower = as.matrix(forecasts.object$lower),
                 actual = test$weekly_mean
)


forecast.df %>%
  ggplot()+
    geom_line(mapping = aes(x = week, y = forecasts), col = my2cols[1], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = actual), col = my2cols[2], lwd = 1.05)+
    geom_line(mapping = aes(x = week, y = lower.95.), lty = 2)+
    geom_line(mapping = aes(x = week, y = upper.95.), lty = 2)+
    labs(y = "Temperature",x = "Week",title = "Actual vs Forecasted Value")+
    theme(plot.title = element_text(lineheight=.8, face="bold"),legend.position="top")+
    theme_minimal()
```

The blue line represent the forecasted values while the red one represents the actual ones. The dotted lines on the other hand represent the upper and lower confidence intervals. We can see the model does a pretty good job at forecasting the values for the test data set. 

Finally, let's calculate the accuracy of the forecasted values:

```{r}
error = function(actual, pred){
  mse = sqrt(mean((actual - pred)^2))
  mad =  mean(abs(actual - pred))
  mape = mean(abs((actual - pred)/actual))
  list(mse = mse, mape = mape, mad = mad)
}

error(forecast.df$actual, forecast.df$forecasts)
```

The error function computes the standard error measures (Mean Square Error, Mean Absolute Deviation, Mean Absolute Percentage Error). The error values seem to be sufficiently small.
